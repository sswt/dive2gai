{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODE & SDE models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цели:\n",
    "- Познакомиться с Нейронными ОДУ\n",
    "- Познакомиться с Нейронными СДУ\n",
    "- Узнать, где их можно использовать\n",
    "\n",
    "Содержание:\n",
    "- Neural ODE\n",
    "    - Введение\n",
    "\n",
    "Ссылки:\n",
    "1. [Neural Ordinary Differential Equations](https://arxiv.org/pdf/1806.07366.pdf)\n",
    "2. [Знакомство с Neural ODE](https://habr.com/ru/companies/ods/articles/442002/)\n",
    "3. [Efficient and Accurate Gradients for Neural SDEs](https://arxiv.org/pdf/2105.13493.pdf)\n",
    "\n",
    "Используемые пакеты:\n",
    "1. [Репо torchdiffeq](https://github.com/rtqichen/torchdiffeq/)\n",
    "2. [Репо torchsde](https://github.com/google-research/torchsde/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "В 2019 году вышла статья **Neural Ordinary Differential Equations** и взяла какие-то (уточнить какие) награды на конференциях. В чем была основная идея и какие проблемы решала данная статья?\n",
    "\n",
    "Рассмотрим простую рекурентную сеть, которая генерирует дискретные временные последовательности\n",
    "$$\n",
    "h_{t+1} = h_t + f(h_t, \\theta_t)\n",
    "$$\n",
    "где $t=0,\\dots,T$ — переменная дискретного времени и $h_t\\in\\mathbb{R}^D$ — состояния модели на $t$-ом шаге.\n",
    "\n",
    "Вопрос: Что будет, если мы увеличим число слоев и уменьшим размер шага по времени? В пределе получим, что данное представление соответствует ОДУ первого порядка\n",
    "$$\n",
    "\\dfrac{dh(t)}{dt} = f(h(t), t, \\theta)\n",
    "$$\n",
    "зная значение функции $h(t)$ в начальный момент времени $h(0)$, мы можем получить его значение $h(T)$ решив задачу Коши\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{dh(t)}{dt} = f(h(t), t, \\theta)\\\\\n",
    "h(0) = h_0\n",
    "\\end{cases}\n",
    "$$\n",
    "решить которую мы можем численно. \n",
    "\n",
    "При этом по авторы статьи выделяют следующие плюсы\n",
    "- **Эффективность по памяти:** Авторы предлагают более эфективный способ рассчета обратного распространения ошибки нежели стандартный.\n",
    "\n",
    "- **Адаптивные вычисления:** Варьируя методы решения задачи Коши ОДУ можно балансировать между сложностьи вычислений и невязкой (ошибкой метода).\n",
    "\n",
    "- **Скалируемые и инвертируемые нормализующие потоки:** Авторы показывают, как можно упростить модель NF [рассмотрим в следующей лекции].\n",
    "\n",
    "- **Непрерывные во времени модели временных динамик:** Очевидно, что благодаря подходу можно обучать сети, моделирующие непрерывные динамические системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы решения ОДУ\n",
    "\n",
    "Рассмотрим задачу Коши первого порядка обыкновенного дифференциального уравнения\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{dy(t)}{dt} = f(y(t), t) \\\\\n",
    "y(t_0) = y_0\n",
    "\\end{cases}\n",
    "$$\n",
    "Можно преодразовать уравнение как\n",
    "$$\n",
    "\\dfrac{dy(t)}{dt} = f(y(t), t)\\Rightarrow dy(t) = f(y(t), t)dt\n",
    "$$\n",
    "Возьмем интеграл для некоторого отрезка $[t, t+h]$ с шагом $h$\n",
    "$$\n",
    "\\int\\limits_t^{t+h} dy(t) = \\int\\limits_t^{t+h}f(y(t), t)dt \\Rightarrow y(t+h) - y(t) = \\int\\limits_t^{t+h}f(y(t), t)dt\n",
    "$$\n",
    "Тогда решение принимает вид\n",
    "$$\n",
    "y(t+h) = y(t) + \\int\\limits_t^{t+h}f(y(t), t)dt \\Rightarrow y(t+h) = y(t) + \\int\\limits_0^{h}f(y(t + x), t)dx\n",
    "$$\n",
    "Далее численные решения задачи бедет отличаться тем, как мы будем аппроксимировать численное вычисление оставшегося интеграла $\\int_t^{t+h}f(y(t), t)dt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Явные методы Рунге-Кутты\n",
    "\n",
    "<figure>\n",
    "<img src=\"../.github/images/w12/left-rects.png\" alt=\"Метод аппроксимации интеграла левыми прямоугольниками\">\n",
    "<figcaption align = \"center\">Метод аппроксимации интеграла левыми прямоугольниками</figcaption>\n",
    "</figure>\n",
    "\n",
    "Аппроксимируем интеграл по методу левых прямоугольников, тогда\n",
    "$$\n",
    "\\int\\limits_{0}^{h} f(y, t+x) dx \\approx hf(y, t) + O(h^2)\n",
    "$$\n",
    "это можно расписать в виде рекурентной формы на сетке фиксированного шага ширины $h$\n",
    "$$\n",
    "y_{i+1} = y_i + hf(y_i, t_i)\n",
    "$$\n",
    "этот метод называется методом Эйлера, также извыстным как метод Рунге-Кутты 1-го порядка. На всем интервале имеет ошибку $O(h)$, а на шаге $O(h^2)$.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../.github/images/w12/trap.png\" alt=\"Метод аппроксимации интеграла трапециями\">\n",
    "<figcaption align = \"center\">Метод аппроксимации интеграла трапециями</figcaption>\n",
    "</figure>\n",
    "\n",
    "Теперь попробуем построить более сложную аппроксимацию, возьмем для этого метод трапеций\n",
    "$$\n",
    "\\int\\limits_{0}^{h} f(y, t+x) dx \\approx \\dfrac{h}{2}(f(y(t+h), t+h) + f(y(t), t)) + O(h^3)\n",
    "$$\n",
    "однако $y(t+h)$ мы не знаем, но можем посчитать методом Эйлера, тогда \n",
    "$$\n",
    "\\begin{align*}\n",
    "y^*(t+h) & = y(t) + hf(y(t), t) \\\\\n",
    "y^*(t+h) & = \\dfrac{h}{2}(f(y^*(t+h), t+h) + f(y(t), t)) + O(h^3)\n",
    "\\end{align*}\n",
    "$$\n",
    "тогда рекурентная форма будет\n",
    "$$\n",
    "\\begin{align*}\n",
    "y^*_{i+1} & = y_i + hf(y_i, t_i) \\\\ \n",
    "y_{i+1} & = y_i + \\dfrac{h}{2}(f(y^*_{i+1}, t_{i+1}) + f(y_i, t_i)) + O(h^3)\n",
    "\\end{align*}\n",
    "$$\n",
    "то есть речь идет о некоторой корректировке изначального прогноза. Такой метод называется методом Эйлера с пересчетом. Встает вопрос, что если использовать не один пересчет, а несколько, тогда получим\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i+1}^0 & = y_i + hf(y_i, t_i) \\\\ \n",
    "y_{i+1}^{k+1} & = y_i + \\dfrac{h}{2}(f(y^k_{i+1}, t_{i+1}) + f(y_i, t_i)) + O(h^3)\n",
    "\\end{align*}\n",
    "$$\n",
    "где $k>0$ и выбирается волевым образом.\n",
    "\n",
    "В общем виде явный метод Рунге-Кутты имеет вид\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i+1} & = y_i + h\\sum\\limits_{j=1}^sb_jk_j \\\\\n",
    "k_j & = f(y_i + h\\sum_{n=1}^{j-1} a_{jn}k_n, t_i + c_jh)\n",
    "\\end{align*}\n",
    "$$\n",
    "где $\\sum_{j=1}^s b_j = 1$, $\\sum_{n=1}^{j-1}a_{jn} = c_j$, то есть речь идет о вычислении на шаге постепенном вычислении промежудочных шагов отрезка $[t, t+h]$. Метод называется явным, так как формула рассчета выводится \"явно\". Примером явного метода Рунге-Кутты большего порядка является метод 4-го порядка\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i+1} & = y_i + \\dfrac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4) + O(h^5)\\\\\n",
    "k_1 & = f(y_i, t_i) \\\\\n",
    "k_2 & = f(y_i + \\dfrac{h}{2}k_1, t_i + \\dfrac{h}{2}) \\\\\n",
    "k_3 & = f(y_i + \\dfrac{h}{2}k_2, t_i + \\dfrac{h}{2}) \\\\\n",
    "k_4 & = f(y_i + hk_3, t_i + h) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Неявный метод Рунге-Кутты\n",
    "\n",
    "<figure>\n",
    "<img src=\"../.github/images/w12/right-rects.png\" alt=\"Метод аппроксимации интеграла правыми прямоугольниками\">\n",
    "<figcaption align = \"center\">Метод аппроксимации интеграла правыми прямоугольниками</figcaption>\n",
    "</figure>\n",
    "\n",
    "Аппроксимируем интеграл методом правых прямоугольников, тогда\n",
    "$$\n",
    "\\int\\limits_{0}^{h} f(y, t+x) dx \\approx hf(y(t+h), t+h) + O(h^2)\n",
    "$$\n",
    "в рекурентной форме получим\n",
    "$$\n",
    "y_{i+1} = y_i + hf(y_{i+1}, t_{i+1})\n",
    "$$\n",
    "это __неявный метод Эйлера__. Полученное уравнение имеет вид нелинейного алгебраического уравнения и может быть решена методом простых итераций или методом Ньютона.\n",
    "\n",
    "В общем виде неявный метод Рунге-Кутты имеет вид\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i+1} & = y_i + h\\sum\\limits_{j=1}^sb_jk_j \\\\\n",
    "k_j & = f(y_i + h\\sum_{n=1}^{s} a_{jn}k_n, t_i + c_jh)\n",
    "\\end{align*}\n",
    "$$\n",
    "методы этой группы более сложные с точки зрения вычислений, однако обладают большей устойчивостью в сранении с явными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод Адамса\n",
    "\n",
    "До этого мы рассматривали методы где в вычислениях используется одна точка. Встает вопрос, что если использовать для рассчетов несколько точек в качестве некоторого исторического периода динамики решения. Семейства таких методов называется методами Адамса.\n",
    "$$\n",
    "y_{i+1} = y_i + h\\sum\\limits_{j=0}^k u_{j}f(y_{i-j}, t_{i-j})\n",
    "$$\n",
    "получим метод Адамса-Башфорта. При различном выборе $k$ полчим разные схемы. Так, при $k=0$\n",
    "$$\n",
    "y_{i+1} = y_i + hu_0f(y_{i}, t_{i})~\\text{— метод Эйлера при}~u_0=1\n",
    "$$\n",
    "после получения второй точки методом Эйлера возьмем $k=1$\n",
    "$$\n",
    "y_{i+1} = y_i + hu_0f(y_{i}, t_{i}) + hu_1f(y_{i-1}, t_{i-1})\n",
    "$$\n",
    "и так далее.\n",
    "Неявный метод имеет вид \n",
    "$$\n",
    "y_{i+1} = y_i + h\\sum\\limits_{j=-1}^{k-1} u_{j}f(y_{i-j}, t_{i-j})\n",
    "$$\n",
    "и называется метод Адамса-Мультона.\n",
    "\n",
    "Так как для вычисления методом Адамса $k$-го порядка требуется иметь $k$ первых точек, их обычно вычисляют методом Рунге-Кутты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Название | Вид | Точность шага | Точность на отрезке |\n",
    "|:--------|:--------------------:|:-------------:|:-------------------:|\n",
    "|Явный метод Эйлера| $$y_{i+1} = y_i + hf(y_i, t_i)$$ | $O(h^2)$ | $O(h)$ |\n",
    "|Невный метод Эйлера| $$y_{i+1} = y_i + hf(y_{i+1}, t_{i+1})$$ | $O(h^2)$ | $O(h)$ |\n",
    "|Явный метод Эйлера с пересчетом| $$ y_{i+1}^0 = y_i + hf(y_i, t_i)\\\\ y_{i+1}^{k+1} = y_i + \\dfrac{h}{2}(f(y^k_{i+1}, t_{i+1}) + f(y_i, t_i)) $$ | $O(h^3)$ | $O(h^2)$ |\n",
    "|Явный метод Рунге-Кутты 4-го порядка| $$ y_{i+1} = y_i + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\\\ k_1 = f(y_i, t_i)\\\\ k_2 = f(y_i + k_1\\frac{h}{2}, t_i+\\frac{h}{2})\\\\ k_3 = f(y_i + k_2\\frac{h}{2}, t_i+\\frac{h}{2})\\\\ k_4 = f(y_i + k_3h, t_i+h) $$ | $O(h^5)$ | $O(h^4)$ |\n",
    "|Метод Адамса-Башфорта| $$y_{i+1} = y_i + h\\sum\\limits_{j=0}^k u_{j}f(y_{i-j}, t_{i-j})$$ | $O(h^{k+1})$ | $O(h^k)$ |\n",
    "|Метод Адамса-Мультона| $$y_{i+1} = y_i + h\\sum\\limits_{j=-1}^{k-1} u_{j}f(y_{i-j}, t_{i-j})$$ | $O(h^{k+1})$ | $O(h^k)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Процесс обучения нейронных ОДУ\n",
    "\n",
    "Возьмем в качестве производной $f$ функцию $f(z(t), t, θ)$, параметризованную функцию некоторого семейства параметризованных функций (например нейронную сеть). Тогда при наличии множества пар $\\{z_i, t_i\\}_{i=0}^N$ можно аппроксимировать неизвестную нам функцию $f(z(t), t)$ функцией $f(z(t), t, θ)$. Делать мы это будем решая уравнение методом решения ОДУ $ODESolver$ и вычисляя функцию потерь\n",
    "$$\n",
    "L(z_N) = L(ODESolver(z_0, f_θ, t_0, t_N, θ))\\to\\min\\limits_{θ}\n",
    "$$\n",
    "\n",
    "Далее нужно посчитать обратное распространение ошибки. Современные фреймворки позволяют автоматически рассчитывать обратное распространение, однако авторы метода предлагают воспользоваться свойством ОДУ, позволяющим вычислять уравнение в обратном направлении дл ускорения рассчетов.\n",
    "\n",
    "Для рассчета обратного распространения необходимо рассчитать градиенты по всем параметрам $z_0$, $t_0$, $t_N$, $θ$. Для их рассчета предлагается параметризация градиентов через переменную $a(t)$\n",
    "$$\n",
    "a(t) = -\\dfrac{∂L}{∂z(t)}\n",
    "$$\n",
    "\n",
    "Эта переменная назыается сопряженным (adjoint) состоянием. Ее градиент рассчитывается как\n",
    "$$\n",
    "\\dfrac{da(t)}{dt} = -a(t)\\dfrac{∂ f(z(t), t, θ)}{∂ z}\n",
    "$$\n",
    "\n",
    "Вывод этой формулы предагается в оригинальной статье и вычисляется через сложную производную (chain rule). С помощью этой параметризации градиент для обратного распространения ошибки сводится к решению ОДУ в обратную сторону\n",
    "$$\n",
    "\\dfrac{∂L}{∂z(t_0)} = \\int\\limits_{t_N}^{t_0} a(t)\\dfrac{∂ f(z(t), t, θ)}{∂ z}dt\n",
    "$$\n",
    "Рассчет градиентов $t$ и $θ$ можно расчитать как часть состояния. Такое состояние завется аугментированным.\n",
    "$$\n",
    "\\dfrac{d}{dt}\n",
    "\\begin{bmatrix}\n",
    "z \\\\ θ \\\\ t\n",
    "\\end{bmatrix} (t) = f_{aug}([z, θ, t]) =\n",
    "\\begin{bmatrix}\n",
    "f(z(t), t, θ) \\\\ 0 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Тогда сопряженное состояние к этому аугментированному состоянию\n",
    "$$\n",
    "a_{aug} =\n",
    "\\begin{bmatrix}\n",
    "a \\\\ a_θ \\\\ a_t\n",
    "\\end{bmatrix},~\n",
    "a_θ(t) = \\dfrac{∂L}{∂θ(t)},~a_t(t) = \\dfrac{∂L}{∂t(t)}\n",
    "$$\n",
    "Градиент аугментированной динамики\n",
    "$$\n",
    "\\dfrac{da_{aug}}{dt} = -\n",
    "\\begin{bmatrix}\n",
    "a\\frac{∂f}{∂z} & a\\frac{∂f}{∂θ} & a\\frac{∂f}{∂t}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "Решение этого ОДУ назад во времени дает градиенты по всем входным параметрам в решатель ОДУ и могут быть вычислены за один проход метода $ODESolver$\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{∂L}{∂z(t_0)} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂z}dt \\\\\n",
    "\\dfrac{∂L}{∂θ} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂θ}dt \\\\\n",
    "\\dfrac{∂L}{∂t_0} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂t}dt \\\\\n",
    "\\dfrac{∂L}{∂t_N} = -a(t) \\dfrac{∂f(z(t), t, θ)}{∂t}dt \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "Ниже приведена иллюстрация прямого и обратного распространения ошибки в нейронных ОДУ.\n",
    "<figure>\n",
    "<img src=\"https://habrastorage.org/r/w1560/webt/7e/65/hf/7e65hfxs1amdqyy_uy6emdwulkg.png\" alt=\"Примеры схем генеративных моделй\">\n",
    "<figcaption align = \"center\">Схема прямого и обратного распространения ошибки нейронных ОДУ</figcaption>\n",
    "</figure>\n",
    "Также представлен алгоритм рассчета обратного распространения ошибки.\n",
    "<figure>\n",
    "<img src=\"https://habrastorage.org/r/w1560/webt/8k/pz/uk/8kpzukmizpmezmywov4b3zm29lc.png\" alt=\"Примеры схем генеративных моделй\">\n",
    "<figcaption align = \"center\">Алгоритм вычисления обратного распространения ошибки</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super().__init__()\n",
    "        self.A = A\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y, self.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(y, t, batch_time=8, batch_size=256):\n",
    "    size = len(t)\n",
    "    start = np.random.randint(size-batch_time, size=batch_size)\n",
    "    y0 = y[start]\n",
    "    t0 = np.random.randint(size-batch_time)\n",
    "    ts = t[t0:t0+batch_time]\n",
    "    ys = torch.stack([y[start + i] for i in range(batch_time)], dim=0)\n",
    "    return y0, ts, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEF(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(2, 2, bias=False)\n",
    "        nn.init.normal_(self.net.weight)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(func, y, y0, t, epoch: int = None):\n",
    "    y_pred = odeint(func, y0, t, method='rk4')[:, 0]\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_title('Фазовый портрет')\n",
    "    ax.plot(y_pred.detach()[:, 0].cpu(), y_pred.detach()[:, 1].cpu(), c='r')\n",
    "    ax.scatter(y.detach()[:, 0].cpu(), y.detach()[:, 1].cpu(), s=1, c='k', alpha=0.7)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_xlabel('$y_1$')\n",
    "    ax.set_ylabel('$y_2$')\n",
    "\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    ax.set_title('Траектории')\n",
    "    ax.plot(t.cpu(), y_pred.detach()[:, 0].cpu())\n",
    "    ax.plot(t.cpu(), y_pred.detach()[:, 1].cpu())\n",
    "    ax.scatter(t.cpu(), y.detach()[:, 0].cpu(), s=1, alpha=0.7)\n",
    "    ax.scatter(t.cpu(), y.detach()[:, 1].cpu(), s=1, alpha=0.7)\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$y_1$, $y_2$')\n",
    "    ax.set_xlim(t.cpu().min(), t.cpu().max())\n",
    "    ax.set_ylim(-1, 1)\n",
    "\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    Y, X = np.mgrid[-1:1:21j, -1:1:21j]\n",
    "    dydt = func(0, torch.Tensor(np.stack([X, Y], -1).reshape(21 * 21, 2)).to(device)).cpu().detach().numpy()\n",
    "    mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n",
    "    dydt = (dydt / mag)\n",
    "    dydt = dydt.reshape(21, 21, 2)\n",
    "\n",
    "    ax.set_title('Векторное поле обучения')\n",
    "    ax.streamplot(X, Y, dydt[:, :, 0], dydt[:, :, 1], color=\"black\")\n",
    "    ax.set_xlabel('$y_1$')\n",
    "    ax.set_ylabel('$y_2$')\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_xlim(-1, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if epoch is None:\n",
    "        plt.show();\n",
    "    else:\n",
    "        plt.savefig('imgs/{:04d}'.format(epoch));\n",
    "        plt.clf();\n",
    "        plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2402fdd30745b793edd881721bfe0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Path('imgs').mkdir(exist_ok=True)\n",
    "\n",
    "random.seed(0);\n",
    "np.random.seed(0);\n",
    "torch.manual_seed(0);\n",
    "torch.cuda.manual_seed(0);\n",
    "torch.cuda.manual_seed_all(0);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y0 = torch.Tensor([[0.6, 0.3]]).to(device)\n",
    "t = torch.linspace(0., 20., 200).to(device)\n",
    "A = torch.Tensor([[-0.1, -1.], [1., -0.1]]).to(device)\n",
    "\n",
    "y = odeint(Lambda(A), y0, t, method='rk4')[:, 0]\n",
    "y = y + 0.01 * torch.randn(y.size(), device=device)\n",
    "\n",
    "epoches = 500\n",
    "func = ODEF().to(device)\n",
    "optimizer = torch.optim.Adam(func.parameters(), lr=0.01)\n",
    "\n",
    "pbar = tqdm(total=epoches)\n",
    "\n",
    "with pbar:\n",
    "    for i in range(epoches):\n",
    "        optimizer.zero_grad()\n",
    "        y0, ts, ys = get_batch(y, t)\n",
    "        y_pred = odeint(func, y0, ts, method='rk4').to(device)\n",
    "        loss = torch.mean(torch.abs(y_pred - ys))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Epoch: %i, Loss: %.3f\" % (i, loss.item()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if (i % 20) == 0:\n",
    "                y0 = torch.Tensor([[0.6, 0.3]]).to(device)\n",
    "                plot(func, y, y0, t, i);\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y0 = torch.Tensor([[0.6, 0.3]]).to(device)\n",
    "        plot(func, y, y0, t, epoches);\n",
    "\n",
    "images = []\n",
    "filenames = sorted(list(Path('imgs/').glob('*.png')))\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(str(filename)))\n",
    "imageio.mimsave('simple_ode.gif', images, duration=5, format='GIF')\n",
    "shutil.rmtree('imgs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](../.github/simple_ode.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural SDE\n",
    "\n",
    "Рассмотрим задачу Коши уравнения \n",
    "$$\n",
    "\\begin{cases}\n",
    "dz(t) = \\mu(z(t), t, \\theta)dt + \\sigma(z(t), t, \\theta)dW(t), \\\\\n",
    "z(t_0) = z_0\n",
    "\\end{cases}\n",
    "$$\n",
    "где $\\mu$ — функция дрейфа, $\\sigma$ — , $dW$ — многомерное Броуновское движение.\n",
    "\n",
    "Для решения этого уранвения можно прибегнуть к \"трюку\"\n",
    "$$\n",
    "dz(t) = \\mu(z(t), t, \\theta)dt + \\sigma(z(t), t, \\theta)dW(t) \\Rightarrow \\dfrac{dz(t)}{dt} = \\mu(z(t), t, \\theta) + \\sigma(z(t), t, \\theta)\\dfrac{dW(t)}{dt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
