{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPStxxifGa9v"
      },
      "source": [
        "# Continious Normalizing Flow\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RWtQ5iRjM8K"
      },
      "source": [
        "## Введение\n",
        "\n",
        "В нормализирующих потоках мы использовали теорему о подмене переменной чтобы получить точное изменение распределения при помощи биективной функции $f$\n",
        "$$\n",
        "z_1 = f(z_0) \\Longrightarrow \\log p(z_1) = \\log p(z_0) - \\log\\left|\\det\\dfrac{df}{dz_0}\\right|\n",
        "$$\n",
        "Например планарный (линейный) нормализующий поток можно записать так\n",
        "$$\n",
        "z(t+1) = z(t) + uh(w^T z(t) + b) \\Longrightarrow \\log p(z(t+1)) = \\log p(z(t)) - \\log\\left|1 + u^T\\dfrac{dh}{dz}\\right|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV_Yl3OKGavf"
      },
      "source": [
        "## Теорема о замене переменной\n",
        "\n",
        "Положим, что $z(t)$ — континуальная случайная величина с плотностью $p(z(t))$, зависящая от времени. Положим, что $dz(t)/dt = f(z(t), t)$ — дифференциальное уравнение, описывающее изменение нашей величины от времени. Положим, что $f$ — Липшецево отображение, тогда\n",
        "$$\n",
        "\\dfrac{∂\\log p(z(t))}{∂t} = -\\text{tr}\\left(\\dfrac{df}{dz(t)}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykZdNUSajBxK"
      },
      "source": [
        "## Доказательство\n",
        "\n",
        "Возьмем небольшое приращение времени $ɛ: |ɛ|\\ll 1$. Запишем изменение $z(t)$ при приращении как\n",
        "$$\n",
        "z(t + ɛ)\\overset{\\text{def}}{=}T_ɛ (z(t))\n",
        "$$\n",
        "Положим, что $f$ — непрерывно по Липшецу на $z(t)$ и непрерывно по $t$. Тогда по теоремме Пикарда о существовании каждое знаение аргумента имеет единственное значение функции. Также полагаем, что $z(t)$ — ограничено. Тогда $f$, $T_ɛ$ и $\\frac{∂}{∂z}T_ɛ$ — тоже ограничены. Теперь пользуясь всем этим знанием распишем частную производную\n",
        "$$\n",
        "\\begin{split}\n",
        "\\dfrac{∂\\log p(z(t))}{∂t} & = \\lim\\limits_{ɛ\\to 0^+}\\dfrac{\\log p(z(t)) - \\log|\\det\\frac{∂}{∂z}T_ɛ(z(t))| - \\log p(z(t))}{ɛ} \\\\\n",
        "& = - \\lim\\limits_{ɛ\\to 0^+}\\dfrac{\\log|\\det\\frac{∂}{∂z}T_ɛ(z(t))|}{ɛ} \\\\\n",
        "& = - \\lim\\limits_{ɛ\\to 0^+}\\dfrac{\\frac{∂}{∂ɛ}\\log|\\det\\frac{∂}{∂z}T_ɛ(z(t))|}{\\frac{∂}{∂ɛ}ɛ}~\\text{(правило Лопиталя)} \\\\\n",
        "& = - \\lim\\limits_{ɛ\\to 0^+}\\dfrac{\\frac{∂}{∂ɛ}|\\det\\frac{∂}{∂z}T_ɛ(z(t))|}{|\\det\\frac{∂}{∂z}T_ɛ(z(t))|} \\\\\n",
        "& = - \\lim\\limits_{ɛ\\to 0^+}|\\det\\frac{∂}{∂z}T_ɛ(z(t))|\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Производная определителя может быть расписан по формуле Якоби\n",
        "$$\n",
        "\\begin{split}\n",
        "\\dfrac{∂\\log p(z(t))}{∂t} & =  - \\lim\\limits_{ɛ\\to 0^+}\\text{tr}\\left(\\text{adj}\\left(\\dfrac{∂}{∂z}T_ɛ(z(t))\\right)\\dfrac{∂}{∂ɛ}\\dfrac{∂}{∂z}T_ɛ(z(t))\\right) \\\\\n",
        "& =  - \\text{tr}\\left(\\left(\\lim\\limits_{ɛ\\to 0^+}\\text{adj}\\left(\\dfrac{∂}{∂z}T_ɛ(z(t))\\right)\\right)\\left(\\lim\\limits_{ɛ\\to 0^+}\\dfrac{∂}{∂ɛ}\\dfrac{∂}{∂z}T_ɛ(z(t))\\right)\\right) \\\\\n",
        "& =  - \\text{tr}\\left(\\lim\\limits_{ɛ\\to 0^+}\\dfrac{∂}{∂ɛ}\\dfrac{∂}{∂z}T_ɛ(z(t))\\right)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Закончим доказательство разложением $T_ɛ(z(t))$ по Тейлору\n",
        "$$\n",
        "\\begin{split}\n",
        "\\dfrac{∂\\log p(z(t))}{∂t} & = - \\text{tr}\\left(\\lim\\limits_{ɛ\\to 0^+}\\dfrac{∂}{∂ɛ}\\dfrac{∂}{∂z}(z + ɛf(z(t), t)+O(ɛ^2)+O(ɛ^3)+…)\\right) \\\\\n",
        "& = - \\text{tr}\\left(\\lim\\limits_{ɛ\\to 0^+}\\dfrac{∂}{∂ɛ}(1 + \\dfrac{∂}{∂z}ɛf(z(t), t)+O(ɛ^2)+O(ɛ^3)+…)\\right) \\\\\n",
        "& = - \\text{tr}\\left(\\lim\\limits_{ɛ\\to 0^+}(\\dfrac{∂}{∂z}f(z(t), t)+O(ɛ)+O(ɛ^2)+…)\\right) \\\\\n",
        "& = - \\text{tr}\\left(\\dfrac{∂}{∂z}f(z(t), t)\\right)\n",
        "\\end{split}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1QWSkCnQaAT"
      },
      "source": [
        "Таким образом мы можем представить непрерывный аналог планарного потока\n",
        "$$\n",
        "\\dfrac{dz(t)}{dt} = uh(w^T z(t) + b) \\Longrightarrow \\dfrac{∂\\log p(z(t))}{∂t} = -u^T\\dfrac{dh}{dz}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SowyWsIRRl2k"
      },
      "source": [
        "## Многомерные сети\n",
        "\n",
        "Пока поток $f$ не линейна, путь по Якобиана обладает свойством\n",
        "$$\n",
        "\\text{tr}\\left(\\sum\\limits_n J_n\\right) = \\sum\\limits_n \\text{tr}\\left(J_n\\right)\n",
        "$$\n",
        "Таким образом если наша динамика пораждена суммой функций, диффур логарифма плотности также является суммой\n",
        "$$\n",
        "\\dfrac{dz(t)}{dt} = \\sum\\limits_{i=1}^Mf_i(z(t)) \\Longrightarrow \\dfrac{∂\\log p(z(t))}{∂t} = \\sum\\limits_{i=1}^M\\text{tr}\\left\n",
        "(\\dfrac{df_i}{dz}\\right)\n",
        "$$\n",
        "\n",
        "Получается мы можем дешево (по сложности) обучать потоковые модели с большим числом скрытых слоев со сложностью равной числу скрытых слоев $O(M)$. В тоже время использование таких \"широких\" потоков в обычных нормализационных моделях имеет сложность $O(M^3)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSB9OCFjUxY7"
      },
      "source": [
        "## Использование динамики во времени\n",
        "\n",
        "Также интересно то, что мы можем использовать один поток с параметром времени, то есть явно зависимый от времени $f(z(t), t)$, а не $f(z(t))$. Также авторы метода предлагают новый страбирующий метод\n",
        "$$\n",
        "\\dfrac{dz(t)}{dt} = \\sum\\limits_nσ_n(t)f_n(z),~σ\\in(0,1)\n",
        "$$\n",
        "где $σ_n(t)$ — нейронная сеть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VFmZYO4ChpB"
      },
      "source": [
        "## Численное решение задачи Коши ОДУ\n",
        "\n",
        "Рассмотрим задачу Коши первого порядка обыкновенного дифференциального уравнения\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\dfrac{dz(t)}{dt} = f(z(t), t), \\\\\n",
        "z(t_0) = z_0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Такое уравнение мы можем решать численно методами Рунге-Кутты. Для того чтобы понять, как это работает, рассмотрим вывод самого простого метода — метода Эйлера. Проинтегрируем обе части уравнения\n",
        "$$\n",
        "\\dfrac{dz(t)}{dt} = f(z(t), t)⇒ dz(t) = f(z(t), t)dt⇒\\int dz(t) = \\int f(z(t), t)dt\n",
        "$$\n",
        "Так как численное решение не может быть рассчитано на всем бесконечном временном отрезке, возьмем отрезок $[t_0, t_N]$ для которого определен отрезок $[z_0, z_N]$. Тогда решение на этом отрезке расписывается как определенный интеграл\n",
        "$$\n",
        "\\int\\limits_{z_0}^{z_N} dz(t) = \\int\\limits_{t_0}^{t_N} f(z(t), t)dt⇒z_N - z_0 = \\int\\limits_{t_0}^{t_N} f(z(t), t)dt⇒z_N = z_0 + \\int\\limits_{t_0}^{t_N} f(z(t), t)dt\n",
        "$$\n",
        "Для численного интегрирования воспользуемся методом прямоугольников. Для этого введем равномерную сетку по времени количеством значений $N$ и с шагом $h$ такие, что $h = \\frac{|t_1 - t_0|}{N}↔N=\\frac{|t_1 - t_0|}{h}$. Так сетка по времени выглядит как $t_i=t_0+h\\cdot i,~i=0,\\dots,N$.\n",
        "$$\n",
        "\\int\\limits_{t_0}^{t_N} f(z(t), t)dt≈∑\\limits_{i=0}^N hf(z_i, t_i)⇒z_N = z_0 + ∑\\limits_{i=0}^N hf(z_i, t_i)\n",
        "$$\n",
        "Но так как в задаче Коши нам дано только значение $z(t_0)$, то решать задачу будем итерационно\n",
        "$$\n",
        "z_i = z_{i-1} + hf(z_i, t_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcAuAMlfIYbl"
      },
      "source": [
        "## Нейронные ОДУ\n",
        "\n",
        "Теперь положим, что мы не знаем $f(z(t), t)$. Возьмем в качестве производной $f$ функцию $f(z(t), t, θ)$, параметризованную функцию некоторого семейства параметризованных функций (например нейронную сеть). Тогда при наличии множества пар $\\{z_i, t_i\\}_{i=0}^N$ можно аппроксимировать неизвестную нам функцию $f(z(t), t)$ функцией $f(z(t), t, θ)$. Делать мы это будем решая уравнение методом Рунге-Кутты $rk$ и вычисляя функцию потерь\n",
        "$$\n",
        "L(z_N) = L(rk(z_0, f_θ, t_0, t_N, θ))\\to\\min\\limits_{θ}\n",
        "$$\n",
        "\n",
        "Далее нужно посчитать обратное распространение ошибки. Современные фреймворки позволяют автоматически рассчитывать обратное распространение, однако авторы метода предлагают воспользоваться свойством ОДУ, позволяющим вычислять уравнение в обратном направлении дл ускорения рассчетов.\n",
        "\n",
        "Для рассчета обратного распространения необходимо рассчитать градиенты по всем параметрам $z_0$, $t_0$, $t_N$, $θ$. Для их рассчета предлагается параметризация градиентов через переменную $a(t)$\n",
        "$$\n",
        "a(t) = -\\dfrac{∂L}{∂z(t)}\n",
        "$$\n",
        "\n",
        "Эта переменная назыается сопряженным (adjoint) состоянием. Ее градиент рассчитывается как\n",
        "$$\n",
        "\\dfrac{da(t)}{dt} = -a(t)\\dfrac{∂ f(z(t), t, θ)}{∂ z}\n",
        "$$\n",
        "\n",
        "Вывод этой формулы предагается в оригинальной статье и вычисляется через сложную производную (chain rule). С помощью этой параметризации градиент для обратного распространения ошибки сводится к решению ОДУ в обратную сторону\n",
        "$$\n",
        "\\dfrac{∂L}{∂z(t_0)} = \\int\\limits_{t_N}^{t_0} a(t)\\dfrac{∂ f(z(t), t, θ)}{∂ z}dt\n",
        "$$\n",
        "Рассчет градиентов $t$ и $θ$ можно расчитать как честь состояния. Такое состояние завется аугментированным.\n",
        "$$\n",
        "\\dfrac{d}{dt}\n",
        "\\begin{bmatrix}\n",
        "z \\\\ θ \\\\ t\n",
        "\\end{bmatrix} (t) = f_{aug}([z, θ, t]) =\n",
        "\\begin{bmatrix}\n",
        "f(z(t), t, θ) \\\\ 0 \\\\ 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Тогда сопряженное состояние к этому аугментированному состоянию\n",
        "$$\n",
        "a_{aug} =\n",
        "\\begin{bmatrix}\n",
        "a \\\\ a_θ \\\\ a_t\n",
        "\\end{bmatrix},~\n",
        "a_θ(t) = \\dfrac{∂L}{∂θ(t)},~a_t(t) = \\dfrac{∂L}{∂t(t)}\n",
        "$$\n",
        "Градиент аугментированной динамики\n",
        "$$\n",
        "\\dfrac{da_{aug}}{dt} = -\n",
        "\\begin{bmatrix}\n",
        "a\\frac{∂f}{∂z} & a\\frac{∂f}{∂θ} & a\\frac{∂f}{∂t}\n",
        "\\end{bmatrix}^T\n",
        "$$\n",
        "\n",
        "Решение этого ОДУ назад во времени дает градиенты по всем входным параметрам в решатель ОДУ и могут быть вычислены за один проход метода $rk$\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\dfrac{∂L}{∂z(t_0)} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂z}dt \\\\\n",
        "\\dfrac{∂L}{∂θ} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂θ}dt \\\\\n",
        "\\dfrac{∂L}{∂t_0} = \\int\\limits_{t_N}^{t_0} a(t) \\dfrac{∂f(z(t), t, θ)}{∂t}dt \\\\\n",
        "\\dfrac{∂L}{∂t_N} = -a(t) \\dfrac{∂f(z(t), t, θ)}{∂t}dt \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "Ниже приведена иллюстрация прямого и обратного распространения ошибки в нейронных ОДУ.\n",
        "<figure>\n",
        "<img src=\"https://habrastorage.org/r/w1560/webt/7e/65/hf/7e65hfxs1amdqyy_uy6emdwulkg.png\" alt=\"Примеры схем генеративных моделй\" style=\"width:60%\">\n",
        "<figcaption align = \"center\">Схема прямого и обратного распространения ошибки нейронных ОДУ</figcaption>\n",
        "</figure>\n",
        "Также представлен алгоритм рассчета обратного распространения ошибки.\n",
        "<figure>\n",
        "<img src=\"https://habrastorage.org/r/w1560/webt/8k/pz/uk/8kpzukmizpmezmywov4b3zm29lc.png\" alt=\"Примеры схем генеративных моделй\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Алгоритм вычисления обратного распространения ошибки</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6fUJx8IGLhB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJZ1u6KhOY83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ссылки\n",
        "\n",
        "1. []()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPS9q2inSsbXsbfymDsFCV9",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
