{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9vDGpenj4p1"
      },
      "source": [
        "# Dive to Normalizing Flows\n",
        "---\n",
        "\n",
        "Цели:\n",
        "- Разобраться с формальным выводом NF\n",
        "- Подробнее рассмотреть потоки и их свойства\n",
        "- Деквантизация\n",
        "\n",
        "Содержание:\n",
        "- [Вывод нормализующих потоков](#intro)\n",
        "- [Planar Flow](#planar)\n",
        "- [Affine Coupling Layer](#affine)\n",
        "- [Другие потоки и их свойства](#also)\n",
        "\n",
        "Ссылки:\n",
        "- [Variational Inference with Normalizing Flows](https://arxiv.org/pdf/1505.05770v6.pdf)\n",
        "- [Learning Discrete Distributions by Dequantization](https://arxiv.org/pdf/2001.11235.pdf)\n",
        "- [DENSITY ESTIMATION USING REAL NVP](https://arxiv.org/pdf/1605.08803v3.pdf)\n",
        "- [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/pdf/1807.03039v2.pdf)\n",
        "- [Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design](https://arxiv.org/pdf/1902.00275.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFlAPKoj9Ku"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Вывод нормализующих потоков\n",
        "\n",
        "Давайте посмотрим еще раз на этот рисунок и попробуем расписать итерационный процесс\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png\" alt=\"Примеры схем генеративных моделй\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Иллюстрация преобразования нормально распределенного z0 в zK из реального  распределения.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Положим, существуют\n",
        "$$\n",
        "z_{i-1}\\sim p_{i-1}(z_{i-1}), ~ z_i = f_i(z_{i-1})\\Leftrightarrow z_{i-1} = f^{-1}_i(z_i)\n",
        "$$\n",
        "\n",
        "где $z_{i-1}$ — вектор, распределенный как $p_{i-1}$, а $f_i$ — функция, отображающая $z_{i-1}$ в $z_i$. Также существует обратная к $f$ функция $f^{-1}$, которая отображает в обратную сторону.\n",
        "\n",
        "Переход по графу на рисунке можно расписать как\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "p_i(z_i) & \\stackrel{(1)}{=} p_{i-1}(f_i^{-1}(z_i))\\left|\\det\\left(\\dfrac{df^{-1}_i(z_i)}{dz_i}\\right)\\right| \\\\\n",
        "& \\stackrel{(2)}{=} p_{i-1}(z_{i-1})\\left|\\det\\left(\\dfrac{df_i(z_{i-1})}{dz_{i-1}}\\right)^{-1}\\right| \\\\\n",
        "& \\stackrel{(3)}{=} p_{i-1}(z_{i-1})\\left|\\det\\left(\\dfrac{df_i(z_{i-1})}{dz_{i-1}}\\right)\\right|^{-1} \\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Разберем подробнее каждый переход\n",
        "\n",
        "1. Получено из теоремы о замене переменной для многомерных случайных величин.\n",
        "2. Следует из теоремы об обратимых функциях.\n",
        "3. Для обратимых матриц существует свойство определителя $\\det(A^{-1}) = \\det(A)^{-1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX6KRI5WITfh"
      },
      "source": [
        "### Теорема о замене переменной\n",
        "\n",
        "Положим существует переменная $z$ распределенная как $z\\sim\\pi(z)$, при помощи которой мы хотим получить новую независимую переменную, используя преобразование $x=f(z)$. Функцию $f$ выберем так, что $\\exists f^{-1}: z=f^{-1}(x)$. Вопрос, каким будет распределение $x\\sim p(x)$?\n",
        "\n",
        "Заметим, что если $f(z)$ — дифференцируема на некоторой области и кусочно-монотонна, тогда по свойству преобразования случайной величины\n",
        "$$\n",
        "p(x) = \\pi(z)\\left|\\dfrac{dz}{dx}\\right|\n",
        "$$\n",
        "отсюда получаем, что\n",
        "$$\n",
        "\\pi(z)\\left|\\dfrac{dz}{dx}\\right| = \\pi(f^{-1}(x))\\left|\\dfrac{df^{-1}}{dx}\\right| = \\pi(f^{-1}(x))|(f^{-1})^{'}(x)|\n",
        "$$\n",
        "\n",
        "В случае многомерной случайной величины выражение схоже\n",
        "\n",
        "$$\n",
        "\\vec{z}\\sim\\pi(\\vec{z}), ~\\vec{x}=f(\\vec{z}),~\\vec{z}=f^{-1}(\\vec{x})\n",
        "$$\n",
        "\n",
        "$$\n",
        "p(\\vec{x}) = \\pi(\\vec{z})\\left|det\\dfrac{d\\vec{z}}{d\\vec{x}}\\right| = \\pi(f^{-1}(\\vec{x}))\\left|det\\dfrac{df^{-1}(\\vec{x})}{d\\vec{x}}\\right|\n",
        "$$\n",
        "\n",
        "где $det\\frac{df^{-1}(\\vec{x})}{d\\vec{x}}$ — Якобиан (определитель матрицы Якоби) функции $f^{-1}$. Матрицей Якоби многомерной функции $f(\\vec{x})$ вектора $\\vec{x}$ называется матрица всех частных производных первого порядка\n",
        "$$\n",
        "\\frac{df}{dz} =\n",
        "\\begin{bmatrix}\n",
        "\\dfrac{\\partial f_0}{\\partial x_0} & \\ldots & \\dfrac{\\partial f_0}{\\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\dfrac{\\partial f_n}{\\partial x_0} & \\ldots & \\dfrac{\\partial f_n}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "здесть индексы — номера компонентов векторов. Вывод приведенной формулы для $p_x$ подробнее рассмотрим в advanced части."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vw9XephIWC2"
      },
      "source": [
        "### Теорема об обратных функциях\n",
        "Теорема об обратных функциях говорит о том, что если $y = f(x),~x=f^{-1}(y)$, то\n",
        "$$\n",
        "\\dfrac{df^{-1}(y)}{dy} = \\dfrac{dx}{dy}=\\left(\\dfrac{dy}{dx}\\right)^{-1} = \\left(\\dfrac{df(x)}{dx}\\right)^{-1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHeutqrGIXSE"
      },
      "source": [
        "### Свойство определителя обратной матрицы\n",
        "$$\n",
        "A\\cdot A^{-1} = I \\Rightarrow \\det(A\\cdot A^{-1}) = \\det(I)  \\Rightarrow \\det(A\\cdot A^{-1}) = 1 \\\\\n",
        "\\det(A)\\cdot \\det(A^{-1}) = 1 \\Rightarrow \\det(A^{-1}) = \\dfrac{1}{\\det(A)} = \\det(A)^{-1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBsIZnlR2T5h"
      },
      "source": [
        "### Переход к суперпозиции\n",
        "\n",
        "Таким образом получить из нормально распределенных $z_0$ объекты $x=z_K$ можно через суперпозицию всех функций $f_i$ как\n",
        "$$\n",
        "x=z_K=f_K\\circ f_{K-1}\\circ\\cdots\\circ f_1(z_0)\n",
        "$$\n",
        "\n",
        "Тогда трансформация логарифма плотности распределения\n",
        "$$\n",
        "\\begin{split}\n",
        "\\log p(x) = \\log\\pi_K(z_K) & = \\log\\pi_{K-1}(z_{K-1})-\\log\\left|\\det\\dfrac{df_K}{dz_{K-1}}\\right| \\\\\n",
        "& = \\log\\pi_{K-2}(z_{K-2}) -\\log\\left|\\det\\dfrac{df_{K-1}}{dz_{K-2}}\\right| -\\log\\left|\\det\\dfrac{df_K}{dz_{K-1}}\\right| \\\\\n",
        "& = \\cdots \\\\\n",
        "& = \\log\\pi_0(z_0) - \\sum\\limits_{i=1}^K\\log\\left|\\det\\dfrac{df_i}{dz_{i-1}}\\right|\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Эта функция является логарифмом правдоподобия и может быть использована в качестве функции потерь для обучения. Однако, так как оптимизационная задача с функцией правдоподобия — задача максимизации, возьмем функцию логарифма правдоподобия с обратным знаком (negative log likelihood loss), которую и будем минимизировать.\n",
        "$$\n",
        "NLLLoss = - \\mathbb{E}_{z_K\\sim \\pi_K}\\left[\\log\\pi_0(z_0) - \\sum\\limits_{i=1}^K\\log\\left|\\det\\dfrac{df_i}{dz_{i-1}}\\right|\\right]\n",
        "$$\n",
        "\n",
        "Здесь преобразование $z_i=f_i(z_{i-1})$ называется потоком (flow), а полная последовательность из преобразований называется нормализующим потоком (normalizing flow).\n",
        "\n",
        "Стоит уточнить, что на функции $f_i$ накладывается ряд требований:\n",
        "\n",
        "1. $f_i$ должна быть легко обратимой функцией\n",
        "2. Якобиан $\\frac{df_i}{dz_{i-1}}$ должен быть легко вычислимым"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBP6KDSj6YxB"
      },
      "source": [
        "<a name=\"planar\"></a>\n",
        "## Planar Flow\n",
        "\n",
        "Рассмотрим первую трансформацию, использующуюся в качестве потока — планарный поток. Это семейство функций вида\n",
        "$$\n",
        "f(x) = uh(w^Tx + b)\n",
        "$$\n",
        "где\n",
        "- $h$ — функция активации\n",
        "- $u, w, b$ — обучаемые параметры\n",
        "\n",
        "### Обратимость\n",
        "\n",
        "Проблема этого потока заключается в том, что не каждая функция этого семейства может быть обратима. Для обратимости требуется чтобы\n",
        "1. $h(x) = \\tanh(x)$\n",
        "2. $w^Tu \\geq -1$\n",
        "\n",
        "Второе условие может быть достигнуто при посощи обновления вектора $u$ так, что\n",
        "$$\n",
        "\\hat{u}(w, u) = u + \\left[m(w^Tu) - w^Tu\\right] \\frac{w}{||w||_2}\n",
        "$$\n",
        "где $m(x) = -1+\\log{(1+e^x)}$. \n",
        "\n",
        "### Якобиан\n",
        "Якобиан можно записать как\n",
        "$$\n",
        "\\det{J} = \\det{\\left(I + u\\psi(x)^T\\right)} = 1 + u^T\\psi(x)\n",
        "$$\n",
        "где\n",
        "- $I$ — единичная матрица\n",
        "- $\\psi(x) = h'(w^Tx + b)$\n",
        "- $h'$ — произвлдная от $h$, при $\\tanh(x)$ производная\n",
        "$$\n",
        "\\tanh(x)' = \\left(\\dfrac{e^x - e^{-x}}{e^x + e^{-x}}\\right)' = \\dfrac{(e^x + e^{-x})^2 - (e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 - \\dfrac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 - \\tanh(x)^2\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMbH8flhj9D8"
      },
      "source": [
        "<a name=\"affine\"></a>\n",
        "## Affine Coupling Layer\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i = x_i,~\\forall i=1,2,\\dots,d \\\\\n",
        "y_i = x_i \\odot \\exp(s(x_{i-d})) + t(x_{i-d}),~\\forall i=d+1,\\dots,D\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "где $s(\\cdot)$ — масштабирование, $t(\\cdot)$ — смещение, а $\\odot$ — поэлементное умножение.\n",
        "\n",
        "Теперь убедимся, что это преобразование подходит нам по критериям функции $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3M51EAJICr3"
      },
      "source": [
        "### Обратимость\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i = x_i~,\\\\\n",
        "y_i = x_i \\odot \\exp(s(x_{i-d})) + t(x_{i-d})~;\n",
        "\\end{cases} \\Leftrightarrow\n",
        "\\begin{cases}\n",
        "x_i = y_i~,\\\\\n",
        "x_i = (y_i - t(x_{i-d})) \\odot \\exp(-s(x_{i-d}))~.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Так как преобразование линейно, оно не требует поиска обратных функций к $s(\\cdot)$ и $t(\\cdot)$, а значит, что вычисляется быстро."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMl8OpSWIFN8"
      },
      "source": [
        "### Якобиан\n",
        "\n",
        "Легко заметить, что Якобиан функции имеет вид нижней треугольной матрицы\n",
        "\n",
        "$$\n",
        "J =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{I} & \\mathbf{0} \\\\\n",
        "\\frac{\\partial y_{d+1:D}}{\\partial x^T_{1:d}} & \\text{diag}(\\exp[s(x_{1:d})])\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "где\n",
        "- $\\mathbf{I}_d$ — единичная матрица, полученная из $\\frac{\\partial y_{1:d}}{\\partial x_{1:d}} = \\frac{\\partial x_{1:d}}{\\partial x_{1:d}}$ размерности $d\\times d$\n",
        "- $\\mathbf{0}$ — нулевая матрица, полученная из $\\frac{\\partial y_{1:d}}{\\partial x_{d+1:D}} = \\frac{\\partial x_{1:d}}{\\partial x_{d+1:D}}$ размерности $d\\times (D-d)$\n",
        "- $\\frac{\\partial y_{d+1:D}}{\\partial x^T_{1:d}}$ — минор матрицы Якоби размерности $(D-d)\\times d$\n",
        "- $\\text{diag}(\\exp[s(x_{1:d})])$ — диагональная матрица, где на диагонали значения $\\exp[s(x_{1:d})]$, а вне — нули. Размерность это матрицы $(D-d)\\times(D-d)$\n",
        "\n",
        "Можно заметить, что такая матрица может существовать только при $d = \\frac{D}{2}$.\n",
        "\n",
        "Так как матрица имеет вид нижней треугольно, ее определитель считается по диагонали, а значит, что левую нижнюю часть матрицы знать не обязательно. Тогда Якобиан вычисляется как\n",
        "\n",
        "$$\n",
        "\\det(J) = \\prod\\limits_{j=1}^{D-d}\\exp(s(x_{1:d}))_j = \\exp\\left(\\sum\\limits_{j=1}^{D-d}s(x_{1:d})_j\\right)\n",
        "$$\n",
        "\n",
        "Так как вычисление определителя сводится к вычислению экспоненты суммы выходов $s(\\cdot)$, считаться определитель будет быстро.\n",
        "\n",
        "Так как к функциям $s(\\cdot)$ и $t(\\cdot)$ не выставлено никаких сложных требований, в их качестве можно использовать функции класса нейронных сетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqIIFQm_IFBs"
      },
      "source": [
        "### Использование маски\n",
        "\n",
        "Разбиаение преобразований на тривиальные и афинные в Affine Coupling Layer можно представить как бинарную маску $b$. Тогда преобразование можно переписать как\n",
        "$$\n",
        "y = b ⊙ x + (1-b) ⊙ (x ⊙ \\exp(s(b ⊙ x)) + t(b ⊙ x))\n",
        "$$\n",
        "\n",
        "Тогда обратное преобразование получается следующим\n",
        "$$\n",
        "x = b ⊙ y + (1 - b) ⊙ ((y - t(b ⊙ y)) ⊙ \\exp(-s(b ⊙ y)))\n",
        "$$\n",
        "\n",
        "Так как маска влияет на порядок строк и столбцов в матрице Якобиана нужно сказать, что перестановка строк или столбцов в определители влияет только на знак определителя матрицы, а так как для вычисления плотности распределения мы используем модуль определителя, перестановка строк и столбцов на вычисление плотности не влияет.\n",
        "\n",
        "Таким образом мы можем свести вычисление оперделителя при использовании маски к базовому определению афинного потока выше, переставляя строки матрицы.\n",
        "\n",
        "Сам Якобиан можно переписать как\n",
        "$$\n",
        "\\det(J) = \\prod(1-b) ⊙ \\exp(s(b \\odot x)) = \\exp\\left(\\sum(1-b) ⊙ s(b \\odot x)\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP4Sdc4Nj8-O"
      },
      "source": [
        "<a name=\"dequant\"></a>\n",
        "## Деквантизация\n",
        "\n",
        "Нормализующие потоки полагаются на правило изменения переменных, которое естественным образом определено в непрерывном пространстве. Применение потоков непосредственно к дискретным данным приводит к нежелательным моделям плотности.\n",
        "\n",
        "Так на рисунке ниже иллюстрируется как нормализующий поток пытается преобразовать нормальное распределение в дискретное распределение прдставленное двумя точками — $(0, 1),~(1, 0)$. Видно, что модель в итоге приводит распределение к некоторой \"линии\", что никак не сходится с требуемым распределением. Есть несколько вариантов решения проблемы:\n",
        "- Решать задачу для дискретного распределения\n",
        "- Придумать поток, который бы приводил диксретное распределение к непрерывному и наоборот\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/simple_flow.png?raw=true\" alt=\"Пример работы нормализующего потока без квантизации с дискретным распределением\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока без квантизации с дискретным распределением.</figcaption>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Деквантизация равномерным распределением\n",
        "\n",
        "Первой идеей перехода от дискретного к непрерывному распределению в нормализующих потоках стала деквантизация с добавлением аддитивного равномерного шума. Для данных $x$ размерности $D$ таких, что $x\\in\\{0,1,\\dots,N\\}:=G\\subset\\mathbb{N}$ деквантизация определяется как $y = x + u$, где $u\\sim\\mathcal{U}[0, 1)^D$. Покажем, что обучение нашей модели непрерывной плотности $p_{model}$ на данных $y$ может быть представлено как максимизация нижней границы логарифма функции правдоподобия некоторой дискретной модели $P_{model}$ для оригинального дискретного набора $x$\n",
        "$$\n",
        "P_{model} := \\int\\limits_{[0, 1)^D} p_{model}(x+u)du.\n",
        "$$\n",
        "Обознаим распределение дискретных данных $x$ как $P_{data}$ и $p_{data}$ — распределение деквантизированных данных. Тогда функция логарифма правдоподобия расписывается как\n",
        "$$\n",
        "\\begin{split}\n",
        "\\mathbb{E}_{y\\sim p_{data}}[\\log{p_{model}(y)}] & = \\int p_{data}(y)\\log{p_{model}(y)}dy \\\\\n",
        "&= \\int\\limits_G P_{data}(x)dx\\int\\limits_{[0, 1)^D}\\log{p_{model}(x+u)}du \\\\\n",
        "&= \\sum\\limits_{x\\in G} P_{data}(x)\\int\\limits_{[0, 1)^D}\\log{p_{model}(x+u)}du\n",
        "\\end{split}\n",
        "$$\n",
        "далее по неравенству Йенсена, которое гласит, что для выпуклой функции $f$ выполняется следующее\n",
        "$$\n",
        "f(tx + (1-t)y)\\leq tf(x)+(1-t)f(y)\n",
        "$$\n",
        "Для нашего выражения \n",
        "$$\n",
        "\\sum\\limits_{x\\in G} P_{data}(x)\\int\\limits_{[0, 1)^D}\\log{p_{model}(x+u)}du \\leq \\sum\\limits_{x\\in G} P_{data}(x)\\log{\\int\\limits_{[0, 1)^D}p_{model}(x+u)du} = \\mathbb{E}_{x\\sim P_{data}}[\\log{P_{model}(x)}]\n",
        "$$\n",
        "Буквально это значит, что \n",
        "$$\n",
        "\\mathbb{E}_{y\\sim p_{data}}[\\log{p_{model}(y)}]\\leq\\mathbb{E}_{x\\sim P_{data}}[\\log{P_{model}(x)}],\n",
        "$$\n",
        "что и требовалось показать. Из этого следует, что максимизируя левую часть неравенства, максимизируется и правая. Это доказывает, что мы можем использовать этот вид деквантизации при обучении нормализующего потока.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/dequant_flow.png?raw=true\" alt=\"Пример работы нормализующего потока с простой квантизацией.\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с простой квантизацией.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Вариационная деквантизация\n",
        "\n",
        "По предыдущему примеру можно видеть, что деквантизация с использованием нормального распределения несет за собой некоторые издержки. Так, например, довольно сложно потоками получить нормальное многомерное распределение, что видно на примере выше. Также нормальное распределение не учитывает изходную частотность элементов распределения, что тоже делает генерацию некорректной в случаях с несбалансированными распределениями.\n",
        "\n",
        "Что если вместо заданного распределения шума мы обучим дополнительный поток прогназировать шум исходя из природы исходных данных?\n",
        "\n",
        "Введем распределение шума $q(u|X)$, где $u\\sim\\mathcal{U}[0, 1)^D$ как и раньше. И снова распишем логарифм функции правдоподобия\n",
        "$$\n",
        "\\begin{split}\n",
        "\\mathbb{E}_{x\\sim P_{data}}[\\log{P_{model}(x)}] &= \\mathbb{E}_{x\\sim P_{data}}\\left[\\log{\\int\\limits_{[0, 1)^D}q(u|x)\\dfrac{p_{model}(x+u)}{q(u|x)}du}\\right] \\\\\n",
        "&\\geq \\mathbb{E}_{x\\sim P_{data}}\\left[\\int\\limits_{[0, 1)^D}q(u|x)\\log{\\dfrac{p_{model}(x+u)}{q(u|x)}}du\\right] \\\\\n",
        "&= \\mathbb{E}_{x\\sim P_{data}}\\mathbb{E}_{u\\sim q(\\cdot|x)}\\left[\\log{\\dfrac{p_{model}(x+u)}{q(u|x)}}\\right] \\\\\n",
        "\\end{split}\n",
        "$$\n",
        "Исходный шум $u$ будем задавать как $u = q_x(\\varepsilon)$, где $\\varepsilon\\sim p=\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, а $q_x$ — выбранным нами поток аппроксимации шума. Тогда распределение $q(u|x)$ можно расписать как\n",
        "$$\n",
        "q(u|x) = p(q_x^{-1}(u))\\cdot\\left|\\dfrac{dq_x^{-1}}{du}\\right| = p(\\varepsilon)\\cdot\\left|\\dfrac{dq_x}{d\\varepsilon}\\right|^{-1}\n",
        "$$\n",
        "и подставив это выражение в оценку правдоподобия получим\n",
        "$$\n",
        "\\begin{split}\n",
        "\\mathbb{E}_{x\\sim P_{data}}\\mathbb{E}_{u\\sim q(\\cdot|x)}\\left[\\log{\\dfrac{p_{model}(x+u)}{q(u|x)}}\\right] &= \\mathbb{E}_{x\\sim P_{data}, \\varepsilon\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})}\\left[\\log{\\dfrac{p_{model}(x+q_x(\\varepsilon))}{p(\\varepsilon)\\cdot\\left|dq_x/d\\varepsilon\\right|^{-1}}}\\right] \\\\\n",
        "&= \\mathbb{E}_{x\\sim P_{data}, \\varepsilon\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})}\\left[\\log{p_{model}(x+q_x(\\varepsilon))} - \\log{p(\\varepsilon)\\cdot\\left|dq_x/d\\varepsilon\\right|^{-1}}\\right]\n",
        "\\end{split}\n",
        "$$\n",
        "Получается, что модель с вариационной деквантизацией является нижней оценкой исходной задачи максимизации логарифма функции правдоподобия\n",
        "$$\n",
        "\\mathbb{E}_{x\\sim P_{data}, \\varepsilon\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})}\\left[\\log{p_{model}(x+q_x(\\varepsilon))} - \\log{p(\\varepsilon)\\cdot\\left|dq_x/d\\varepsilon\\right|^{-1}}\\right] \\leq \\mathbb{E}_{x\\sim P_{data}}[\\log{P_{model}(x)}]\n",
        "$$\n",
        "Заметим, что оценка равномерной деквантизацией — частный случай вариационной при условии, что шум не зависит от исходных данных.\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/var_deq_flow.png?raw=true\" alt=\"Пример работы нормализующего потока с вариационной деквантизацией\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с вариационной деквантизацией.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thQYT5Xg4ZDO"
      },
      "source": [
        "<a name=\"also\"></a>\n",
        "## Другие потоки и их свойства\n",
        "\n",
        "Далее рассмотрим потоки, предложенные в модели Glow. Краткое описание можно посмотреть на каринке ниже"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<figure>\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/glow-table.png\" alt=\"Краткое описание введенных потоков модели Glow\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с вариационной деквантизацией.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Все эти потоки использются в общем блоке следующего вида\n",
        "<figure>\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/one-glow-step.png\" alt=\"Поток модели Glow с использованием трех потоков\" style=\"width:50%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с вариационной деквантизацией.</figcaption>\n",
        "</figure>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOB8ZkIz7cMd0dbayjbbni0",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
