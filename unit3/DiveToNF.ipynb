{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB8ZkIz7cMd0dbayjbbni0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dive to Normalizing Flows\n",
        "---"
      ],
      "metadata": {
        "id": "l9vDGpenj4p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вывод нормализующих потоков\n",
        "\n",
        "Давайте посмотрим еще раз на этот рисунок и попробуем расписать итерационный процесс\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png\" alt=\"Примеры схем генеративных моделй\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Иллюстрация преобразования нормально распределенного z0 в zK из реального  распределения.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Положим, существуют\n",
        "$$\n",
        "z_{i-1}\\sim p_{i-1}(z_{i-1}), ~ z_i = f_i(z_{i-1})\\Leftrightarrow z_{i-1} = f^{-1}_i(z_i)\n",
        "$$\n",
        "\n",
        "где $z_{i-1}$ — вектор, распределенный как $p_{i-1}$, а $f_i$ — функция, отображающая $z_{i-1}$ в $z_i$. Также существует обратная к $f$ функция $f^{-1}$, которая отображает в обратную сторону.\n",
        "\n",
        "Переход по графу на рисунке можно расписать как\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "p_i(z_i) & \\stackrel{(1)}{=} p_{i-1}(f_i^{-1}(z_i))\\left|\\det\\left(\\dfrac{df^{-1}_i(z_i)}{dz_i}\\right)\\right| \\\\\n",
        "& \\stackrel{(2)}{=} p_{i-1}(z_{i-1})\\left|\\det\\left(\\dfrac{df_i(z_{i-1})}{dz_{i-1}}\\right)^{-1}\\right| \\\\\n",
        "& \\stackrel{(3)}{=} p_{i-1}(z_{i-1})\\left|\\det\\left(\\dfrac{df_i(z_{i-1})}{dz_{i-1}}\\right)\\right|^{-1} \\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Разберем подробнее каждый переход\n",
        "\n",
        "1. Получено из теоремы о замене переменной для многомерных случайных величин.\n",
        "2. Следует из теоремы об обратимых функциях.\n",
        "3. Для обратимых матриц существует свойство определителя $\\det(A^{-1}) = \\det(A)^{-1}$."
      ],
      "metadata": {
        "id": "rkFlAPKoj9Ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теорема о замене переменной\n",
        "\n",
        "Положим существует переменная $z$ распределенная как $z\\sim\\pi(z)$, при помощи которой мы хотим получить новую независимую переменную, используя преобразование $x=f(z)$. Функцию $f$ выберем так, что $\\exists f^{-1}: z=f^{-1}(x)$. Вопрос, каким будет распределение $x\\sim p(x)$?\n",
        "\n",
        "Заметим, что если $f(z)$ — дифференцируема на некоторой области и кусочно-монотонна, тогда по свойству преобразования случайной величины\n",
        "$$\n",
        "p(x) = \\pi(z)\\left|\\dfrac{dz}{dx}\\right|\n",
        "$$\n",
        "отсюда получаем, что\n",
        "$$\n",
        "\\pi(z)\\left|\\dfrac{dz}{dx}\\right| = \\pi(f^{-1}(x))\\left|\\dfrac{df^{-1}}{dx}\\right| = \\pi(f^{-1}(x))|(f^{-1})^{'}(x)|\n",
        "$$\n",
        "\n",
        "В случае многомерной случайной величины выражение схоже\n",
        "\n",
        "$$\n",
        "\\vec{z}\\sim\\pi(\\vec{z}), ~\\vec{x}=f(\\vec{z}),~\\vec{z}=f^{-1}(\\vec{x})\n",
        "$$\n",
        "\n",
        "$$\n",
        "p(\\vec{x}) = \\pi(\\vec{z})\\left|det\\dfrac{d\\vec{z}}{d\\vec{x}}\\right| = \\pi(f^{-1}(\\vec{x}))\\left|det\\dfrac{df^{-1}(\\vec{x})}{d\\vec{x}}\\right|\n",
        "$$\n",
        "\n",
        "где $det\\frac{df^{-1}(\\vec{x})}{d\\vec{x}}$ — определитель Якобиана функции $f^{-1}$."
      ],
      "metadata": {
        "id": "WX6KRI5WITfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теорема об обратных функциях\n",
        "Теорема об обратных функциях говорит о том, что если $y = f(x),~x=f^{-1}(y)$, то\n",
        "$$\n",
        "\\dfrac{df^{-1}(y)}{dy} = \\dfrac{dx}{dy}=\\left(\\dfrac{dy}{dx}\\right)^{-1} = \\left(\\dfrac{df(x)}{dx}\\right)^{-1}\n",
        "$$"
      ],
      "metadata": {
        "id": "3Vw9XephIWC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Свойство определителя обратной матрицы\n",
        "$$\n",
        "A\\cdot A^{-1} = I \\Rightarrow \\det(A\\cdot A^{-1}) = \\det(I)  \\Rightarrow \\det(A\\cdot A^{-1}) = 1 \\\\\n",
        "\\det(A)\\cdot \\det(A^{-1}) = 1 \\Rightarrow \\det(A^{-1}) = \\dfrac{1}{\\det(A)} = \\det(A)^{-1}\n",
        "$$"
      ],
      "metadata": {
        "id": "KHeutqrGIXSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Переход к суперпозиции\n",
        "\n",
        "Таким образом получить из нормально распределенных $z_0$ объекты $x=z_K$ можно через суперпозицию всех функций $f_i$ как\n",
        "$$\n",
        "x=z_K=f_K\\circ f_{K-1}\\circ\\cdots\\circ f_1(z_0)\n",
        "$$\n",
        "\n",
        "Тогда трансформация логарифма плотности распределения\n",
        "$$\n",
        "\\begin{split}\n",
        "\\log p(x) = \\log\\pi_K(z_K) & = \\log\\pi_{K-1}(z_{K-1})-\\log\\left|\\det\\dfrac{df_K}{dz_{K-1}}\\right| \\\\\n",
        "& = \\log\\pi_{K-2}(z_{K-2}) -\\log\\left|\\det\\dfrac{df_{K-1}}{dz_{K-2}}\\right| -\\log\\left|\\det\\dfrac{df_K}{dz_{K-1}}\\right| \\\\\n",
        "& = \\cdots \\\\\n",
        "& = \\log\\pi_0(z_0) - \\sum\\limits_{i=1}^K\\log\\left|\\det\\dfrac{df_i}{dz_{i-1}}\\right|\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Эта функция является логарифмом правдоподобия и может быть использована в качестве обучения. Однако, так как оптимизационная задача с функцией правдоподобия — задача максимизации, возьмем отрицательную функцию логаифма правдоподобия (negative log likelyhood loss), которую и будем минимизировать.\n",
        "$$\n",
        "NLLLoss(x) = - \\log\\pi_0(z_0) + \\sum\\limits_{i=1}^K\\log\\left|\\det\\dfrac{df_i}{dz_{i-1}}\\right|\n",
        "$$\n",
        "\n",
        "Здесь преобразование $z_i=f_i(z_{i-1})$ называется потоком (flow), а полная последовательность из преобразований называется нормализующим потоком (normalizing flow).\n",
        "\n",
        "Стоит уточнить, что на функции $f_i$ накладывается ряд требований:\n",
        "\n",
        "1. $f_i$ должна быть легко обратимой функцией\n",
        "2. Якобиан $\\frac{df_i}{dz_{i-1}}$ должен быть легко вычислимым"
      ],
      "metadata": {
        "id": "hBsIZnlR2T5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Planar Flow\n",
        "\n",
        "Рассмотрим первую трансформацию, использующуюся в качестве потока — планарный поток. Это семейство функций вида\n",
        "$$\n",
        "f(x) = uh(w^Tx + b)\n",
        "$$\n",
        "где\n",
        "- $h$ — функция активации\n",
        "- $⟨⋅, ⋅⟩$ — cкалярное произведение\n",
        "- $u, w, b$ — обучаемые параметры\n",
        "\n",
        "### Обратимость\n",
        "\n",
        "Проблема этого потока заключается в том, что не каждая функция этого семейства может быть обратима. Для обратимости требуется чтобы\n",
        "1. $h(x) = \\tanh(x)$\n",
        "2. $w^Tu \\geq -1$\n",
        "\n",
        "Второе условие может быть достигнуто при посощи обновления вектора $u$ так, что\n",
        "$$\n",
        "\\hat{u}(w, u) = u + \\left[m(w^Tu) - w^Tu\\right] \\frac{w}{||w||_2}\n",
        "$$\n",
        "где $m(x) = -1+\\log{(1+e^x)}$.\n",
        "\n",
        "### Определитель Якобиана\n",
        "Определитель Якобиана можно записать как\n",
        "$$\n",
        "\\det{J} = \\det{\\left(I + u\\psi(x)^T\\right)} = 1 + u^T\\psi(x)\n",
        "$$\n",
        "где\n",
        "- $I$ — единичная матрица\n",
        "- $\\psi(x) = h'(w^Tx + b)$\n",
        "- $h'$ — произвлдная от $h$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBP6KDSj6YxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Affine Coupling Layer\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i = x_i,~\\forall i=1,2,\\dots,d \\\\\n",
        "y_i = x_i \\odot \\exp(s(x_{i-d})) + t(x_{i-d}),~\\forall i=d+1,\\dots,D\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "где $s(\\cdot)$ — масштабирование, $t(\\cdot)$ — смещение, а $\\odot$ — поэлементное умножение.\n",
        "\n",
        "Теперь убедимся, что это преобразование подходит нам по критериям функции $f$."
      ],
      "metadata": {
        "id": "yMbH8flhj9D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обратимость\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i = x_i~,\\\\\n",
        "y_i = x_i \\odot \\exp(s(x_{i-d})) + t(x_{i-d})~;\n",
        "\\end{cases} \\Leftrightarrow\n",
        "\\begin{cases}\n",
        "x_i = y_i~,\\\\\n",
        "x_i = (y_i - t(x_{i-d})) \\odot \\exp(-s(x_{i-d}))~.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Так как преобразование линейно, оно не требует поиска обратных функций к $s(\\cdot)$ и $t(\\cdot)$, а значит, что вычисляется быстро."
      ],
      "metadata": {
        "id": "C3M51EAJICr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определитель Якобиана\n",
        "\n",
        "Легко заметить, что Якобиан функции имеет вид нижней треугольной матрицы\n",
        "\n",
        "$$\n",
        "J =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{I} & \\mathbf{0} \\\\\n",
        "\\frac{\\partial y_{d+1:D}}{\\partial x^T_{1:d}} & \\text{diag}(\\exp[s(x_{1:d})])\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "где\n",
        "- $\\mathbf{I}_d$ — единичная матрица, полученная из $\\frac{\\partial y_{1:d}}{\\partial x_{1:d}} = \\frac{\\partial x_{1:d}}{\\partial x_{1:d}}$ размерности $d\\times d$\n",
        "- $\\mathbf{0}$ — нулевая матрица, полученная из $\\frac{\\partial y_{1:d}}{\\partial x_{d+1:D}} = \\frac{\\partial x_{1:d}}{\\partial x_{d+1:D}}$ размерности $d\\times (D-d)$\n",
        "- $\\frac{\\partial y_{d+1:D}}{\\partial x^T_{1:d}}$ — минор Якобиана размерности $(D-d)\\times d$\n",
        "- $\\text{diag}(\\exp[s(x_{1:d})])$ — диагональная матрица, где на диагонали значения $\\exp[s(x_{1:d})]$, а вне — нули. Размерность это матрицы $(D-d)\\times(D-d)$\n",
        "\n",
        "Можно заметить, что такая матрица может существовать только при $d = \\frac{D}{2}$.\n",
        "\n",
        "Так как матрица имеет вид нижней треугольно, ее определитель считается по диагонали, а значит, что левую нижнюю часть матрицы знать не обязательно. Тогда определитель Якобиана вычисляется как\n",
        "\n",
        "$$\n",
        "\\det(J) = \\prod\\limits_{j=1}^{D-d}\\exp(s(x_{1:d}))_j = \\exp\\left(\\sum\\limits_{j=1}^{D-d}s(x_{1:d})_j\\right)\n",
        "$$\n",
        "\n",
        "Так как вычисление определителя сводится к вычислению экспоненты суммы выходов $s(\\cdot)$, считаться определитель будет быстро.\n",
        "\n",
        "Так как к функциям $s(\\cdot)$ и $t(\\cdot)$ не выставлено никаких сложных требований, в их качестве можно использовать функции класса нейронных сетей."
      ],
      "metadata": {
        "id": "jMl8OpSWIFN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование маски\n",
        "\n",
        "Разбиаение преобразований на тривиальные и афинные в Affine Coupling Layer можно представить как бинарную маску $b$. Тогда преобразование можно переписать как\n",
        "$$\n",
        "y = b ⊙ x + (1-b) ⊙ (x ⊙ \\exp(s(b ⊙ x)) + t(b ⊙ x))\n",
        "$$\n",
        "\n",
        "Тогда обратное преобразование получается следующим\n",
        "$$\n",
        "x = b ⊙ y + (1 - b) ⊙ ((y - t(b ⊙ y)) ⊙ \\exp(-s(b ⊙ y)))\n",
        "$$\n",
        "\n",
        "Так как маска влияет на порядок строк и столбцов в матрице Якобиана нужно сказать, что перестановка строк или столбцов в определители влияет только на знак определителя матрицы, а так как для вычисления плотности распределения мы используем модуль определителя, перестановка строк и столбцов на вычисление плотности не влияет.\n",
        "\n",
        "Таким образом мы можем свести вычисление оперделителя при использовании маски к базовому определению афинного потока выше, переставляя строки матрицы.\n",
        "\n",
        "Сам определитель Якобиана можно переписать как\n",
        "$$\n",
        "\\det(J) = \\prod(1-b) ⊙ \\exp(s(b \\odot x)) = \\exp\\left(\\sum(1-b) ⊙ s(b \\odot x)\\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "ZqIIFQm_IFBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Деквантизация\n",
        "\n",
        "Нормализующие потоки полагаются на правило изменения переменных, которое естественным образом определено в непрерывном пространстве. Применение потоков непосредственно к дискретным данным приводит к нежелательным моделям плотности.\n",
        "\n",
        "Так на рисунке ниже иллюстрируется как нормализующий поток пытается преобразовать нормальное распределение в дискретное распределение прдставленное двумя точками — $(0, 1),~(1, 0)$. Видно, что модель в итоге приводит распределение к некоторой \"линии\", что никак не сходится с требуемым распределением. Есть несколько вариантов решения проблемы:\n",
        "- Решать задачу для дискретного распределения\n",
        "- Придумать поток, который бы приводил диксретное распределение к непрерывному и наоборот\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/simple_flow.png?raw=true\" alt=\"Пример работы нормализующего потока без квантизации с дискретным распределением\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока без квантизации с дискретным распределением.</figcaption>\n",
        "</figure>\n",
        "\n"
      ],
      "metadata": {
        "id": "WP4Sdc4Nj8-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ТУТ РАСПИСАТЬ ИЗ СТАТЬИ ВСЕ"
      ],
      "metadata": {
        "id": "1mMYle3o17l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обычная деквантизация\n",
        "\n",
        "Положим, что у нас есть дискретная переменная $x$ с плотностью $p(x)$. Скажем, что шум $u\\sim U(0, 1)^D$ — аддитивный шум из многомерного равномерного распределения и есть результат добавления шума $y = x + u$. Для новой величины плотность может быть представлена как\n",
        "$$\n",
        "p(y) = \\int p(x+u)du = \\int\\dfrac{q(u|x)}{q(u|x)}p(x+u)du=\\int\\dfrac{p(x+u)}{q(u|x)}q(u|x)du = \\mathbb{E}_{u\\sim q(u|x)}\\left[\\dfrac{p(x+u)}{q(u|x)}\\right]\n",
        "$$\n",
        "\n",
        "где $q(u|x)$ — на самом деле является плотностью $U(0, 1)^D$, так как $u$ не зависит от $x$, то есть $q(u|x) = q(u) = U(0, 1)^D$. Тогда мы можем переписать плотность как\n",
        "$$\n",
        "p(y) = \\mathbb{E}_{u\\sim U(0, 1)^D}\\left[p(x+u)\\right]\n",
        "$$\n",
        "\n",
        "Также, так как дискретное распределение может быть в диапазоне сильно отличном от диапазона нормального распределения принято использовать обратную к сигмоиде функцию $\\sigma^{-1}(y)$.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/dequant_flow.png?raw=true\" alt=\"Пример работы нормализующего потока с простой квантизацией.\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с простой квантизацией.</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "BSQ4CM_zIeOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вариационная деквантизация\n",
        "\n",
        "По предыдущему примеру можно видеть, что деквантизация с использованием нормального распределения несет за собой некоторые издержки. Так, например, довольно сложно потоками получить нормальное многомерное распределение, что видно на примере выше. Также нормальное распределение не учитывает изходную частотность элементов распределения, что тоже делает генерацию некорректной в случаях с несбалансированными распределениями.\n",
        "\n",
        "Что если вместо заданного распределения шума мы обучим дополнительный поток прогназировать шум исходя из природы исходных данных?\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/sswt/dive2gai/blob/unit3/.github/images/var_deq_flow.png?raw=true\" alt=\"Пример работы нормализующего потока с вариационной деквантизацией\" style=\"width:100%\">\n",
        "<figcaption align = \"center\">Пример работы нормализующего потока с вариационной деквантизацией.</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "SIVjD-rPIffP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgfWpljBH6iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKI7_guJxhPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Другие потоки и их свойства"
      ],
      "metadata": {
        "id": "thQYT5Xg4ZDO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpYK9qORwVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5wxXlWRwVvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Полезные ссылки\n",
        "\n",
        "### Статьи\n",
        "\n",
        "- [Variational Inference with Normalizing Flows](https://arxiv.org/pdf/1505.05770v6.pdf)\n",
        "- [Learning Discrete Distributions by Dequantization](https://arxiv.org/pdf/2001.11235.pdf)\n",
        "- [DENSITY ESTIMATION USING REAL NVP](https://arxiv.org/pdf/1605.08803v3.pdf)\n",
        "- [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/pdf/1807.03039v2.pdf)\n",
        "- [Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design](https://arxiv.org/pdf/1902.00275.pdf)"
      ],
      "metadata": {
        "id": "2TgNSzgKSRpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcI_4JFWSgX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}