{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21fc6c2-f439-4087-a1b3-ebecab156882",
   "metadata": {},
   "source": [
    "# Что нам нужно?\n",
    "\n",
    "* Метод, при помощи которого мы будем генерировать какое-то не существующее до этого изображение ([forvard/reverse diffusion](https://arxiv.org/abs/2006.11239))\n",
    "* Способ соединить вместе текст и изображение ([text-image representation model](https://arxiv.org/abs/2103.00020))\n",
    "* Что-то для сжатия изображений (SD is a LDM)([autoencoder](https://arxiv.org/abs/2112.10752))\n",
    "* Способ задать направление для генерации ([U-net + attention](https://arxiv.org/abs/2112.10752))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52e9f0-1d2f-4d7c-ba89-b497ef1f38cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](./data/breaf_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d2c63-f5cf-4dea-bde1-a74f1e153f3c",
   "metadata": {},
   "source": [
    "## Метод, при помощи которого мы будем генерировать какое-то не существующее до этого изображение\n",
    "В предыдущем докладе данного юнита мы подробно останавливались на том, что такое диффузия, и какой она бывает.\n",
    "В связи с этим далее будет картинка-плэйсхолдер, которая должна освежить память слушателя. (A если этого не произойдёт, к предыдущему докладу всегда можно [вернуться](./data/from_scratch.ipynb))\n",
    "\n",
    "![](./data/ddpm_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0558544-0a54-41fe-9214-137310023acb",
   "metadata": {},
   "source": [
    "## Способ соединить вместе текст и изображение\n",
    "CLIP - Contrastive Language-Image Pre-training\n",
    "![](./data/CLIP_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd0b74-9b14-4d65-8f40-58e7e09954ae",
   "metadata": {},
   "source": [
    "(1) В первой части схемы показан принцип дообучения энкодера для изображений (например, ResNet50 или ViT) и энкодера для текстов (GPT-like трансформер) в Contrastive стратегии для батча размера N. Из-за того, что используемое расстояние не симметрично, \"расталкивать\" представления нужно в обе стороны, что выливается в соответствие поставленной задачи минимизации следующей функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8f15d-299c-410b-93fd-cd29889dbd4a",
   "metadata": {},
   "source": [
    "$$\\Large \\ell^{(I\\rightarrow T)}_i = -log\\frac{e^{\\frac{\\langle I_i, T_i\\rangle}{τ}}}{\\sum_{k=1}^{N} e^{\\frac{\\langle I_i, T_k\\rangle}{τ}}};$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\Large \\ell^{(T\\rightarrow I)}_i = -log\\frac{e^{\\frac{\\langle T_i, I_i\\rangle}{τ}}}{\\sum_{k=1}^{N} e^{\\frac{\\langle T_i, I_k\\rangle}{τ}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028ba38-bfbc-438d-9d2a-0c894caab87f",
   "metadata": {},
   "source": [
    "$$\\Large \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (\\lambda \\ell^{(I\\rightarrow T)}_i + (1 - \\lambda) \\ell^{(T\\rightarrow I)}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8053f9-63c3-420a-b3e3-740f04739f07",
   "metadata": {},
   "source": [
    "(2) - (3) Во второй и третьей частях показано, как полученный результат можно использовать для zero-shot предсказаний на своём датасете. То, почему это названо созданием линейного классификатора можно представить, если принять  $T_1 ... T_n$ за наборы весов нейронов линейного слоя на N нейронов, а представление $I_1$ за вход для классификатора.\n",
    "\n",
    "<img src=\"./data/perf_clip.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82475c6d-9ac9-4c1c-83f3-10d894b941b9",
   "metadata": {},
   "source": [
    "### Особенности и ограничения CLIP\n",
    "\n",
    "* Датасет для обучения 400mil пар картинка + текстовое описание\n",
    "* По данным авторов, версия Zero-Shot CLIP, упомянутая в оригинальной статье, достигает near SOTA (SOTA как правило получено при помощи supervised техник) результатов на большинстве датасетов в supervised задачах. Для того, чтобы приблизиться к SOTA, нужно всего лишь х1000 времени и данных, что, однако __\"infeasible to train with current hardware\"__\n",
    "* Модель сталкивается с трудностями генерализации, когда видит что-то, чего не было в обучающей выборке __\"While zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we’ve observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. ... CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP.\"__\n",
    "* Модель не подходит для генерации текстового описания изображения и получила некоторый Social Bias из обучающей выборки. __\"CLIP is trained on text paired with images on the internet.These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases.\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feafc7-ddaa-4e1f-89e1-37f4577aa4cd",
   "metadata": {},
   "source": [
    "### Где может быть использован в LDM\n",
    "\n",
    "* Генерация представления для текста (text-encoder)\n",
    "* Генерация представления для изображения (image-encoder)\n",
    "* Ранжирование изображений (DALLE использует CLIP также для упорядочивания сгенерированных изображений перед тем, как отдать их пользователю)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a6ec3-a36f-4120-bbfa-3404b82f01fb",
   "metadata": {},
   "source": [
    "## Что-то для сжатия изображений\n",
    "![](./data/шакал.jpeg)\n",
    "\n",
    "\n",
    "Пространство пикселей является пространством очень высокой размерности (каждый пиксель == измерение), в нём очень долго и затратно работать, при этом достаточно малое число пикселей несет действительно важную информацию\n",
    "\n",
    "Метод (может варьироваться в разных имплементациях LDM, как и почти любой другой компонент): строим отображение в пространство меньшей размерности, в котором, тем не менее, изображение не лишается своих свойств - представление в новом пространстве всё ещё является изображением в привычном для нас понимании этого слова. Таким образом процедура представляет собой получение скетча, который сохраняет в себе максимум информации из входа\n",
    "\n",
    "Характерным моментом при обучении является подбор размерности, нужно соблюсти баланс между желанием максимального сжатия/ускорения/удешевления расчетов и сохранением детализации в получаемом пространстве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee90c1-cfd3-4773-8f00-6d9d6ba8de68",
   "metadata": {},
   "source": [
    "Ниже приведен график, на котором отражена динамика обучения моделей с различным значением downsampling factor, который представляет собой следующее отношение:\n",
    "\n",
    "$$x ∈ R^{H×W×3};   z ∈ R^{h×w×c}$$\n",
    "\n",
    "\n",
    "$$f = H/h = W/w$$\n",
    "\n",
    "![](./data/spatial_downsampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959536a-bb21-4283-b8ba-6c43b6083631",
   "metadata": {},
   "source": [
    "## Способ задать направление для генерации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd05558-f253-4f2e-ac44-d63642ca516f",
   "metadata": {},
   "source": [
    "### Внимание-внимание!\n",
    "Слайды позаимствованы из [презентации](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/stable-diffusion-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216ad73-2473-4256-87b5-90518794da00",
   "metadata": {},
   "source": [
    "![](./data/slide_1.png)\n",
    "![](./data/slide_2.png)\n",
    "![](./data/slide_3.png)\n",
    "![](./data/slide_4.png)\n",
    "![](./data/slide_5.png)\n",
    "![](./data/slide_6.png)\n",
    "![](./data/slide_7.png)\n",
    "![](./data/slide_8.png)\n",
    "![](./data/slide_9.png)\n",
    "![](./data/slide_10.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
