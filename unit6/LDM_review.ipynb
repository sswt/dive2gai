{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a7dcbb-2c36-40e5-921a-eddf38106c7a",
   "metadata": {},
   "source": [
    "![](./data/mdjn.png)\n",
    "input: multimodal diffusion, ai, latent space, text input, image output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21fc6c2-f439-4087-a1b3-ebecab156882",
   "metadata": {},
   "source": [
    "# Что нам нужно?\n",
    "\n",
    "* Метод, при помощи которого мы будем генерировать какое-то не существующее до этого изображение ([forvard/reverse diffusion](https://arxiv.org/abs/2006.11239))\n",
    "* Способ соединить вместе текст и изображение ([text-image representation model](https://arxiv.org/abs/2103.00020))\n",
    "* Что-то для сжатия изображений (SD is a LDM)([autoencoder](https://arxiv.org/abs/2112.10752))\n",
    "* Способ задать направление для генерации ([U-net + attention](https://arxiv.org/abs/2112.10752))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52e9f0-1d2f-4d7c-ba89-b497ef1f38cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](./data/breaf_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d2c63-f5cf-4dea-bde1-a74f1e153f3c",
   "metadata": {},
   "source": [
    "## Метод, при помощи которого мы будем генерировать какое-то не существующее до этого изображение\n",
    "В предыдущем докладе данного юнита мы подробно останавливались на том, что такое диффузия, и какой она бывает.\n",
    "В связи с этим далее будет картинка-плэйсхолдер, которая должна освежить память слушателя. (A если этого не произойдёт, к предыдущему докладу всегда можно [вернуться](./data/from_scratch.ipynb))\n",
    "\n",
    "![](./data/ddpm_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0558544-0a54-41fe-9214-137310023acb",
   "metadata": {},
   "source": [
    "## Способ соединить вместе текст и изображение\n",
    "CLIP - Contrastive Language-Image Pre-training\n",
    "![](./data/CLIP_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd0b74-9b14-4d65-8f40-58e7e09954ae",
   "metadata": {},
   "source": [
    "(1) В первой части схемы показан принцип дообучения энкодера для изображений (например, ResNet50 или ViT) и энкодера для текстов (GPT-like трансформер) в Contrastive стратегии для батча размера N. Из-за того, что используемое расстояние не симметрично, \"расталкивать\" представления нужно в обе стороны, что выливается в соответствие поставленной задачи минимизации следующей функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8f15d-299c-410b-93fd-cd29889dbd4a",
   "metadata": {},
   "source": [
    "$$\\Large \\ell^{(I\\rightarrow T)}_i = -log\\frac{e^{\\frac{\\langle I_i, T_i\\rangle}{τ}}}{\\sum_{k=1}^{N} e^{\\frac{\\langle I_i, T_k\\rangle}{τ}}};$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\Large \\ell^{(T\\rightarrow I)}_i = -log\\frac{e^{\\frac{\\langle T_i, I_i\\rangle}{τ}}}{\\sum_{k=1}^{N} e^{\\frac{\\langle T_i, I_k\\rangle}{τ}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028ba38-bfbc-438d-9d2a-0c894caab87f",
   "metadata": {},
   "source": [
    "$$\\Large \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (\\lambda \\ell^{(I\\rightarrow T)}_i + (1 - \\lambda) \\ell^{(T\\rightarrow I)}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8053f9-63c3-420a-b3e3-740f04739f07",
   "metadata": {},
   "source": [
    "(2) - (3) Во второй и третьей частях показано, как полученный результат можно использовать для zero-shot предсказаний на своём датасете. То, почему это названо созданием линейного классификатора можно представить, если принять  $T_1 ... T_n$ за наборы весов нейронов линейного слоя на N нейронов, а представление $I_1$ за вход для классификатора.\n",
    "\n",
    "<img src=\"./data/perf_clip.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82475c6d-9ac9-4c1c-83f3-10d894b941b9",
   "metadata": {},
   "source": [
    "### Особенности и ограничения CLIP\n",
    "\n",
    "* Датасет для обучения 400mil пар картинка + текстовое описание\n",
    "* По данным авторов, версия Zero-Shot CLIP, упомянутая в оригинальной статье, достигает near SOTA (SOTA как правило получено при помощи supervised техник) результатов на большинстве датасетов в supervised задачах. Для того, чтобы приблизиться к SOTA, нужно всего лишь х1000 времени и данных, что, однако __\"infeasible to train with current hardware\"__\n",
    "* Модель сталкивается с трудностями генерализации, когда видит что-то, чего не было в обучающей выборке __\"While zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we’ve observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. ... CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP.\"__\n",
    "* Модель не подходит для генерации текстового описания изображения и получила некоторый Social Bias из обучающей выборки. __\"CLIP is trained on text paired with images on the internet.These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases.\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feafc7-ddaa-4e1f-89e1-37f4577aa4cd",
   "metadata": {},
   "source": [
    "### Где может быть использован в LDM\n",
    "\n",
    "* Генерация представления для текста (text-encoder)\n",
    "* Генерация представления для изображения (image-encoder)\n",
    "* Ранжирование изображений (DALLE использует CLIP также для упорядочивания сгенерированных изображений перед тем, как отдать их пользователю)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a6ec3-a36f-4120-bbfa-3404b82f01fb",
   "metadata": {},
   "source": [
    "## Что-то для сжатия изображений\n",
    "![](./data/шакал.jpeg)\n",
    "\n",
    "\n",
    "Пространство пикселей является пространством очень высокой размерности (каждый пиксель == измерение), в нём очень долго и затратно работать, при этом достаточно малое число пикселей несет действительно важную информацию\n",
    "\n",
    "Метод (может варьироваться в разных имплементациях LDM, как и почти любой другой компонент): строим отображение в пространство меньшей размерности, в котором, тем не менее, изображение не лишается своих свойств - представление в новом пространстве всё ещё является изображением в привычном для нас понимании этого слова. Таким образом процедура представляет собой получение скетча, который сохраняет в себе максимум информации из входа\n",
    "\n",
    "Характерным моментом при обучении является подбор размерности, нужно соблюсти баланс между желанием максимального сжатия/ускорения/удешевления расчетов и сохранением детализации в получаемом пространстве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee90c1-cfd3-4773-8f00-6d9d6ba8de68",
   "metadata": {},
   "source": [
    "Ниже приведен график, на котором отражена динамика обучения моделей с различным значением downsampling factor, который представляет собой следующее отношение:\n",
    "\n",
    "$$x ∈ R^{H×W×3};   z ∈ R^{h×w×c}$$\n",
    "\n",
    "\n",
    "$$f = H/h = W/w$$\n",
    "\n",
    "![](./data/spatial_downsampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959536a-bb21-4283-b8ba-6c43b6083631",
   "metadata": {},
   "source": [
    "## Способ задать направление для генерации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd05558-f253-4f2e-ac44-d63642ca516f",
   "metadata": {},
   "source": [
    "### Внимание-внимание!\n",
    "Слайды позаимствованы из [презентации](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/stable-diffusion-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216ad73-2473-4256-87b5-90518794da00",
   "metadata": {},
   "source": [
    "![](./data/slide_1.png)\n",
    "![](./data/slide_2.png)\n",
    "![](./data/slide_3.png)\n",
    "![](./data/slide_4.png)\n",
    "![](./data/slide_5.png)\n",
    "![](./data/slide_6.png)\n",
    "![](./data/slide_7.png)\n",
    "![](./data/slide_8.png)\n",
    "![](./data/slide_9.png)\n",
    "![](./data/slide_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889c43c-0354-4393-8ba6-a2febbf6a015",
   "metadata": {},
   "source": [
    "## Ещё немного практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24b5e6-11f6-47d3-8b86-3b67ed66ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a098d-7e3e-479f-b694-d32fcf65c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33d1c2-e066-46ed-95e8-ca69bb276172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfe8d4-909b-45ec-90c2-a50d7eb2f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10159eec-bfea-42f0-a5b9-40ab401793c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=10, class_emb_size=4):\n",
    "    super().__init__()\n",
    "\n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=28,           # the target image resolution\n",
    "        in_channels=1 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=1,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(32, 64, 64),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "\n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the UNet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5e885-81fc-405e-984f-65b4f8455d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Our network\n",
    "net = ClassConditionedUnet().to('cuda')\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "\n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to('cuda') * 2 - 1 # Data on the GPU (mapped to (-1, 1))\n",
    "        y = y.to('cuda')\n",
    "        noise = torch.randn_like(x)\n",
    "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to('cuda')\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, noise) # How close is the output to the noise\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print out the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403fdd5-26fa-4486-82b5-148170f59e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare random x to start from, plus some desired labels y\n",
    "x = torch.randn(80, 1, 28, 28).to('cuda')\n",
    "y = torch.tensor([[i]*8 for i in range(10)]).flatten().to('cuda')\n",
    "\n",
    "# Sampling loop\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
    "\n",
    "    # Update sample with step\n",
    "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "# Show the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926cab7-565e-45b3-84bf-95f22dd3d283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba09b32f-ea13-40e9-8790-49edadc74e96",
   "metadata": {},
   "source": [
    "### Если очень хочется прикоснуться к прекрасному"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81f0d0-5d4c-47c5-848b-eb71ffbb52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadda67-dd4b-4420-b663-fc75031e991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ece1f-d241-455f-a62b-5e6d93667442",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    use_auth_token=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640613cb-b16e-4df3-8deb-cf443a33f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"LOTR landscape cinematic 4k hires\"\n",
    "image = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115792d-4756-4cf5-a702-570fdfe7e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
