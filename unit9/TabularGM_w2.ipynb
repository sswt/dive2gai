{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Табличные генеративные модели\n",
        "\n",
        "Цели:\n",
        "\n",
        "* рассмотреть модели на основе LLM\n",
        "* рассмотреть модели на основе диффузии\n",
        "* познакомиться со списком работ по теме\n",
        "\n",
        "Содержание:\n",
        "\n",
        "* Некоторые работы по теме\n",
        "* Модели на основе LLM\n",
        "  * GReaT\n",
        "  * TabuLa\n",
        "* Модели на основе дифузии\n",
        "  * TabDDPM\n",
        "    * Background\n",
        "    * Модель\n",
        "  * TabSyn\n",
        "    * Преамбула\n",
        "    * Tokenizer\n",
        "    * VAE\n",
        "    * Score-based генеративная модель в латентном пространстве\n",
        "    * Missing value imputation\n",
        "    * Сравнение с другими моделями\n",
        "* Заключение\n"
      ],
      "metadata": {
        "id": "M5Ea5yqbvHRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Некоторые работы по теме"
      ],
      "metadata": {
        "id": "7ZESZ0exvU-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN\n",
        "\n",
        "* TableGAN, 06.2018\n",
        "    * [Data Synthesis based on Generative Adversarial Networks](https://arxiv.org/pdf/1806.03384.pdf)\n",
        "    * [code](https://github.com/mahmoodm2/tableGAN)\n",
        "* CTGAN, 07.2019\n",
        "    * [Conditional Tabular GAN](https://arxiv.org/pdf/1907.00503.pdf)\n",
        "    * [original code (SDV)](https://github.com/sdv-dev/CTGAN), [Gretel's ACTGAN](https://synthetics.docs.gretel.ai/en/stable/models/actgan.html)\n",
        "* CTAB-GAN, 02.2021\n",
        "    * [CTAB-GAN: Effective Table Data Synthesizing](https://arxiv.org/pdf/2102.08369.pdf)\n",
        "    * [code](https://github.com/Team-TUD/CTAB-GAN/blob/main/Experiment_Script_Adult.ipynb)\n",
        "* CTAB-GAN+, 04.2022\n",
        "    * [CTAB-GAN+: Enhancing Tabular Data Synthesis](https://arxiv.org/pdf/2204.00401.pdf)\n",
        "    * [code](https://github.com/Team-TUD/CTAB-GAN-Plus/blob/main/Experiment_Script_Adult.ipynb)\n",
        "\n",
        "### VAE\n",
        "\n",
        "* GOGGLE, 02.2023\n",
        "\n",
        "  VAE с использованием графовых сетей в качестве энкодера и декодера, явно моделировали взаимосвязи между столбцами\n",
        "    * [GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure](https://openreview.net/pdf?id=fPVRcJqspu)\n",
        "    * [code](https://github.com/vanderschaarlab/GOGGLE)\n",
        "\n",
        "### Autoregressive (на основе LLM)\n",
        "\n",
        "* TabFormer, 11.2020\n",
        "  \n",
        "  Вводится синтетический датасет транзакция кредитных карт. Эмбеддинги строятся иерархически, сначала для строк, а затем для последовательностей транзакций пользователя окном длины T. Поверх этого пробовали обучать две модели - TabBERT и TabGPT. Для вещественных признаков применялась квантизация, что позволяло строить словарь для каждой колонки, как в NLP, но происходила потеря информации.\n",
        "    * [Tabular Transformers for Modeling Multivariate Time Series](https://arxiv.org/pdf/2011.01843.pdf)\n",
        "    * [code (IBM/TabFormer)](https://github.com/IBM/TabFormer)\n",
        "* GReaT, 10.2022\n",
        "    * [Language Models are Realistic Tabular Data Generators](https://arxiv.org/pdf/2210.06280.pdf)\n",
        "    * [code](https://github.com/kathrinse/be_great/tree/main)\n",
        "* TabuLa, 10.2023\n",
        "    * [TabuLa: Harnessing Language Models for Tabular Data Synthesis](https://arxiv.org/pdf/2310.12746.pdf)\n",
        "    * [code](https://github.com/zhao-zilong/Tabula)\n",
        "* REaLTabFormer, 02.2023\n",
        "  \n",
        "  Поддерживает реляционные данные. Родительская таблица генерируется с помощью авторегрессионной GPT-2, а затем условно на родительскую генерируется дочерняя с помощью Seq2Seq модели с GPT-2 декодером.\n",
        "  Обработка численных данных производилась довольно специфическим образом - сначала числа округлялись до двух знаков после запятой, затем выравнивались нулями слева и справа, чтобы получить одинаковую длину затем нарезались на кусочки одинаковой длины (например по 1 или 2 символа), чтобы получить токены.\n",
        "    * [REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers](https://arxiv.org/pdf/2302.02041.pdf)\n",
        "    * [code](https://github.com/avsolatorio/REaLTabFormer)\n",
        "\n",
        "### Diffusion-based\n",
        "\n",
        "* SOS, 06.2022\n",
        "  \n",
        "  Применение score-based моделей к табличным данным. Для моделирования прямого и обратного процесса диффузии используются стохастические дифференциальные уравнения. Мы как обычно хотим максимизировать логарифм правдоподобия данных и в данном случае добиваемся этого через оценку (score) его градиента по параметрам модели. Для предсказания score используется нейросеть. Отличие данной работы в том, что для борьбы с дисбалансом тренируются две модели - одна для не-целевого класса, другая для целевого (которого предполагается, что мало).\n",
        "    * [SOS: Score-based Oversampling for Tabular Data](https://arxiv.org/pdf/2206.08555.pdf)\n",
        "    * [code](https://github.com/jayoungkim408/sos)\n",
        "* TabDDPM, 09.2022\n",
        "    * [TabDDPM: Modelling Tabular Data with Diffusion Models](https://arxiv.org/pdf/2209.15421.pdf)\n",
        "    * [code](https://github.com/yandex-research/tab-ddpm/tree/main)\n",
        "*  TabSyn, 10.2023\n",
        "    * [Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://arxiv.org/pdf/2310.09656.pdf)\n",
        "    * [code](https://github.com/amazon-science/tabsyn)\n",
        "* STaSy, 10.2022\n",
        "  \n",
        "  Ещё одна score-based модель, здесь реализовали \"наивную\" версию и улучшили её своим способом обучения и файнтюнинга. В качестве обработки признаков используются min-max нормализация для числовых признаков и one-hot для категориальных, которые затем рассматриваются просто как числовые. По метрикам превосходит все недифузионные модели и чуть-чуть SOS.\n",
        "    * [STaSy: Score-based Tabular data Synthesis](https://arxiv.org/pdf/2210.04018.pdf)\n",
        "    * [code](https://github.com/JayoungKim408/STaSy)\n",
        "* CoDi, 04.2023\n",
        "\n",
        "  Непрерывные и категориальные признаки генерируются двумя разными диффузионными моделями, которые обусловлены друг на друга.\n",
        "    * [CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis](https://arxiv.org/pdf/2304.12654.pdf)\n",
        "    * [code](https://github.com/ChaejeongLee/CoDi)\n",
        "* TabCSDI, 10.2022\n",
        "\n",
        "  Применение дифузионных моделей для заполнения пропусков в табличных данных. Для одновременной обработки категориальных и числовых переменных рассматриваются три метода: one-hot кодирование, аналоговое кодирование битов и токенизация признаков.\n",
        "    * [Diffusion models for missing value imputation in tabular data](https://arxiv.org/pdf/2210.17128.pdf)\n",
        "    * [code](https://github.com/pfnet-research/TabCSDI)\n"
      ],
      "metadata": {
        "id": "t0KNvYhQ6w45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mодели на основе LLM"
      ],
      "metadata": {
        "id": "KxBCCiItvGMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GReaT"
      ],
      "metadata": {
        "id": "MzjYpzJ_xGgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**А можно ли использовать предобученную на текстах LLM (GPT и прочие) для генерации табличных данных?**\n",
        "\n",
        "В статье [Language Models are Realistic Tabular Data Generators (GReaT)](https://arxiv.org/pdf/2210.06280.pdf) был (впервые) предложен такой подход.\n",
        "\n",
        "По заключению авторов, при правильном преобразовании таблиц в текст предобученные LLM походят для моделирования гетерогенных табличных данных.\n",
        "\n",
        "Для этого на основе названий колонок и их значений строятся синтаксически корректные предложения, этот процесс называется текстовый энкодинг. На деле шаблон составления текста очень простой - `Column is value,`\n",
        "\n",
        "Так как имена атрибутов учитываются при кодировании, модель обученная на данных имеет доступ к контексту (например, PhD нельзя защитить в 8 лет).\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/GReaT_data_pipeline.png\" alt=\"GReaT data pipeline\" width=\"100%\">\n",
        "<figcaption align = \"center\"> Image credit: GReaT paper</figcaption>\n",
        "</figure>\n",
        "\n",
        "Процесс состоит из двух шагов:\n",
        "\n",
        "* файнтюнинг предобученной LLM на преобразованных табличных данных\n",
        "* генерация с помощью этой модели\n",
        "\n",
        "При обучении порядок столбцов каждый раз произвольным образом переставлялся.\n",
        "\n",
        "Это даёт преимущество условной генерации по фактически произвольному набору атрибутов - просто фиксируем их в качестве префикса. Причём это работает не только с категориальными, но и вещественными столбцами.\n",
        "\n",
        "Обратное преобразование в таблицу производилось с помощью регулярок. Если формат нарушался, то строчка игнорировалась (таких было менее 1%).\n",
        "\n",
        "В экспериментах не было никакого препроцессинга датасетов, данные брались как есть. Не очень понятно, насколько разумно обрабатывать текстовым токенизатором вещественные столбцы с большим количеством знаков после запятой.\n",
        "\n",
        "По результатам из публикации DistilGPT почти на всех датасетах по метрикам превосходит TVAE, CopulaGAN и CTGAN. Не дистилированная, полная версия GPT, превосходит на всех, но требует очень много времени на обучение (когда CTGAN обучался за 1 минуту, GReaT требовалось 9 часов). TVAE при этом уступает незначительно."
      ],
      "metadata": {
        "id": "DPvOc0d1v_nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TabuLa"
      ],
      "metadata": {
        "id": "7Abi4TWZxI-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В статье [TabuLa: Harnessing Language Models for Tabular Data Synthesis](https://arxiv.org/pdf/2310.12746.pdf), которая вышла на полгода позже, развили эту идею.\n",
        "\n",
        "Токенизация в LLM также полностью текстовая и не нужно задавать типы столбцов, как в GAN и диффузионках.\n",
        "\n",
        "Основные фичи:\n",
        "\n",
        "* Вместо предобученной LLM используют модель со случайно инициализированными весами\n",
        "* Компрессия последовательности токенов - заменили `X is Y` на `X Y`\n",
        "* Стратегия выравнивания последовательностей *Middle Padding*. Вместо привычного добавления padding-токенов в начале или конце, они добавляются внутрь последовательности. Достигается это путём отдельной токенизации столбцов и выравнивания их по максимальной длине. Так релевантные токены находятся всё время на одних и тех же позициях. При этом название столбца сохраняется только для первого, для остальных только значения.\n",
        "* \"Foundation model\" для табличных данных\n",
        "\n",
        "Брали код файнтюнинга от GReaT и сравнивали 3 режима файнтюнинга:\n",
        "\n",
        "* предобученная [DistilGPT](https://huggingface.co/distilgpt2) дала самый худший результат\n",
        "* рандомная DistilGPT сработала получше\n",
        "* рандомная DistilGPT, файнтюненная на другом табличном датасете показала себя лучше всех\n",
        "\n",
        "Главный пойнт авторов в том, что хотя преобразованные табличные данные похожи на текст, он принадлежит довольно специфическому паттерну. GPT-2 может ухватить и этот паттерн, но **случайно инициализированная модель сделает это быстрее предобученной**.\n",
        "\n",
        "Ускорение по сравнению с GReaT получилось раза в полтора-два за счёт компрессии токенов.\n",
        "\n",
        "Foundation model - когда модель, зафайнтюненную на другом табличном датасете используют на новом. При этом качество зависит от датасета, как видно на рисунке ниже.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/Tabula_losses.png\" alt=\"TabuLa losses\" width=\"60%\">\n",
        "<figcaption align = \"center\"> Image credit: TabuLa paper</figcaption>\n",
        "</figure>\n",
        "\n",
        "Цепочный finetuning foundation model дало прирост в качестве даже после двух датасетов.\n"
      ],
      "metadata": {
        "id": "I73Mq0r7xM76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Модели основе диффузии"
      ],
      "metadata": {
        "id": "-okin1qmvqgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TabDDPM"
      ],
      "metadata": {
        "id": "YwRsTIjYHYZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Background\n",
        "\n",
        "##### Диффузные модели\n",
        "\n",
        "Диффузные модели - генеративные модели, которые определяются через прямой и обратный Марковский процесс.\n",
        "\n",
        "Прямой процесс постепенно добавляет шум к начальному образцу $x_0$ из распределения $q (x_0)$, сэмплируя шум из заранее определённых распределений $q (x_t|x_{t-1})$ с дисперсиями ${β_1, ..., β_T}$:\n",
        "$$q (x_{1:T} |x_0) = \\prod_{t=1}^T q (x_t|x_{t-1})$$\n",
        "Обратный процесс диффузии постепенно убирает шум из  $x_T \\sim q (x_T)$ и позволяет генерировать новые образцы данных из $q(x_0)$:\n",
        "$$p (x_{0:T}) = \\prod_{t=1}^T p (x_{t-1}|x_t)$$\n",
        "\n",
        "Распределения $p (x_{t-1}|x_t)$ обычно неизвестны и аппроксимируются нейронной сетью с параметрами $\\theta$. Обучение нейросети происходит путём оптимизации вариационной нижней границы логарифма правдоподобия:\n",
        "$$\\log q (x_0) \\geq \\mathbb{E}_{q(x_0)} \\Big[\\log p_{\\theta} (x_0|x_1) - \\text{KL} (q (x_T |x_0) | q (x_T )) - \\sum_{t=2}^T \\text{KL} (q (x_{t-1}|x_t, x_0) \\| p_{\\theta} (x_{t-1}|x_t))\\Big] \\tag 1$$\n",
        "\n",
        "##### Гауссовские диффузные модели\n",
        "\n",
        "Работают в непрерывных пространствах $(x_t ∈ R^n)$, где прямые и обратные процессы характеризуются гауссовскими распределениями. Для прямого процесса:\n",
        "\n",
        "$$q (x_t|x_{t-1}) := \\mathcal{N}\\left(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I\\right),$$\n",
        "где $β_t ∈ (0, 1)$ - \"расписание\" дисперсии (variance schedule).\n",
        "\n",
        "В результате, после $T$ шагов, мы приходим к стандартному нормальному распределению:\n",
        "$$q (x_T) := \\mathcal{N}\\left(x_T; 0, I\\right)$$\n",
        "Аппроксимированное распределение для обратного процесса:\n",
        "$$p_{\\theta} (x_{t-1}|x_t) := \\mathcal{N}\\left(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t)\\right), $$\n",
        "где $\\mu_{\\theta}(x_t, t)$ и $\\Sigma_{\\theta}(x_t, t)$ - параметризованные функции, определенные с использованием нейросети.\n",
        "\n",
        "Для реализации предложенного метода в оригинальной работе по DDPM рекомендовали использовать диагональную матрицу ковариации $\\Sigma_{\\theta} (x_t, t)$ с постоянной $\\sigma_t$, а $\\mu_{\\theta} (x_t, t)$ вычислять как функцию от $x_t$ и $\\epsilon_{\\theta} (x_t, t)$:\n",
        "$$\n",
        "\\mu_{\\theta} (x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta} (x_t, t) \\right)\n",
        "$$\n",
        "\n",
        "где $\\alpha_t := 1 - \\beta_t$, $\\bar{\\alpha}_t := \\prod_{i \\leq t} \\alpha_i$ и $\\epsilon_{\\theta} (x_t, t)$ предсказывает \"истинный\" компонент шума $\\epsilon$ для зашумленного образца данных $x_t$.\n",
        "\n",
        "На практике, цель (1) может быть упрощена до суммы среднеквадратичных ошибок между $\\epsilon_{\\theta} (x_t, t)$ и $\\epsilon$ на всех временных шагах $t$:\n",
        "$$\n",
        "L^{\\text{simple}}_t = \\mathbb{E}_{x_0, \\epsilon, t} \\left\\| \\epsilon - \\epsilon_{\\theta} (x_t, t) \\right\\|_2^2\n",
        "$$\n",
        "\n",
        "##### Мультиномиальные диффузионные модели\n",
        "\n",
        "В дискретных пространствах, для генерации категориальных данных, где $x_t \\in \\{0, 1\\}^K$ представляет собой категориальную переменную с кодированием \"one-hot\" из $K$ значений предназначены мультиномиальные модели ([Hoogeboom et al., 2021](https://arxiv.org/pdf/2102.05379.pdf)).\n",
        "\n",
        "Вместо нормального шума используется равномерное распределение вероятностей по возможным классам. Полиномиальный прямой процесс диффузии определяет $q (x_t|x_{t-1})$ как категориальное распределение, искажающее данные с использованием равномерного шума по $K$ классам. То есть получаем категориальное распределение с заданными параметрами-вероятностями:\n",
        "$$q(x_t|x_{t-1}) := \\text{Cat} (x_t; (1 - \\beta_t) x_{t-1} + \\frac{\\beta_t}{K})$$\n",
        "В результате, после $T$ шагов, мы приходим к равномерному распределению по категориям с вероятностью $1/K$:\n",
        "$$q (x_T ) := \\text{Cat} (x_T ; \\frac{1}{K})$$\n",
        "$$q (x_t|x_0) = \\text{Cat} \\Big(x_t; \\bar{\\alpha}_t x_0 + \\frac{(1 - \\bar{\\alpha}_t)}{K}\\Big)$$\n",
        "\n",
        "Получается на каждом шаге добавляется небольшое количество равномерного шума с коэффициентом $\\beta_t$ и с большой вероятностью $(1-\\beta_t)$ сэмплируется предыдущее значение $x_{t-1}$.\n",
        "\n",
        "Используя уравнения выше, можно получить апостериорное распределение прямого процесса $q(x_{t-1}|x_t, x_0)$:\n",
        "$$q (x_{t-1}|x_t, x_0) = \\text{Cat} \\left( x_{t-1}; \\frac{\\pi}{\\sum_{k=1}^{K} \\pi_k} \\right)$$\n",
        "где $\\pi = [\\alpha_t x_t + \\frac{(1 - \\alpha_t)}{K}] \\odot [\\bar{\\alpha}_{t-1} x_0 + \\frac{(1 - \\bar{\\alpha}_{t-1})}{K}]$.\n",
        "\n",
        "Это позволяет аналитически вычислять последний член KL-дивергенции в формуле (1).\n",
        "\n",
        "Обратное распределение $p_{\\theta} (x_{t-1}|x_t)$ параметризуется как $q (x_{t-1}|x_t, \\hat{x}_0(x_t, t))$, где $\\hat{x}_0$ предсказывается нейронной сетью.\n",
        "\n",
        "Затем модель обучается путём максимизации вариационной нижней границы (1)."
      ],
      "metadata": {
        "id": "JL4E6RdiuwOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Модель\n",
        "\n",
        "TabDDPM для моделирования числовых признаков использует гауссовскую диффузию, а для категориальных и бинарных признаков - мультиномиальную.\n",
        "\n",
        "Более подробно, для строчки таблицы $x = [x_{\\text{num}}, x_{\\text{cat}_1}, ..., x_{\\text{cat}_C}]$, состоящей из $N_{\\text{num}}$ числовых признаков ($x_{\\text{num}} \\in \\mathbb{R}^{N_{\\text{num}}}$) и $C$ категориальных признаков $x_{\\text{cat}_i}$ с $K_i$ категориями каждый, модель принимает на вход нормализованные числовые признаки и \"one-hot\" для категориальных признаков (т.е. $x^{\\text{ohe}}_{cat_i} \\in \\{0, 1\\}^{K_i}$) и . Таким образом, вход $x_0$ имеет размерность ( $N_{\\text{num}} + \\sum K_i$ ).\n",
        "\n",
        "В качестве предварительной обработки используются гауссовское квантильное преобразование из библиотеки scikit-learn (QuantileTransformer). Каждый категориальный признак обрабатывается отдельным прямым процессом, то есть шумовые компоненты для всех признаков выбираются независимо. То есть категориальные признаки моделируются **независимо** друг от друга и от непрерывных признаков.\n",
        "\n",
        "Обратный шаг диффузии в TabDDPM моделируется многослойной нейронной сетью, которая имеет выход той же размерности, что и $x_0$, где первые $N_{\\text{num}}$ координат - это предсказания шума $\\epsilon$ для гауссовской диффузии, а остальные - предсказания $x^{\\text{ohe}}_{cat_i}$ для мультиномиальной.\n",
        "\n",
        "$$\n",
        "\\text{MLP}(x) = \\text{Linear}(\\text{MLPBlock}(\\ldots(\\text{MLPBlock}(x))))\n",
        "$$\n",
        "$$\n",
        "\\text{MLPBlock}(x) = \\text{Dropout}(\\text{ReLU}(\\text{Linear}(x)))\n",
        "$$\n",
        "\n",
        "Для задач классификации используется условная модель по классу $p_θ (x_{t−1}|x_t, y)$.\n",
        "\n",
        "Вход $x_{in}$, временной шаг $t$ и метка класса $y$ обрабатываются следующим образом:\n",
        "$$\n",
        "t_{\\text{emb}} = \\text{Linear}(\\text{SiLU}(\\text{Linear}(\\text{SinTimeEmb}(t))))\n",
        "$$\n",
        "$$\n",
        "y_{\\text{emb}} = \\text{Embedding}(y)\n",
        "$$\n",
        "$$x = \\text{Linear}(x_{in}) + t_{\\text{emb}} + y_{\\text{emb}}$$\n",
        "\n",
        "В экспериментах модель почти на всех датасетах превосходит TVAE и CTABGAN+ и часто довольно существенно, а если уступает, то незначительно."
      ],
      "metadata": {
        "id": "KeV3_Kk8Foz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TabSyn"
      ],
      "metadata": {
        "id": "vyHyLr8Gu4jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Преамбула\n",
        "\n",
        "Существующие решения либо преобразуют категориальные признаки в числовые с использованием техник, таких как one-hot кодирование (STaSy, GOGGLE) и аналоговое битовое кодирование (TabCSDI), либо прибегают к двум отдельным процессам диффузии для числовых и категориальных признаков (TabDDPM, CoDi).\n",
        "\n",
        "В TABSYN стремились разработать модель диффузии в совместном пространстве числовых и категориальных признаков, которая сохраняет взаимосвязь между столбцами.\n",
        "\n",
        "TABSYN сначала преобразует исходные табличные данные в непрерывное пространство эмбеддингов, где уже можно использовать проверенные модели диффузии с гауссовским шумом. Затем обучается score-based модель диффузии в пространстве эмбеддингов, чтобы уловить распределение эмбеддингов.\n",
        "\n",
        "Чтобы обучить информативное, сглаженное латентное пространство, сохраняя возможность качественной реконструкции декодером, была специально разработана модель VAE для табличных данных.\n",
        "\n",
        "Предложенная модель VAE включает:\n",
        "\n",
        "1. Энкодеры и декодеры на основе трансформеров для моделирования взаимосвязей между столбцами и получения представлений на уровне токенов\n",
        "2. Адаптивное взвешивание лосса для динамической корректировки вклада компонента восстановления (reconstruction term) и KL-дивергенции в функцию потерь, что позволяет модели постепенно улучшать качество реконструкции, сохраняя по мере сил регуляризованность пространства эмбеддингов\n",
        "\n",
        "Наконец, при применении моделей диффузии в латентном пространстве был предложен упрощенный процесс прямой диффузии, который добавляет гауссовские шумы стандартного отклонения линейно относительно времени. Демонстрируется теоретически и экспериментально, что этот подход может уменьшить ошибки в обратном процессе, тем самым улучшая скорость сэмплирования.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/TABSYN_arch.png\" alt=\"TABSYN arch\" width=\"70%\">\n",
        "<figcaption align = \"center\"> Image credit: TABSYN paper</figcaption>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "__GoCO4-HO7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer\n",
        "\n",
        "Токенизатор преобразует каждый столбец (как числовой, так и категориальный) в вектор размерности $d$. Сначала используется \"one-hot\" кодирование для предварительной обработки категориальных признаков, то есть, $x_{cat_i} \\Rightarrow x_{oh_i} \\in \\mathbb{R}^{1 \\times C_i}$. Тогда каждая строчка представляется как $x = [x^{num}, x^{oh}_1, \\ldots, x^{oh}_{M_{cat}}] \\in \\mathbb{R}^{M_{num} + \\sum_{i=1}^{M_{cat}} C_i}$.\n",
        "\n",
        "Затем для числовых столбцов применяется линейное преобразование, а для категориальных - обучаемый слой эмбеддингов (embedding lookup table), то есть,\n",
        "$$e_{num} = x^{num}_i \\cdot w^{num}_i + b^{num}_i, \\quad e^{cat}_i = x^{oh}_i \\cdot W^{cat}_i + b^{cat}_i \\tag 1$$\n",
        "где $w_{num_i}, b_{num_i}, b_{cat_i} \\in \\mathbb{R}^{1 \\times d}$, $W_{cat_i} \\in \\mathbb{R}^{C_i \\times d}$ - обучаемые параметры токенизатора, $e_{num_i}, e_{cat_i} \\in \\mathbb{R}^{1 \\times d}$.\n",
        "\n",
        "В результате каждая строчка представляется как вертикальная конкатенация векторов эмбеддингов всех столбцов (получается матрица на выходе)\n",
        "$$E = [e^{num}_1, \\ldots, e^{num}_{M_{num}}, e^{cat}_1, \\ldots, e^{cat}_{M_{cat}}] \\in \\mathbb{R}^{M \\times d} \\tag 2$$\n",
        "\n",
        "#### Detokenizer\n",
        "\n",
        "Для восстановления значений столбцов используется детокенизатор, дизайн которого симметричен дизайну токенизатора:\n",
        "\n",
        "$\\hat{x}^{num}_i = \\hat{e}^{num}_i \\cdot \\hat{w}^{num}_i + \\hat{b}^{num}_i, \\quad \\hat{x}^{oh}_i = \\text{Softmax}(\\hat{e}^{cat}_i \\cdot \\hat{W}^{cat}_i + \\hat{b}^{cat}_i),$\n",
        "\n",
        "$\\hat{x} = [\\hat{x}^{num}_1, \\ldots, \\hat{x}^{num}_{M_{num}}, \\hat{x}^{oh}_1, \\ldots, \\hat{x}^{oh}_{M_{cat}}], \\tag 3$\n",
        "\n",
        "где $\\hat{w}^{num}_i \\in \\mathbb{R}^{d \\times 1}$, $\\hat{b}^{num}_i \\in \\mathbb{R}^{1 \\times 1}$, $\\hat{W}^{cat}_i \\in \\mathbb{R}^{d \\times C_i}$, $\\hat{b}^{cat}_i \\in \\mathbb{R}^{1 \\times C_i}$ - параметры детокенизатора."
      ],
      "metadata": {
        "id": "h4WfjBHPpTFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VAE\n",
        "\n",
        "Как в обычном VAE, энкодер используется для получения среднего и логарифма дисперсии латентной переменной. Однако вместо ELBO использовался $\\beta$-VAE, где коэффициент $β$ балансирует между лоссом реконструкции и KL-дивергенцией:\n",
        "$$\\mathcal L = \\mathcal{L}_{recon}(x, \\hat{x}) + \\beta \\mathcal{L}_{kl}, \\tag 4$$\n",
        "где $\\mathcal{L}_{recon}$ - это ошибка реконструкции входных данных, а $\\mathcal{L}_{kl}$ - KL-дивергенция, которая регуляризует среднее и дисперсию скрытого пространства.\n",
        "\n",
        "В обычной модели VAE значение $\\beta$ устанавливается равным 1, поскольку оба компонента потерь равнозначны. Однако в данной модели ожидается, что $\\beta$ будет меньше 1, в связи с тем, что не требуется, чтобы распределение эмбеддингов точно соответствовало стандартному нормальному распределению, так как есть дополнительная модель диффузии. Поэтому предлагается адаптивное изменение масштаба $\\beta$ в процессе обучения, поощряя модель в первую очередь достигать более низкой ошибки реконструкции, сохраняя при этом подходящую форму эмбеддингов.\n",
        "\n",
        "В начале задаётся максимальное значение $\\beta = \\beta_{\\text{max}}$, и отслеживается $\\mathcal{L}_{\\text{recon}}$ по эпохам. Когда он не уменьшается в течение заранее определенного числа эпох (что указывает на то, что KL-дивергенция преобладает в общей потере), вес $\\beta$ изменяется как $\\beta = \\lambda \\beta$, где $\\lambda < 1$. Этот процесс продолжается до тех пор, пока $\\beta$ не достигнет заранее определенного минимального значения $\\beta_{\\text{min}}$.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/TABSYN_VAE.png\" alt=\"TABSYN VAE\" width=\"70%\">\n",
        "<figcaption align = \"center\"> Image credit: TABSYN paper</figcaption>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "eswZMPBOpV6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Score-based генеративная модель в латентном пространстве\n",
        "\n",
        "После того как модель VAE обучена, с помощью её энкодера извлекаются вектора эмбеддингов строк $z = \\text{Flatten}(\\text{Encoder}(x)) \\in \\mathbb{R}^{1 \\times M \\cdot d}$.\n",
        "\n",
        "Далее используется диффузия на основе SDE.\n",
        "\n",
        "За счёт линейного расписания уровня шума во времени минимизируется ошибка аппроксимации в обратном процессе, что в свою очередь позволяет увеличить интервал между двумя шагами времени, тем самым уменьшая общее количество шагов сэмплирования и ускоряя сэмплирование. C таким подходом TABSYN может генерировать высококачественные синтетические табличные данные с менее чем 20 оценками функций (NFE - количество оценок функций).\n",
        "\n",
        "TabDDPM требует 1000 шагов диффузии для получения приемлемого результата, STaSy - 50-256, TabSyn - 12-20.\n"
      ],
      "metadata": {
        "id": "n9Pgsd4JHNMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Детали\n",
        "\n",
        "Чтобы выучить распределение эмбеддингов $p(z)$, используется диффузия на основе стохастических дифференциальных уравнений:\n",
        "$$z_t = z_0 + \\sigma(t)\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\text{ (Forward process)} \\tag 5$$\n",
        "$$dz_t = -2\\dot{\\sigma}(t)\\sigma(t)\\nabla_{z_t} \\log p(z_t)dt + p^2\\dot{\\sigma}(t)\\sigma(t)d\\omega_t\\text{ (Reverse process)} \\tag 6$$\n",
        "где $z_0 = z$ - начальный эмбеддинг от энкодера, $z_t$ - диффузионный эмбеддинг в момент времени $t$, и $\\sigma(t)$ - уровень шума. В обратном процессе $\\nabla_{z_t} \\log p_t(z_t)$ представляет собой score-функцию для $z_t$, а $\\omega_t$ - стандартный винеровский процесс (непрерывное случайное блуждание).\n",
        "\n",
        "Обучение модели диффузии достигается с использованием метода denoising score matching:\n",
        "$$L = \\mathbb{E}_{z_0 \\sim p(z_0)}\\mathbb{E}_{t \\sim p(t)}\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\|\\epsilon_{\\theta}(z_t, t) - \\epsilon\\|^2_2, \\text{ где } z_t = z_0 + \\sigma(t)\\epsilon, \\tag 7$$\n",
        "где $\\epsilon_{\\theta}$ - нейронная сеть (названная функцией сглаживания), приближающая гауссовский шум с использованием исказанных данных $x_t$ и времени $t$. Тогда $\\nabla_{z_t} \\log p(z_t) = -\\epsilon_{\\theta}(z_t, t)/\\sigma(t)$.\n",
        "\n",
        "После обучения модели синтетические данные могут быть получены с помощью обратного процесса (6).\n",
        "\n",
        "Уровень шума $\\sigma(t)$ определяет масштаб шума для пертурбации данных на различных временных шагах и существенно влияет на финальные траектории решений дифференциальных уравнений . Следуя рекомендациям из статьи [Elucidating the design space of diffusion-based generative model](https://arxiv.org/pdf/2206.00364.pdf), уровень шума устанавливается $\\sigma(t) = t$, то есть линейно по времени. Как показано в Предложении 1, линейное расписание уровня шума приводит к наименьшим ошибкам аппроксимации в обратном процессе:\n",
        "\n",
        "**Предложение 1.** Рассмотрим обратный процесс диффузии в уравнении (6) от $z_{t_b}$ до $z_{t_a}$ ($t_b > t_a$). Численное решение $\\hat{z}_{t_a}$ имеет наименьшую ошибку аппроксимации к $z_{t_a}$, когда $\\sigma(t) = t$."
      ],
      "metadata": {
        "id": "sHFdyCyIqENr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing value imputation\n",
        "\n",
        "Диффузные модели используются для inpainting-задач, авторы работы предложили как перенести этот подход на таблицы, что позволяет эффективно восстанавливать пропуски в данных."
      ],
      "metadata": {
        "id": "b1jjJ1wYqM6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сравнение с другими моделями\n",
        "\n",
        "Данные взяты из статьи\n",
        "\n",
        "Сравниваемые характеристики моделей:\n",
        "\n",
        "1. **Совместимость:** возможность метода обрабатывать данные смешанного типа, например, числовые и категориальные столбцы.\n",
        "2. **Устойчивость:** стабильность работы метода на различных наборах данных (измеряемая стандартным отклонением оценок (≤ 10% или нет) на разных наборах данных.\n",
        "3. **Качество:** способность синтетических данных проходить колоночный тест хи-квадрат (p ≥ 0.95).\n",
        "4. **Эффективность:** каждый метод способен генерировать синтетические табличные данные удовлетворительного качества за менее чем 20 шагов.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/TABSYN_baselines_comparison.png\" alt=\"TABSYN baselines comparison\" width=\"70%\">\n",
        "<figcaption align = \"center\"> Image credit: TABSYN paper</figcaption>\n",
        "</figure>\n",
        "\n",
        "Восстановление плотности распределения\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w9/TABSYN_kde_comparison.png\" alt=\"TABSYN kde comparison\" width=\"70%\">\n",
        "<figcaption align = \"center\"> Image credit: TABSYN paper</figcaption>\n",
        "</figure>\n",
        "\n",
        "Время работы\n",
        "\n",
        "| Method   | Training        | Sampling  |\n",
        "|----------|-----------------|-----------|\n",
        "| CTGAN    | 1029.8s         | 0.8621s   |\n",
        "| TVAE     | 352.6s          | 0.5118s   |\n",
        "| GOGGLE   | 1h 34min        | 5.342s    |\n",
        "| GReaT    | 2h 27min        | 2min 19s  |\n",
        "| STaSy    | 2283s           | 8.941s    |\n",
        "| CoDi     | 2h 56min        | 4.616s    |\n",
        "| TabDDPM  | 1031s           | 28.92s    |\n",
        "| TABSYN   | 1758s + 664s    | 1.784s    |\n",
        "\n",
        "Итого TABSYN:\n",
        "\n",
        "* отображает данные в пространство эмбеддингов с помощью VAE\n",
        "* VAE особенный\n",
        "\t* на трансформерах\n",
        "\t* динамически меняется вклад лоссов в ходе обучения, по ходу обучения все меньше веса даётся KL-дивергенции\n",
        "* обучает диффузионку на эмбеддингах\n",
        "\t* упрощённый процесс прямой диффузии (linear noise schedule, быстрая генерация за менее чем 20 шагов)\n",
        "\n",
        "Рассматривается задача безусловной генерации.\n",
        "\n",
        "Для каждого столбца строится свой обучаемый эмбеддинг и затем [поколоночные](https://arxiv.org/pdf/2106.11959.pdf) представления подаются в трансформер для учёта внутренних взаимосвязей между столбцами.\n",
        "\n"
      ],
      "metadata": {
        "id": "ecEoH9Txq8vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Заключение\n",
        "\n",
        "Выводы:\n",
        "\n",
        "* Диффузные модели для табличных данных в целом демонстрируют лучшие показатели по сравнению с моделями, основанными на GAN, согласно метрикам качества.\n",
        "\n",
        "* Модели, основанные на LLM, выглядят менее теоретически обоснованными, однако их также интересно рассматривать в свете впечатляющих успехов данного класса моделей."
      ],
      "metadata": {
        "id": "_zQs1up_FbDX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Nfc4Cnq9lzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}