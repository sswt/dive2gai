{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continious Generative Models\n",
    "---\n",
    "\n",
    "Цели:\n",
    "1. Познакомиться с непрерывними нормализующими потоками\n",
    "2. Познакомиться с непрерывными диффузионными моделями\n",
    "\n",
    "Содержание:\n",
    "- [Continious Normalizing Flow](#cnf)\n",
    "    - [Теорема о замене переменной](#theor)\n",
    "    - [Многомерные потоки](#mult)\n",
    "    - [Использование динамики во времени](#timedep)\n",
    "- [Continious Diffusion Models](#cdm)\n",
    "    - [Прямой процесс диффузии](#forward)\n",
    "    - [Обратный процесс диффузии](#reverse)\n",
    "    - [Score-Based Generative Modeling Throught SDE](#sdm)\n",
    "\n",
    "Ссылки:\n",
    "1. [Neural Ordinary Differential Equations](https://arxiv.org/pdf/1806.07366.pdf)\n",
    "2. [Знакомство с Neural ODE](https://habr.com/ru/companies/ods/articles/442002/)\n",
    "3. [PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows](https://arxiv.org/pdf/1906.12320v3.pdf)\n",
    "4. [Multi-Resolution Continuous Normalizing Flows](https://arxiv.org/pdf/2106.08462v5.pdf)\n",
    "5. [Efficient and Accurate Gradients for Neural SDEs](https://arxiv.org/pdf/2105.13493.pdf)\n",
    "6. [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/pdf/2011.13456.pdf)\n",
    "\n",
    "Используемые пакеты:\n",
    "1. [Репо torchdiffeq](https://github.com/rtqichen/torchdiffeq/)\n",
    "2. [Репо torchsde](https://github.com/google-research/torchsde/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cnf\"></a>\n",
    "## Continious Normalizing Flow\n",
    "\n",
    "Давайте еще раз вспомним про постановку задачи для нормализующих потоков\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png\" alt=\"Примеры схем генеративных моделй\" style=\"width:100%\">\n",
    "<figcaption align = \"center\">Иллюстрация преобразования нормально распределенного z0 в zK из реального  распределения.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Это модель с дискретным временем, шаг которой можно описать как\n",
    "$$\n",
    "z_1 = f(z_0) \\Longrightarrow \\log p(z_1) = \\log p(z_0) - \\log\\left|\\det\\dfrac{df}{dz_0}\\right|\n",
    "$$\n",
    "В частности при использовании планарных потоков шаг можно переписать\n",
    "$$\n",
    "z(t+1) = z(t) + uh(w^T z(t) + b) \\Longrightarrow \\log p(z(t+1)) = \\log p(z(t)) - \\log\\left|1 + u^T\\dfrac{dh}{dz}\\right|\n",
    "$$\n",
    "Теперь будем увеличивать число шагов и уменьшать размер шага до тех пор пока задача не станет непрерывной. Тогда выражения выше можно переписать через малые приращения\n",
    "$$\n",
    "z(t+\\Delta t) = z(t) + uh(w^T z(t) + b) \\Longrightarrow \\log p(z(t+\\Delta t)) = \\log p(z(t)) - \\log\\left|1 + u^T\\dfrac{dh}{dz}\\right|\n",
    "$$\n",
    "$$\n",
    "z(t+\\Delta t) - z(t) = uh(w^T z(t) + b) \\Longrightarrow \\log p(z(t+\\Delta t)) - \\log p(z(t)) = - \\log\\left|1 + u^T\\dfrac{dh}{dz}\\right|\n",
    "$$\n",
    "При взятии предела $\\Delta t\\to 0$ разность можно переписать выражения через производные\n",
    "$$\n",
    "\\dfrac{dz(t)}{dt} = uh(w^T z(t) + b) \\Longrightarrow \\dfrac{\\partial\\log{p(z(t))}}{\\partial t} = -u^T\\dfrac{dh}{dz}\n",
    "$$\n",
    "\n",
    "<a name=\"theor\"></a>\n",
    "### Теорема о замене переменной\n",
    "\n",
    "Положим, что $z(t)$ — континуальная случайная величина с плотностью $p(z(t))$, зависящая от времени. Положим, что $dz(t)/dt = f(z(t), t)$ — дифференциальное уравнение, описывающее изменение нашей величины от времени. Положим, что $f$ — Липшецево отображение, тогда\n",
    "$$\n",
    "\\dfrac{∂\\log p(z(t))}{∂t} = -\\text{tr}\\left(\\dfrac{df}{dz(t)}\\right)\n",
    "$$\n",
    "\n",
    "Тогда в общем виде непрерывные потоки имеют вид\n",
    "$$\n",
    "\\dfrac{dz(t)}{dt} = f(z(t), t, \\theta),~\\dfrac{∂\\log p(z(t))}{∂t} = -\\text{tr}\\left(\\dfrac{df}{dz(t)}\\right)\n",
    "$$\n",
    "\n",
    "<a name=\"mult\"></a>\n",
    "### Многомерные потоки\n",
    "\n",
    "Пока поток $f$ не линейна, путь по Якобиана обладает свойством\n",
    "$$\n",
    "\\text{tr}\\left(\\sum\\limits_n J_n\\right) = \\sum\\limits_n \\text{tr}\\left(J_n\\right)\n",
    "$$\n",
    "Таким образом если наша динамика пораждена суммой функций, диффур логарифма плотности также является суммой\n",
    "$$\n",
    "\\dfrac{dz(t)}{dt} = \\sum\\limits_{i=1}^Mf_i(z(t)) \\Longrightarrow \\dfrac{∂\\log p(z(t))}{∂t} = \\sum\\limits_{i=1}^M\\text{tr}\\left\n",
    "(\\dfrac{df_i}{dz}\\right)\n",
    "$$\n",
    "\n",
    "Получается мы можем дешево (по сложности) обучать потоковые модели с большим числом скрытых слоев со сложностью равной числу скрытых слоев $O(M)$. В тоже время использование таких \"широких\" потоков в обычных нормализационных моделях имеет сложность $O(M^3)$.\n",
    "\n",
    "<a name=\"timedep\"></a>\n",
    "### Использование динамики во времени\n",
    "\n",
    "Также интересно то, что мы можем использовать один поток с параметром времени, то есть явно зависимый от времени $f(z(t), t)$, а не $f(z(t))$. Также авторы метода предлагают новый страбирующий метод\n",
    "$$\n",
    "\\dfrac{dz(t)}{dt} = \\sum\\limits_nσ_n(t)f_n(z),~σ\\in(0,1)\n",
    "$$\n",
    "где $σ_n(t)$ — нейронная сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Lambda, Resize, Compose, Normalize\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from torchsde import sdeint\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(num_samples):\n",
    "    points, _ = make_moons(n_samples=num_samples, noise=0.06)\n",
    "    points = (points - points.mean(0)) / points.std(0) * 1.5\n",
    "    x = torch.tensor(points).type(torch.float32).to(device)\n",
    "    logp_diff_t1 = torch.zeros(num_samples, 1).type(torch.float32).to(device)\n",
    "\n",
    "    return(x, logp_diff_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flow(\n",
    "    p_z0,\n",
    "    func,\n",
    "    num_samples: int = 30_000,\n",
    "    num_timestemps: int = 20,\n",
    "    name: str = 'cnf_ode',\n",
    "    t0: float = 0.,\n",
    "    t1: float = 1.\n",
    "):\n",
    "    Path('imgs').mkdir(exist_ok=True)\n",
    "\n",
    "    timesteps = torch.linspace(t0, t1, num_timestemps, device=device)\n",
    "    target_sample, _ = get_batch(num_samples)\n",
    "\n",
    "    z_t0 = p_z0.sample([num_samples]).to(device)\n",
    "    logp_diff_t0 = torch.zeros(num_samples, 1).type(torch.float32).to(device)\n",
    "\n",
    "    z_t_samples, _ = odeint(\n",
    "        func,\n",
    "        (z_t0, logp_diff_t0),\n",
    "        timesteps,\n",
    "        atol=1e-5,\n",
    "        rtol=1e-5,\n",
    "        method='dopri5',\n",
    "    )\n",
    "\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = np.linspace(-3, 3, 100)\n",
    "    points = np.vstack(np.meshgrid(x, y)).reshape([2, -1]).T\n",
    "\n",
    "    z_t1 = torch.tensor(points).type(torch.float32).to(device)\n",
    "    logp_diff_t1 = torch.zeros(z_t1.shape[0], 1).type(torch.float32).to(device)\n",
    "\n",
    "    z_t_density, logp_diff_t = odeint(\n",
    "        func,\n",
    "        (z_t1, logp_diff_t1),\n",
    "        timesteps.flip(0),\n",
    "        atol=1e-5,\n",
    "        rtol=1e-5,\n",
    "        method='dopri5',\n",
    "    )\n",
    "\n",
    "    for (t, z_sample, z_density, logp_diff) in zip(timesteps, z_t_samples, z_t_density, logp_diff_t):\n",
    "\n",
    "        logp = p_z0.log_prob(z_density) - logp_diff.view(-1)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        plt.axis('off')\n",
    "        plt.margins(0, 0)\n",
    "        plt.suptitle(f'{t:.2f}s', size=15)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        ax1 = fig.add_subplot(1, 3, 1)\n",
    "        ax1.set_title('Target')\n",
    "        ax1.get_xaxis().set_ticks([])\n",
    "        ax1.get_yaxis().set_ticks([])\n",
    "\n",
    "        ax2 = fig.add_subplot(1, 3, 2)\n",
    "        ax2.set_title('Samples')\n",
    "        ax2.get_xaxis().set_ticks([])\n",
    "        ax2.get_yaxis().set_ticks([])\n",
    "\n",
    "        ax3 = fig.add_subplot(1, 3, 3)\n",
    "        ax3.set_title('Probability')\n",
    "        ax3.get_xaxis().set_ticks([])\n",
    "        ax3.get_yaxis().set_ticks([])\n",
    "\n",
    "        ax1.hist2d(*target_sample.detach().cpu().T.numpy(), bins=300, density=True, range=[[-3, 3], [-3, 3]])\n",
    "        ax2.hist2d(*z_sample.detach().cpu().T.numpy(), bins=300, density=True, range=[[-3, 3], [-3, 3]])\n",
    "        plt.pcolormesh(*z_t1.T.view(2, 100, 100).detach().cpu(), logp.exp().view(100, 100).detach().cpu())\n",
    "        plt.savefig('imgs/cnf-%.2f.png' % t)\n",
    "        plt.close()\n",
    "\n",
    "    images = []\n",
    "    filenames = sorted(list(Path('imgs/').glob('*.png')))\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(str(filename)))\n",
    "    imageio.mimsave(f'{name}.gif', images, duration=5, format='GIF')\n",
    "    shutil.rmtree('imgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_df_dz(f, z):\n",
    "    \"\"\"Calculates the trace of the Jacobian df/dz.\n",
    "    Stolen from: https://github.com/rtqichen/ffjord/blob/master/lib/layers/odefunc.py#L13\n",
    "    \"\"\"\n",
    "    sum_diag = 0.\n",
    "    for i in range(z.shape[1]):\n",
    "        sum_diag += torch.autograd.grad(f[:, i].sum(), z, create_graph=True)[0].contiguous()[:, i].contiguous()\n",
    "\n",
    "    return sum_diag.contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, inp: int, width: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inp, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, inp, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class CNF(nn.Module):\n",
    "\n",
    "    def __init__(self, inp: int, width: int):\n",
    "        super().__init__()\n",
    "        self.net = PlanarFlow(inp, width)\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        z = state[0]\n",
    "        logp_z = state[1]\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)\n",
    "            dz_dt = self.net(z)\n",
    "            dlogp_z_dt = -trace_df_dz(dz_dt, z).view(z.size(0), 1)\n",
    "        return (dz_dt, dlogp_z_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0);\n",
    "np.random.seed(0);\n",
    "torch.manual_seed(0);\n",
    "torch.cuda.manual_seed(0);\n",
    "torch.cuda.manual_seed_all(0);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epoches = 1000\n",
    "t0, t1 = 0, 1\n",
    "func = CNF(2, 256).to(device)\n",
    "optimizer = torch.optim.Adam(func.parameters(), lr=1e-3)\n",
    "p_z0 = torch.distributions.MultivariateNormal(\n",
    "        loc=torch.tensor([0.0, 0.0]).to(device),\n",
    "        covariance_matrix=torch.tensor([[1., 0.0], [0.0, 1.]]).to(device)\n",
    "    )\n",
    "pbar = tqdm(total=epoches)\n",
    "total_loss = []\n",
    "with pbar:\n",
    "    for i in range(epoches):\n",
    "        optimizer.zero_grad()\n",
    "        x, logp_diff_t1 = get_batch(512)\n",
    "        z_t, logp_diff_t = odeint(\n",
    "            func,\n",
    "            (x, logp_diff_t1),\n",
    "            torch.linspace(t1, t0, 10).type(torch.float32).to(device),\n",
    "            atol=1e-5,\n",
    "            rtol=1e-5,\n",
    "            method='midpoint',\n",
    "            adjoint_method='midpoint'\n",
    "        )\n",
    "        z_t0, logp_diff_t0 = z_t[-1], logp_diff_t[-1]\n",
    "        logp_x = p_z0.log_prob(z_t0).to(device) - logp_diff_t0.view(-1)\n",
    "        loss = -logp_x.mean(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Epoch: %i, Loss: %.3f\" % (i, loss.item()))\n",
    "        total_loss.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flow(func, num_timestemps=60, name='simple_cnf_ode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "    \"\"\"Adapted from the NumPy implementation at:\n",
    "    https://gist.github.com/rtqichen/91924063aa4cc95e7ef30b3a5491cc52\n",
    "    \"\"\"\n",
    "    def __init__(self, in_out_dim, hidden_dim, width):\n",
    "        super().__init__()\n",
    "        self.in_out_dim = in_out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.width = width\n",
    "        self.hyper_net = HyperNetwork(in_out_dim, hidden_dim, width)\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0]\n",
    "        logp_z = states[1]\n",
    "\n",
    "        batchsize = z.shape[0]\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)\n",
    "\n",
    "            W, B, U = self.hyper_net(t)\n",
    "\n",
    "            Z = torch.unsqueeze(z, 0).repeat(self.width, 1, 1)\n",
    "\n",
    "            h = torch.tanh(torch.matmul(Z, W) + B)\n",
    "            dz_dt = torch.matmul(h, U).mean(0)\n",
    "\n",
    "            dlogp_z_dt = -trace_df_dz(dz_dt, z).view(batchsize, 1)\n",
    "\n",
    "        return (dz_dt, dlogp_z_dt)\n",
    "\n",
    "\n",
    "def trace_df_dz(f, z):\n",
    "    \"\"\"Calculates the trace of the Jacobian df/dz.\n",
    "    Stolen from: https://github.com/rtqichen/ffjord/blob/master/lib/layers/odefunc.py#L13\n",
    "    \"\"\"\n",
    "    sum_diag = 0.\n",
    "    for i in range(z.shape[1]):\n",
    "        sum_diag += torch.autograd.grad(f[:, i].sum(), z, create_graph=True)[0].contiguous()[:, i].contiguous()\n",
    "\n",
    "    return sum_diag.contiguous()\n",
    "\n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"Hyper-network allowing f(z(t), t) to change with time.\n",
    "\n",
    "    Adapted from the NumPy implementation at:\n",
    "    https://gist.github.com/rtqichen/91924063aa4cc95e7ef30b3a5491cc52\n",
    "    \"\"\"\n",
    "    def __init__(self, in_out_dim, hidden_dim, width):\n",
    "        super().__init__()\n",
    "\n",
    "        blocksize = width * in_out_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 3 * blocksize + width)\n",
    "\n",
    "        self.in_out_dim = in_out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.width = width\n",
    "        self.blocksize = blocksize\n",
    "\n",
    "    def forward(self, t):\n",
    "        # predict params\n",
    "        params = t.reshape(1, 1)\n",
    "        params = torch.tanh(self.fc1(params))\n",
    "        params = torch.tanh(self.fc2(params))\n",
    "        params = self.fc3(params)\n",
    "\n",
    "        # restructure\n",
    "        params = params.reshape(-1)\n",
    "        W = params[:self.blocksize].reshape(self.width, self.in_out_dim, 1)\n",
    "\n",
    "        U = params[self.blocksize:2 * self.blocksize].reshape(self.width, 1, self.in_out_dim)\n",
    "\n",
    "        G = params[2 * self.blocksize:3 * self.blocksize].reshape(self.width, 1, self.in_out_dim)\n",
    "        U = U * torch.sigmoid(G)\n",
    "\n",
    "        B = params[3 * self.blocksize:].reshape(self.width, 1, 1)\n",
    "        return [W, B, U]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0);\n",
    "np.random.seed(0);\n",
    "torch.manual_seed(0);\n",
    "torch.cuda.manual_seed(0);\n",
    "torch.cuda.manual_seed_all(0);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epoches = 1000\n",
    "t0, t1 = 0, 1\n",
    "func = CNF(2, 32, 64).to(device)\n",
    "optimizer = torch.optim.Adam(func.parameters(), lr=1e-3)\n",
    "p_z0 = torch.distributions.MultivariateNormal(\n",
    "        loc=torch.tensor([0.0, 0.0]).to(device),\n",
    "        covariance_matrix=torch.tensor([[1., 0.0], [0.0, 1.]]).to(device)\n",
    "    )\n",
    "pbar = tqdm(total=epoches)\n",
    "total_loss = []\n",
    "with pbar:\n",
    "    for i in range(epoches):\n",
    "        optimizer.zero_grad()\n",
    "        x, logp_diff_t1 = get_batch(512)\n",
    "        z_t, logp_diff_t = odeint(\n",
    "            func,\n",
    "            (x, logp_diff_t1),\n",
    "            torch.tensor([t1, t0]).type(torch.float32).to(device),\n",
    "            atol=1e-5,\n",
    "            rtol=1e-5,\n",
    "            method='dopri5',\n",
    "            adjoint_method='dopri5'\n",
    "        )\n",
    "        z_t0, logp_diff_t0 = z_t[-1], logp_diff_t[-1]\n",
    "        logp_x = p_z0.log_prob(z_t0).to(device) - logp_diff_t0.view(-1)\n",
    "        loss = -logp_x.mean(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Epoch: %i, Loss: %.3f\" % (i, loss.item()))\n",
    "        total_loss.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flow(func, num_timestemps=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cdm\"></a>\n",
    "## Continious Diffusion Models\n",
    "\n",
    "Вспомним еще раз как выглядит классическая модель DDPM\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/c880aa21c1f781243db9d59b3f72664019f50e8d/unit6/data/ddpm_plot.png\" alt=\"Схема диффузии\" style=\"width:100%\">\n",
    "<figcaption align = \"center\">Иллюстрация работы DDPM модели.</figcaption>\n",
    "</figure>\n",
    "Имеется прямой и обратный процесс. Прямой процесс зашумляет объект, а обратный — восстанавливает его в исходное состояние. Посмотрим как это может выглядить с непрерывным временем.\n",
    "\n",
    "<a name=\"forward\"></a>\n",
    "### Прямой процесс диффузии\n",
    "\n",
    "В дискретных моделях прямой процесс (зашумление) имеет вид\n",
    "$$\n",
    "x_i = \\sqrt{1-\\beta_i}x_{i-1} + \\sqrt{\\beta_i}z_{i-1}\n",
    "$$\n",
    "где $i=1,2,\\dots,N$. При $N\\to\\infty$ получим, что\n",
    "$$\n",
    "dx = -\\dfrac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dw\n",
    "$$\n",
    "что имеет вид СДУ с дрифтом $f(x(t), t) = -\\frac{1}{2}\\beta(t)x$ и диффузией $g(t) = \\sqrt{\\beta(t)}$.\n",
    "\n",
    "<a name=\"reverse\"></a>\n",
    "### Обратный процесс диффузии\n",
    "\n",
    "Обратный процесс непрерывной диффузии имеет вид\n",
    "$$\n",
    "dx = [f(x, t) - g(t)^2 \\nabla_x\\log{p_t(x)}]dt +g(t)d\\bar w\n",
    "$$\n",
    "где $d\\bar w$ — Винерский обратный процесс (от $T$ до $0$), а $dt$ — отрицательное прирощение по времени. \n",
    "\n",
    "Так как функции $f(x, t)$ и $g(t)$ мы значем, то можно нейронной сетью аппроксимировать неизвестный член $\\nabla_x\\log{p_t(x)}\\approx s_{\\theta}(x(t), t)$ с которым обратный процесс диффузии становится \n",
    "$$\n",
    "dx = [f(x, t) - g(t)^2 s_{\\theta}(x(t), t)]dt +g(t)d\\bar w\n",
    "$$\n",
    "\n",
    "Казалось бы, что все необходимое есть на руках. Сначала решаем прямое СДУ, а потом от результата считаем обратное СДУ и считаем лосс для обучения модели. Однако такой подход будет очень сложный в вычислительном плане и имеет смысл его упростить.\n",
    "\n",
    "<a name=\"sgm\"></a>\n",
    "### Score-Based Generative Modeling Throught SDE\n",
    "\n",
    "Так как в нашем процессе неизвестным является только величина $\\nabla_x\\log{p_t(x)}$ называемая функцией оценки (score function) давайте обучать модель оценивать этот скор. Тогда наша задача оптимизации имеет следующий вид\n",
    "$$\n",
    "\\theta^* = \\operatorname*{argmin}_\\theta\\mathbb{E}_t\\{\\lambda(t)\\mathbb{E}_{x(0)}\\mathbb{E}_{x(T)|x(0)}[||s_\\theta(x, t)-\\nabla_{x(t)}\\log{p_{t}(x)}||_2^2]\\}\n",
    "$$\n",
    "здесь $p_{t}(x)$ — распределение $x(t)$. \n",
    "\n",
    "Однако так как $p_{t}(x)$ мы не знаем и рассчитать аналитически не можем предлагается его заменить на ядро перехода из момента $0$ в произвольный момент $t$, $p_{0t}(x(t)|x(0))$. Тогда \n",
    "$$\n",
    "\\theta^* = \\operatorname*{argmin}_\\theta\\mathbb{E}_t\\{\\lambda(t)\\mathbb{E}_{x(0)}\\mathbb{E}_{x(T)|x(0)}[||s_\\theta(x, t)-\\nabla_{x(t)}\\log{p_{0t}(x(t)|x(0))}||_2^2]\\}\n",
    "$$\n",
    "где $\\lambda(t): [0, T]\\to\\mathbb{R}^+$, выбираем его пропорционально $\\lambda(t) \\propto 1/\\mathbb{E}[||\\nabla_{x(t)}\\log{p_{0t}(x(t)|x(0))}||_2^2]$, а ядро перехода имеет следующий вид (в зависимости от модели)\n",
    "$$\n",
    "p_{0t}(x(t)|x(0)) = \n",
    "\\begin{cases}\n",
    "\\mathcal{N}(x(t); x(0), [\\sigma^2(t)-\\sigma^2(0)]I) & \\text{VE SDE}\\\\\n",
    "\\mathcal{N}(x(t); x(0)e^{-\\frac{1}{2}\\int_0^t\\beta(s)ds}, I - Ie^{-\\int_0^t\\beta(s)ds}) & \\text{VP SDE}\\\\\n",
    "\\mathcal{N}(x(t); x(0)e^{-\\frac{1}{2}\\int_0^t\\beta(s)ds}, [1 - e^{-\\int_0^t\\beta(s)ds}]^2I) & \\text{sub-VP SDE}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "А интеграл по $\\beta(t)$ легко считается аналитически. В статье используется лиенйная функция $\\beta(t)$ так как некоторые свойства модели возможны только для афинных преобразований. Если $\\beta(t)$ — линейная функция $\\beta(t) = \\beta(0) + t\\cdot(\\beta(T) - \\beta(0))$. Тогда интеграл\n",
    "$$\n",
    "\\int\\limits_0^t \\beta(s)ds = \\int\\limits_0^t [\\beta(0) + s\\cdot(\\beta(T) - \\beta(0))]ds = \\beta(0)\\cdot t + (\\beta(T) - \\beta(0))\\cdot\\dfrac{t^2}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Lambda, Resize, Compose, Normalize\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from torchsde import sdeint\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.95\n",
    "\n",
    "def dequantize(x, nvals=256):\n",
    "        \"\"\"[0, 1] -> [0, nvals] -> add uniform noise -> [0, 1]\"\"\"\n",
    "        noise = x.new().resize_as_(x).uniform_()\n",
    "        x = x * (nvals - 1) + noise\n",
    "        x = x / nvals\n",
    "        return x\n",
    "\n",
    "def postprocess(x, alpha=0.95, clamp=True):\n",
    "    x = (x.sigmoid() - alpha) / (1 - 2 * alpha)\n",
    "    return x.clamp(min=0., max=1.) if clamp else x\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    dequantize,\n",
    "    Lambda(lambda x: alpha + (1 - 2 * alpha) * x),\n",
    "    Lambda(lambda x: (x / (1 - x)).log()),\n",
    "])\n",
    "\n",
    "mnist = MNIST('../data/', download=True, transform=transform)\n",
    "# train, _ = torch.utils.data.random_split(mnist, [10000, 50000])\n",
    "train = DataLoader(mnist, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sample = next(iter(train))[0].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreMatching(nn.Module):\n",
    "\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"stratonovich\" # \"ito\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        input_size=(1,28,28),\n",
    "        beta_min=0.1,\n",
    "        beta_max=20,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.device = device\n",
    "        self.t0 = 0.\n",
    "        self.t1 = 1.\n",
    "\n",
    "    def _beta(self, t):\n",
    "        \"\"\"linear beta(t)\"\"\"\n",
    "        return self.beta_min + t * (self.beta_max - self.beta_min)\n",
    "\n",
    "    def _int_beta(self, t):\n",
    "        \"\"\"integrate betas in-time\"\"\"\n",
    "        return self.beta_min * t + 0.5 * t ** 2 * (self.beta_max - self.beta_min)\n",
    "\n",
    "    def analytical_drift(self, t, y):\n",
    "        \"\"\"Drift of SDE\"\"\"\n",
    "        return -0.5 * self._beta(t) * y\n",
    "\n",
    "    def analytical_diffusion(self, t, y):\n",
    "        \"\"\"Diffusion of SDE\"\"\"\n",
    "        return self._beta(t).sqrt().repeat(y.size())\n",
    "\n",
    "    def analytical_mean(self, t, y0):\n",
    "        \"\"\"sub-VP SDE mean of p0t(y(t)|y(0))\"\"\"\n",
    "        return y0 * (- 0.5 * self._int_beta(t)).exp()\n",
    "\n",
    "    def analytical_var(self, t, y):\n",
    "        \"\"\"sub-VP SDE varience of p0t(y(t)|y(0))\"\"\"\n",
    "        return 1 - 1 / self._int_beta(t).exp()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def analytical_score(self, y1, t, y0):\n",
    "        \"\"\"nabla_xt p0t(y(t)|y(0))\"\"\"\n",
    "        mean = self.analytical_mean(t, y0)\n",
    "        var = self.analytical_var(t, y0)\n",
    "        return - (y1 - mean) / var.clamp_min(1e-5)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def noise_sample(self, t, y):\n",
    "        \"\"\"sample noised data with p0t(x(t)|x(0))\"\"\"\n",
    "        mean = self.analytical_mean(t, y)\n",
    "        var = self.analytical_var(t, y)\n",
    "        noise = torch.randn_like(mean)\n",
    "        return mean + noise * var.sqrt()\n",
    "\n",
    "    def model_score(self, t, y):\n",
    "        \"\"\"s(y, t) approximation of score\"\"\"\n",
    "        if t.dim() == 0:\n",
    "            t = t.repeat(y.shape[0])\n",
    "        return self.model(t, y)\n",
    "\n",
    "    def model_drift(self, t, y):\n",
    "        \"\"\"Reverse SDE drift\"\"\"\n",
    "        score = self.model_score(-t, y.view((-1, *self.input_size))).flatten(1)\n",
    "        return -(self.analytical_drift(-t, y) - self.analytical_diffusion(-t, y) ** 2 * score)\n",
    "\n",
    "    def model_diffusion(self, t, y):\n",
    "        \"\"\"Reverse SDE diffusion\"\"\"\n",
    "        return -self.analytical_diffusion(-t, y)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        \"\"\"Reverse ODE-like SDE\"\"\"\n",
    "        return self.model_drift(t, y) + self.model_diffusion(t, y)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tweedie_correction(self, t, y, dt):\n",
    "        \"\"\"SDE solve corrector\"\"\"\n",
    "        return y + dt ** 2 * self.model_score(t, y)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ode_sample(self, batch_size=64, tau=1., t=None, y=None, dt=1e-2, last_only=True):\n",
    "        \"\"\"ODE sampler\"\"\"\n",
    "        t = torch.tensor([-self.t1, -self.t0], device=self.device) if t is None else t\n",
    "        y = torch.randn(size=(batch_size, *self.input_size), device=self.device) * math.sqrt(tau) if y is None else y\n",
    "        sample = odeint(self, y, t, method=\"rk4\", options={\"step_size\": dt})\n",
    "        return sample[-1] if last_only else sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sde_sample(self, batch_size=64, tau=1., t=None, y=None, dt=1e-2, last_only=True):\n",
    "        \"\"\"SDE sampler\"\"\"\n",
    "        t = torch.tensor([-self.t1, -self.t0], device=self.device) if t is None else t\n",
    "        y = torch.randn(size=(batch_size, *self.input_size), device=self.device) * math.sqrt(tau) if y is None else y\n",
    "        sample = sdeint(self, y, t, dt=dt, names={'drift': 'model_drift', 'diffusion': 'model_diffusion'})\n",
    "        # sample[-1] = self.tweedie_correction(self.t0, sample[-1], dt)\n",
    "        return sample[-1] if last_only else sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetSDE(UNet2DModel):\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return super().forward(y, t).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_time(x0, t0, t1, partitions=1):\n",
    "    u = torch.rand(size=(x0.shape[0], partitions), dtype=x0.dtype, device=x0.device)\n",
    "    u.mul_((t1 - t0) / partitions)\n",
    "    shifts = torch.arange(0, partitions, device=x0.device, dtype=x0.dtype)[None, :]\n",
    "    shifts.mul_((t1 - t0) / partitions).add_(t0)\n",
    "    t = (u + shifts).reshape(-1)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed(0);\n",
    "np.random.seed(0);\n",
    "torch.manual_seed(0);\n",
    "torch.cuda.manual_seed(0);\n",
    "torch.cuda.manual_seed_all(0);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epoches = 10\n",
    "t0, t1 = 0, 1\n",
    "denoiser = UNetSDE(\n",
    "    sample_size=28,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 64, 64),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        ),\n",
    ")\n",
    "diffusion = ScoreMatching(denoiser, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(diffusion.parameters(), lr=1e-4)\n",
    "pbar = tqdm(total=epoches)\n",
    "total_loss = []\n",
    "with pbar:\n",
    "    for i in range(epoches):\n",
    "        epoch_loss = []\n",
    "        for sample, _ in train:\n",
    "            optimizer.zero_grad()\n",
    "            sample = sample.to(device)\n",
    "            time = sample_time(sample, t0, t1).unsqueeze(1).to(device)\n",
    "            lambda_t = diffusion.analytical_var(time, None).squeeze(1)\n",
    "            noised_sample = diffusion.noise_sample(time, sample.flatten(1))\n",
    "            real_score = diffusion.analytical_score(noised_sample, time, sample.flatten(1))\n",
    "            fake_score = diffusion.model_score(time.squeeze(1), noised_sample.view(sample.size()))\n",
    "            loss = (lambda_t * ((fake_score.flatten(1) - real_score) ** 2).flatten(start_dim=1).sum(dim=1)).mean(dim=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        pbar.update(1)\n",
    "        total_loss.append(np.mean(epoch_loss))\n",
    "        pbar.set_description(\"Epoch: %i, Loss: %.3f\" % (i, total_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor([-1., -0.]).to(device)\n",
    "noise_sample = torch.randn((64, 784)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ys = sdeint(diffusion, noise_sample, ts, dt=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sample = ys[1].view((64, *sample.size()[1:]))\n",
    "plt.imshow(ToPILImage()(make_grid(postprocess(gen_sample))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweedie_sample = gen_sample + 1e-2 ** 2 * fake_model.score(ts[-1], gen_sample)\n",
    "plt.imshow(ToPILImage()(make_grid(postprocess(tweedie_sample), normalize=True)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor(torch.linspace(-1, 0, 60)).to(device)\n",
    "noise_sample = torch.randn((64, 784)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ys = torchsde.sdeint(fake_model, noise_sample, ts, dt=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_plot(ys, t, name: str = 'conn_ddpm'):\n",
    "    Path('imgs').mkdir(exist_ok=True)\n",
    "    if ys.is_cuda:\n",
    "        ys = ys.cpu()\n",
    "    for y, t in zip(ys, t):\n",
    "        plt.imshow(ToPILImage()(make_grid(postprocess(y), normalize=True)))\n",
    "        plt.title(\"%.2fs\" % t)\n",
    "        plt.axis('off')\n",
    "        plt.margins(0, 0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"imgs/ddpm-%.2f.png\" % t)\n",
    "        plt.close()\n",
    "\n",
    "    images = []\n",
    "    filenames = sorted(list(Path('imgs/').glob('*.png')))\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(str(filename)))\n",
    "    imageio.mimsave(f'{name}.gif', images, duration=5, format='GIF')\n",
    "    shutil.rmtree('imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_plot(ys.view((60, 64, 1, 28, 28)), torch.flip(-ts, (0, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задания:\n",
    "\n",
    "1. Попробовать другие методы решения ОДУ и СДУ (поменять солвер)\n",
    "2. Поиграться с числом шагов семплирования\n",
    "3. Попробовать поменять в SGM ядро перехода (VP или VE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
