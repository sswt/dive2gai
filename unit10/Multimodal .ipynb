{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78196233",
   "metadata": {},
   "source": [
    "# Развитие трансформеров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8443f",
   "metadata": {},
   "source": [
    "* Этап 1: Применение моделей на основе Transformer для разнообразных текстовых задач\n",
    "    + BERT\n",
    "    + GPT-2\n",
    "* Этап 2: Адаптация успешной архитектуры к обработке других модальностей\n",
    "для решения дискриминативных задач и генерации признаков (CV, ASR, . . . )\n",
    "    + CLIP\n",
    "    + HuBERT\n",
    "* Этап 3: Генерация текста композитной моделью, принимающей на вход\n",
    "объекты различных модальностей\n",
    "    + Flamingo\n",
    "    + BLIP-2\n",
    "* Этап 4: Добавление возможности генерировать объекты разных модальностей\n",
    "    + NExT-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3bb60",
   "metadata": {},
   "source": [
    "# Примеры мультимодальных задач\n",
    "- Близость между текстом и изображением / аудиозаписью (в т.ч. для\n",
    "классификации) \n",
    "- Определение связи между объектом на изображении и описывающими его словами из описания (Visual Grounding)\n",
    "- Ответы на вопросы по изображениям (VQA)\n",
    "- Рассуждения по изображениям и их описаниям (Visual Reasoning)\n",
    "- Генерация описаний к изображениям (Image Captioning)\n",
    "- Распознавание символов / фонем по аудиозаписи\n",
    "- Генерация изображений по тексту [и изображению]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b416c5",
   "metadata": {},
   "source": [
    "# Мультимодальный бенчмарк:\n",
    "- ScienceQA\n",
    "- NLVR2\n",
    "- COCO\n",
    "- Один из наиболее свежих и полных MMBench (английский язык, 20 задач, ~3K примеров)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1b7c1",
   "metadata": {},
   "source": [
    "# Этап 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a70ae3",
   "metadata": {},
   "source": [
    "## Vision Transformer \n",
    "\n",
    "- Кодировщик Transformer, на входе изображение, на выходе — его класс\n",
    "- Вход:\n",
    "     + изображение разбивается на квадраты (патчи)\n",
    "     + каждый патч кодируется линейным слоем в вектор (альтернатива — CNN)\n",
    "     + к вектору патча добавляется обучаемый позиционный эмбеддинг\n",
    "- В CLS-токене на предобучении предсказывается класс изображения, на дообучении голова заменяется на новую\n",
    "- При дообучении на целевые задачи повышается размерность изображений ⇒ увеличивается длина последовательности\n",
    "- Эмбеддинги для новых позиций получаются 2D-интерполяцией уже обученных с учётом координат патча на изображении\n",
    "\n",
    "![VIT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/VIT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe42b93",
   "metadata": {},
   "source": [
    "![Metrics](https://habrastorage.org/r/w1560/getpro/habr/upload_files/8fc/dff/5b1/8fcdff5b12cb3fcf92f3d2a23d25f72c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66052895",
   "metadata": {},
   "source": [
    "[Vision Transformer From Scratch](https://colab.research.google.com/github/josebenitezg/vision-transformer-from-scratch/blob/main/vision_transformers.ipynb#scrollTo=qGHq-Nl0rrFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765b826",
   "metadata": {},
   "source": [
    "## CLIP  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ceaf93",
   "metadata": {},
   "source": [
    "- Zero-shot классификатор изображений, регулируемый текстовыми промптами\n",
    "- Два кодировщика:\n",
    "     + для изображений (ResNet с вниманием или ViT)\n",
    "     + для текста (GPT-2 с контекстом 76)\n",
    "- На выходе каждого один вектор: для изображения и для текста его класса/описания\n",
    "- Они переводятся обучаемыми линейными слоями в общее пространство и нормализуются\n",
    "- На обучении между всеми парами «текст»-«изображения» считается матрица близостей и применяется CE-loss по обеим её размерностям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff86cb",
   "metadata": {},
   "source": [
    "- На инференсе модели подаются изображение и N текстов-промптов вида «это собака/кот/машина/. . . »\n",
    "- Вектор изображения умножается на векторы всех промптов и ответ выбирается по лучшему значению близости\n",
    "- Работает в zero-shot хорошо на доменах и классах из обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce4dc8",
   "metadata": {},
   "source": [
    "![CLIP](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/CLIP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35095b",
   "metadata": {},
   "source": [
    "![MMBench](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/MMBench.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c040a",
   "metadata": {},
   "source": [
    "# Wav2Vec 2.0, 2020 (Facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20751ca7",
   "metadata": {},
   "source": [
    "Мы впервые показываем, что обучение мощных представлений только на аудиозаписях речи с последующей тонкой настройкой на транскрибированной речи может превзойти лучшие semi-supervised методы, будучи при этом концептуально более простым. wav2vec 2.0 маскирует речевой вход в латентном пространстве и решает контрастную задачу, определенную над квантованием латентных представлений, которые совместно обучаются. Эксперименты с использованием всех помеченных данных Librispeech достигают 1,8/3,3 WER на чистых/других тестовых наборах. При уменьшении количества помеченных данных до одного часа wav2vec 2.0 превосходит предыдущий уровень техники на 100-часовом подмножестве, используя при этом в 100 раз меньше помеченных данных. При использовании всего десяти минут меченых данных и предварительном обучении на 53 тыс. часов немеченых данных WER достигает 4,8/8,2. Это демонстрирует возможность распознавания речи при ограниченном количестве помеченных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba0927",
   "metadata": {},
   "source": [
    "- Кодировщик Transformer предобучается в стиле BERT на неразмеченных аудиозаписях и дообучается на размеченных\n",
    "- Вход:\n",
    "    1. центрированный сырой сигнал WAV\n",
    "    2. temporal CNN + LayerNorm + GeLU на нем для «токенизации» фреймов\n",
    "    3. CNN + GeLU на выходах 2 для получения векторов позиций\n",
    "    4. выходы 2 и 3 складываются и нормализуется\n",
    "- В качестве unsupervised target используется квантизация векторов входа\n",
    "- Квантизованное представление:\n",
    "    1. заводится G словарей обучаемых кодов-векторов, по V кодов каждом\n",
    "    2. для входного вектора диффиринцируемо выбирается лучший код из каждого словаря (с помощью Gumbel-softmax)\n",
    "    3. G векторов конкатенируются и проецируются в размерность входа\n",
    "    4. получается квантизованное представление исходного вектора\n",
    "    5. векторы-коды учатся вместе с моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30444e",
   "metadata": {},
   "source": [
    "**Предобучение**:\n",
    "- N спанов входа по M векторов с перекрытиями маскируются\n",
    "- для маскированных входов генерируется квантизованные векторы Q\n",
    "- модель учится генерировать для центрального токена спана вектор, который ближе к его вектору из Q, чем к векторам K других маскированных токенов (дистракторов)\n",
    "- регуляризатор — максимизация энтропии в распределениях на векторах-кодах (все коды должны участвовать)\n",
    "\n",
    "**Дообучение**:\n",
    "- квантизация убирается, добавляется полносвязный слой поверх выходов кодировщика\n",
    "- на выходах этого слоя идёт обучение на размеченных аудиозаписях\n",
    "\n",
    "\n",
    "С небольшими потерями качества удалось уменьшить объём размеченных данных на этапе дообучения в 10-1000 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce8076",
   "metadata": {},
   "source": [
    "![wav2vec](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/wav2vec_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4992a",
   "metadata": {},
   "source": [
    "![wav2vec](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/wav2vec_2.png)\n",
    "![wav2vec](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/wav2vec_3.png)\n",
    "![wav2vec](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/wav2vec_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17634b",
   "metadata": {},
   "source": [
    "Во время предварительного обучения мы изучаем представления речевого звука, решая контрастную задачу $L_m$, которая требует определить истинное квантованное латентное представление речи для маскированного временного шага в наборе дистракторов. К этой задаче добавляется потеря разнообразия кодовой книги $L_d$, чтобы побудить модель использовать записи кодовой книги одинаково часто.\n",
    "$$ L = L_m + \\alpha L_d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e65b1",
   "metadata": {},
   "source": [
    "**Contrastive loss**: необходимо определить истинное квантованное латентное представление речи - $q_t$ в наборе из K+1 кандидатов: реального $q_t$ и K дистракторов (равномерно отобранных из других замаскированных временных шагов того же произнесения). сделайте $q_t$ как можно ближе, а другие дистракторы как можно дальше от $c_t$.\n",
    "![loss](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/closs.png)\n",
    "\n",
    "**Diversity Loss**: Поощрение использования равного количества записей в каждой из кодовых книг G путем максимизации энтропии усредненного распределения softmax L по записям кодовых книг (G V ) для каждой кодовой книги в батче высказываний.\n",
    "![loss](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/dloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154f397",
   "metadata": {},
   "source": [
    "![mask](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/mask_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9cf72",
   "metadata": {},
   "source": [
    "![fine](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/fine_tune_wav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7499b",
   "metadata": {},
   "source": [
    "![wav2vec](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/wav2vec_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64124bc2",
   "metadata": {},
   "source": [
    "## HuBERT, 2021 (Facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a6aff",
   "metadata": {},
   "source": [
    "- Идея та же, что и в Wav2Vec 2.0: учить модель для маскированных фреймов предсказывать альтернативные представления\n",
    "- Получаются эти представления, — векторы-коды, — с помощью K-means\n",
    "- Архитектура:\n",
    "    1. CNN-кодировщик аудиосигнала\n",
    "    2. кодировщик Transformer на его выходах\n",
    "    3. слой векторов-кодов\n",
    "    4. слой проекции выходов кодировщика в размерность кодов\n",
    "- При обучении чередуются два этапа:\n",
    "- учится модель K-means на векторах MFCC-признаков фреймов исходного аудиозаписи в WAV\n",
    "- учится основная модель-кодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a687544",
   "metadata": {},
   "source": [
    "**Обучение кодировщика**:\n",
    "- фреймы кодируются CNN и маскируются как в Wav2Vec 2.0\n",
    "- вход подаётся в модель, выход проецируется в размерность векторов-кодов\n",
    "- между выходным вектором каждого маскированного фрейма и всеми векторами-кодами считается косинусное расстояние\n",
    "- результат идёт в softmax, на выходном распределении считается CE-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77037f",
   "metadata": {},
   "source": [
    "![Hubert](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/HuBert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d405bd",
   "metadata": {},
   "source": [
    "- Этапы повторяются итеративно, начиная со 2-й итерации K-means учится на представлениях с промежточных слоёв текущей версии кодировщика, а не на векторах MFCC-признаков\n",
    "- Для повышения качества вместо одного K-means учатся несколько с разным числом кластеров\n",
    "- Разбиение на два этапа упрощает и стабилизирует обучение, больше не требуется сложный loss, как в Wav2Vec 2.0\n",
    "- Предобученная модель дообучается (с замороженной CNN и без K-means) на размеченных записях"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bde628",
   "metadata": {},
   "source": [
    "## Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a76ab0",
   "metadata": {},
   "source": [
    "![att](./data_m/CrossAttention_1.png)\n",
    "\n",
    "Cross Attention - это:\n",
    "\n",
    "- механизм внимания в архитектуре Transformer, который смешивает две различные последовательности встраивания.\n",
    "- две последовательности должны иметь одинаковое измерение\n",
    "- две последовательности могут быть разных модальностей (например, текст, изображение, звук)\n",
    "- одна из последовательностей определяет выходную длину, поскольку играет роль входного запроса\n",
    "- другая последовательность генерирует ключ и значение\n",
    "![att](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/CrossAttention_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da664398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class CrossAttention(nn.Module):\n",
    "    r\"\"\"\n",
    "    A cross attention layer.\n",
    "\n",
    "    Parameters:\n",
    "        query_dim (`int`): The number of channels in the query.\n",
    "        cross_attention_dim (`int`, *optional*):\n",
    "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
    "        heads (`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.\n",
    "        dim_head (`int`,  *optional*, defaults to 64): The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        bias (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
    "    \"\"\"\n",
    "    query = attn.to_q(hidden_states)\n",
    "    query = attn.head_to_batch_dim(query)\n",
    "\n",
    "    encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
    "    key = attn.to_k(encoder_hidden_states)\n",
    "    value = attn.to_v(encoder_hidden_states)\n",
    "    key = attn.head_to_batch_dim(key)\n",
    "    value = attn.head_to_batch_dim(value)\n",
    "\n",
    "    attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "    hidden_states = torch.bmm(attention_probs, value)\n",
    "    \n",
    "    [name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ca205",
   "metadata": {},
   "source": [
    "## Whisper, 2022 (OpenAI)\n",
    "CNN + Transformer Encoder + Transformer Decoder / 39M, 74M, 244M, 769M, 1550M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7f3a9",
   "metadata": {},
   "source": [
    "На текущий момент одно из лучших открытых решений для распознавания речи, которое стоит несколько в стороне от предыдущих работ этого раздела. Ключевые идеи:\n",
    "\n",
    "- unsupervised предобучение с дообучением под конкретный датасет и слабый декодером плохо генерализируется и не работает в zero-shot\n",
    "\n",
    "- хорошо размеченных данных мало, но можно брать размеченные плохо, их тоже немного, но можно попытаться их нагенерировать автоматически\n",
    "\n",
    "- можно учить модель только на размеченных (плохо или хорошо) данных без какого-либо unsupervised предобучения, и в этом случаем модель будет работать в zero-shot режиме без"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07609b6e",
   "metadata": {},
   "source": [
    "Сбор и подготовка данных для Whisper:\n",
    "\n",
    "- данные набираются из сети, предварительная фильтрация и обработка:\n",
    "\n",
    "    - удаление ASR\n",
    "\n",
    "    - записи, в которых язык текста не совпадает с языком звука, переносятся в отдельный набор данных\n",
    "\n",
    "    - записи без текста тоже складываются в отдельный набор\n",
    "\n",
    "- записи нарезаются на файлы по 30 секунд с соответствующей транскрибацией\n",
    "\n",
    "- в результате получается несколько наборов данных:\n",
    "\n",
    "    - текст-запись (с метками времени и без) на английском и других языках\n",
    "\n",
    "    - текст или запись (не обязательно с голосом)\n",
    "\n",
    "    - текст-запись (с метками времени и без), в которых записи на разных языках, а текст на английском\n",
    "\n",
    "- после первой итерации обучения данные повторно фильтруются на основании анализа качества работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f63e36",
   "metadata": {},
   "source": [
    "Архитектура модели:\n",
    "\n",
    "- на входе Log-Mel-спектрограмма на фреймах 25ms со stride 10ms, исходный аудисигнал нормируется в [-1, 1] со средним около 0 по датасету\n",
    "\n",
    "- CNN-кодировщик (2 свёрточных слоя с активацией GeLU)\n",
    "\n",
    "- к выходам добавляются синусоидальные позиционные эмбеддинги\n",
    "\n",
    "- кодировщик трансформера с pre-LayerNorm + LayerNorm на итоговых выходах сети\n",
    "\n",
    "- декодировщик трансформера на текстах с обучаемыми позиционными эмбеддингами и cross-attention на выходах кодировщика\n",
    "\n",
    "- токенизация byte-level BPE из GPT-2, переобучена для мультиязычности без изменения размера словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406999a",
   "metadata": {},
   "source": [
    "![Whisper](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Whisper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c7c5c",
   "metadata": {},
   "source": [
    "![Whisper](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Whisper_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c641da5",
   "metadata": {},
   "source": [
    "![Whisper](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Whisper_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46a469",
   "metadata": {},
   "source": [
    "Задачи обучения (используется универсальный входной формат для всех):\n",
    "- распознавание английской речи в английский текст\n",
    "\n",
    "- распознавание речи на языке Х в текст на языке Х (не английский)\n",
    "\n",
    "- детекция речи\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2177c",
   "metadata": {},
   "source": [
    "[DEMO](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc2aa9",
   "metadata": {},
   "source": [
    "## OmniMAE, 2022 (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d56ac7",
   "metadata": {},
   "source": [
    "- Единая модель для изображений и видео на основе ViT\n",
    "- Объекты рассматриваются как 4D тензоры (T × H × W × 3), у изображений размерность времени всегда 1\n",
    "- Каждый объект разбивается на N квадратных патчей, из которых M маскируются\n",
    "- N - M патчей со своими позициями передаются в кодировщик ViT, на выходе векторы\n",
    "- Эти векторы дополняются M векторами маски, и итоговый набор из N векторов с позициями идёт в декодировщик\n",
    "- Задача декодировщика — предсказать все пиксели каждого изображения/кадра (MSE на нормализованных значениях пикселя)\n",
    "- Для ускорения обучения каждый объект обрабатывается при обучении несколько раз с разными масками\n",
    "\n",
    "![omnimae](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Omnimae.png)\n",
    "\n",
    "- Ставились эксперименты с разными масками, в итоге для обеих модальностей использовалась Random\n",
    "\n",
    "![omnimae](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Omnimaemask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c154f3",
   "metadata": {},
   "source": [
    "# ImageBind, 2023 (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47f2f9",
   "metadata": {},
   "source": [
    "- Пары вида «изображение»-«объект» для объектов 6 модальностей, кодируются и обучаются на InfoNCE loss (верная пара против прочих в батче)\n",
    "- Кодирование объектов:\n",
    "    - изображения — ViT (заморожен)\n",
    "    - видео (2 фрейма) — ViT (заморожен)\n",
    "    - аудио (2 секунды) — мел-спектрограммы + ViT\n",
    "    - карты глубины (изображения) — ViT\n",
    "    - карты температуры (изображения) — ViT\n",
    "    - IMU — 1D свёртка + кодировщик Transformer\n",
    "    - текст — CLIP (заморожен)\n",
    "- У видео и изображений один кодировщик, у всех остальных — свои, у каждого на выходе обучаемая проекция в единую размерность + параметр «температуры» модальности\n",
    "- Для части пар модальностей есть готовые датасеты, часть данных (для специфических модальностей) собиралась в рамках работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f0206",
   "metadata": {},
   "source": [
    "- В итоговом пространстве можно складывать векторы разных модальностей:\n",
    "![demo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/imagebind_demo1.png)\n",
    "  \n",
    "- Внешней модели (сегментации) с CLIP на входе можно подсунуть вектор аудио:\n",
    "![demo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/imagebind_demo2.png)\n",
    "\n",
    "[DEMO](https://imagebind.metademolab.com/demo?modality=AI2I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ce954",
   "metadata": {},
   "source": [
    "# Этап 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddf224",
   "metadata": {},
   "source": [
    "## SimVLM, 2021 (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb50308",
   "metadata": {},
   "source": [
    "- Полный Transformer, на входе кодировщика изображение и префикс текста, на выходе декодировщика предсказывается суффикс\n",
    "- Изображение векторизуется с помощью CNN (три первых блока ResNet), каждому патчу на выходе соответствует вектор\n",
    "- У токенов изображения и текста на входе свои обучаемые позиционные эмбеддинги\n",
    "- Ко всем токенам вместе применяется обычный self-attention\n",
    "- У токенов изображений дополнительно есть относительный 2D attention в блоке кодировщика\n",
    "- Предобучение авторегрессионное одновременно на текстах и парах «текст»-«описание»\n",
    "- Дообучение на 6 задач с настройкой всех параметров модели\n",
    "\n",
    "![simvlm](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/simvlm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913db690",
   "metadata": {},
   "source": [
    "## CoCa, 2022 (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da3a70",
   "metadata": {},
   "source": [
    "- Кодировщик для изображений (ViT или CNN) + общий декодировщик\n",
    "- На выходе кодировщика векторы патчей изображения и агрегированный с помощью attention pooling общий вектор\n",
    "- Декодировщик разделён на две части по вертикали\n",
    "- Первая половина получает на вход только текст префикса и обрабатывает его как обычно, выдавая векторы обычных токенов и вектор CLS-токена\n",
    "- Векторы текста и изображения объединяются во второй части декодировщика через cross-attention\n",
    "- Используются два лосса с весами 1 и 2 соответственно:\n",
    "- близость между общим вектором изображения и вектором CLS-токена для пары «изображение»-«описание»\n",
    "- авторегрессионный лосс на выходе всего декодировщика\n",
    "- При дообучении можно замораживать кодировщик изображения и доучивать только attention pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b714e",
   "metadata": {},
   "source": [
    "![COCA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/CoCA1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185f191",
   "metadata": {},
   "source": [
    "![COCA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/CoCA2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bae3bf",
   "metadata": {},
   "source": [
    "## Flamingo, 2022 (DeepMind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b006b76",
   "metadata": {},
   "source": [
    "- В основе предобученные кодировщик изображений и LLM (Chinchilla), они замораживаются и дополняются обучаемыми параметрами\n",
    "- Формат входа: [\\<img\\>, ...] text [\\<img\\>, ...] text ...\n",
    "- Кодировщик изображений — NormalizerFree ResNet:\n",
    "- 2D изображение ⇒ 1D вектор\n",
    "- видео ⇒ фреймы ⇒ 2D + обучаемый вектор метки времени ⇒ 1D вектор\n",
    "- На выходах кодировщика изображения — Perceiver Resampler:\n",
    "- преобразует набор векторов любой длины в 64 вектора\n",
    "- векторы изображения подаются в cross-attention, формируя ключи и значения\n",
    "- запросы — из обучаемых 64 латентных векторов\n",
    "- результат проходит через FF-слои и идёт в LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120922c",
   "metadata": {},
   "source": [
    "![Flamingo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Flamingo_1.png)\n",
    "![Flamingo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Flamingo_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15edd60",
   "metadata": {},
   "source": [
    "![PR](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/PR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04781718",
   "metadata": {},
   "source": [
    "- В LLM между замороженными слоями добавляются новые блоки\n",
    "- tanh-гейт регулирует пропускную способность, она стартует с 0 и растёт в процессе обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f025d7",
   "metadata": {},
   "source": [
    "![Flamingo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Flamingo_Prompt.png)\n",
    "![Flamingo](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Flamingo_Prompt2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87ccce",
   "metadata": {},
   "source": [
    "**Особенности внимания**:\n",
    "- токен текста в cross-attention может взаимодействовать только с токенами текста до него и последнего изображения\n",
    "- это делается для устранения зависимости от числа изображение на входе\n",
    "- взаимодействие с прочими изображениями идёт через self-attention LLM\n",
    "- В разных версиях модели обучаемые слои добавляются в LLM после каждого 1-го, каждого 4-го и каждого 7-го замороженного слоя\n",
    "- Модель учится по входу из изображений и текста предсказывать текстовый суффикс (Prefix LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7fd46",
   "metadata": {},
   "source": [
    "## BEiT-3, 2022 (Microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fee13a",
   "metadata": {},
   "source": [
    "Кодировщик Transformer с MoE (Mixture-of-Experts)\n",
    "\n",
    "**Архитектура**:\n",
    "- на входе векторы патчей изображения и векторы токенов текста\n",
    "- первые блоки кодировщика содержат общий self-attention и отдельные FF-слои для каждой модальности\n",
    "- в трёх последних блоках и self-attention, и FF-слои общие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766da8",
   "metadata": {},
   "source": [
    "![BEIT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BEiT-3.png)\n",
    "![BEIT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BEIT-3_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12294c9e",
   "metadata": {},
   "source": [
    "- Кодирование патчей как в BEiT v2: ViT + квантизация (VQ), вектор из ViT заменяется на ближайший из обучаемого набора векторов-кодов \n",
    "- Предобучение на Masked Data Modeling (аналог MLM) на текстах, изображениях и их парах\n",
    "- Аугментации изображений: обрезка, растяжение, изменения цветов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaf1fb",
   "metadata": {},
   "source": [
    "![BEIT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BEIT-3_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97b7ec",
   "metadata": {},
   "source": [
    "## BLIP-2, 2023 (Salesforce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58c8a9",
   "metadata": {},
   "source": [
    "- Замороженные кодировщик изображения (CLIP-ViT) и LLM (OPT или Flan-T5) + обучаемый блок между ними (Q-former)\n",
    "- Q-former извлекает из изображения фиксированное количество векторов (определяется числом обучаемых векторов-запросов Q), т.е. «переводит» изображение в понятные LLM токены\n",
    "- В основе Q-former — BERT, на входе векторы Q и векторы текста T\n",
    "- Векторы изображения участвуют в cross-attention с Q self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e5069",
   "metadata": {},
   "source": [
    "![BLIP](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BLIP_1.png)\n",
    "![BLIP](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BLIP_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e7175",
   "metadata": {},
   "source": [
    "**Предобучение идёт в 2 этапа (учится только Q-former)**:\n",
    "- кодировщик изображения + Q-former\n",
    "- кодировщик изображения + Q-former + LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a84c3",
   "metadata": {},
   "source": [
    "**Задачи первого этапа**:\n",
    "- Близость между парой «текст»-«изображение»\n",
    "    - Q и T кодируются независимо\n",
    "    - для всех пар Q и T в батче на выходе считается близость между всеми Q и вектором CLS-токена T, берётся лучшее значение\n",
    "    - у верных пар близость должна быть лучшей в батче\n",
    "- Генерация текста по изображению:\n",
    "    - векторы Q могут в self-attention смотреть только на себя\n",
    "    - векторы T могут смотреть на все векторы Q и на всю T до себя\n",
    "- Сопоставление изображения и текста\n",
    "    - все векторы в self-attention могут смотреть на всех\n",
    "    - выходные представления Q отдельно подаются в линейный классификатор, логиты усредняются и дают значение качества сопоставления\n",
    "    - для верной пары значение должно быть высоким, для прочих — ниже\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73a062",
   "metadata": {},
   "source": [
    "- Итоговые Q проецируются обучаемым полносвязным слоем в размерность LLM\n",
    "- Они становятся входом LLM и могут быть дополнены токенами текста\n",
    "- На втором этапе полная модель с GPT-like [T5-like] LLM обучается предсказывать суффикс по изображению [и тексту префикса]\n",
    "\n",
    "![BLIP](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/BLIP_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f9c91",
   "metadata": {},
   "source": [
    "## LLaVA, 2023 (University of Wisconsin–Madison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ae16a",
   "metadata": {},
   "source": [
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa280b0d",
   "metadata": {},
   "source": [
    "- Предобученный CLIP-ViT с обучаемым проекционным слоем + LLaMA\n",
    "- Основной упор не на модель, а на подготовку инструкционного датасета\n",
    "    - пар «изображение»-«текст» много, а инструкций по ним — мало\n",
    "    - генерировать вручную долго и дорого, можно использовать GPT-4\n",
    "- Простой поход по генерации одношагового примера по изображению:\n",
    "    - вход: случайная формулировка запроса на описание + изображение\n",
    "    - выход: описание изображения\n",
    "- Продвинутый подход без использования изображения:\n",
    "    - набор данных COCO с изображениями, сегментацией и описаниями\n",
    "    - промпты настраивают GPT-4 по описанию изображения и сегментации сгенерировать 3 типа данных:\n",
    "        - диалог по содержимому изображения\n",
    "        - развёрнутое детальное описание изображения\n",
    "        - сложный вопрос к логике изображения и ответ-рассуждение на него"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86957fb4",
   "metadata": {},
   "source": [
    "Предобучение:\n",
    "- 600К пар конвертируются в одношаговые диалоги простым методом \n",
    "- И CLIP, и LLM заморожены, обучается только проекционный слой для сопоставления векторов изображения пространству эмбеддингов LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dbaac",
   "metadata": {},
   "source": [
    "Дообучение:\n",
    "- 160К инструкционных примеров получаются продвинутым методом\n",
    "- CLIP заморожена, учатcя проекционный слой и LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea821c3f",
   "metadata": {},
   "source": [
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_2.png)\n",
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_3.png)\n",
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_4.png)\n",
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_5.png)\n",
    "![LLAVA](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/LLAVA_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f250d",
   "metadata": {},
   "source": [
    "## PaLI, 2022 (Google)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe05a26",
   "metadata": {},
   "source": [
    "ViT (1.8->4B) (предобученный и замороженный) + mT5 (17B обучаемый) с cross-attention на выходы ViT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1c3c6",
   "metadata": {},
   "source": [
    "Задачи обучения (если возможно — кросс-язычные):\n",
    "- MLM, предсказание спанов, замаскированных одним токеном (как в Т5)\n",
    "- 2 задачи дополнения и генерации alt-текста (текстовый image placeholder)\n",
    "- OCR генерация текста по изображению\n",
    "- генерация по изображению и тексту вопроса ответа на вопрос (VQA)\n",
    "- определение наличия объектов на изображении и генерация спискаобъектов (только для английского)\n",
    "- определение наличия объекта в указанных границах на изображении"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca81785",
   "metadata": {},
   "source": [
    "![PALI](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/PALI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d8941",
   "metadata": {},
   "source": [
    "![PALI](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/PALI_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f0c67",
   "metadata": {},
   "source": [
    "## Kosmos-1, 2023 (Microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5dbc99",
   "metadata": {},
   "source": [
    "- CLIP-ViT для изображений + декодировщик Transformer (Magneto, xPos)\n",
    "- В предобученном CLIP замораживаются все слои, кроме последнего, для фиксации числа токенов изображения встроен Resampler из Flamingo\n",
    "- Формат входа: токены текстов и изображений, ограниченные спецтокенами, число и порядок текстов/изображений произвольные\n",
    "- Данные: тексты, изображения, пары «текст»-«изображение», тексты со вставками изображений\n",
    "- Учится предсказывать по мультимодальному входу текстовый суффикс\n",
    "- В обучении есть чисто текстовая инструктивная часть (120К примеров)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49c3a3",
   "metadata": {},
   "source": [
    "![Kosmos](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/Magneto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ac524",
   "metadata": {},
   "source": [
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0637758",
   "metadata": {},
   "source": [
    "## Kosmos-2, 2023 (Microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f328efd",
   "metadata": {},
   "source": [
    "Архитектура та же, что и Kosmos-1 (CLIP-ViT + декодировщик)\n",
    "Особенности:\n",
    "- возможность модели выделять запрошенные объекты на изображениях\n",
    "- возможность модели фокусироваться на объектах на изображении, которые выделил пользователь\n",
    "\n",
    "Ключевой результат — новый набор данных GrIT (Grounded Image-Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534d920",
   "metadata": {},
   "source": [
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80355fc1",
   "metadata": {},
   "source": [
    "**Сбор датасета**:\n",
    "- выделение существительных (SpaCy), удаление абстрактных понятий\n",
    "- подача результатов и изображения в предобученную модель, выделяющую объекты (GLIP)\n",
    "- выделение GLIP границ объектов, фильтрация результатов (уверенность модели, доля пересечения с другими), исключение примеров без объектов\n",
    "- генерация SpaCy дерева синтаксических зависимостей текста описания\n",
    "- расширение существительных их поддеревьями до выражений, фильтрация результатов (полное вхождение в другое выражение)\n",
    "- пример = выражение + изображение + границы объекта\n",
    "\n",
    "\n",
    "Датасет: 91М изображений, 137М границ объектов, 115М выражений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0672c",
   "metadata": {},
   "source": [
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f927d",
   "metadata": {},
   "source": [
    "**Дискретизация границ объектов**:\n",
    "\n",
    "- изображение фиксированного разрешения делится на маленькие патчи\n",
    "- патч характеризуется координатами своего центра на изображении\n",
    "- для каждого патча в словарь модели добавляются токены\n",
    "- граница объекта = координаты левого верхнего и правого нижнего углов\n",
    "- LLM получает границу на вход в виде токенов патчей этих углов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876a278",
   "metadata": {},
   "source": [
    "**Формат входа**:\n",
    "- \\<p\\>object-text\\</p\\>\\<box\\>\\<left-up-token\\>\\<right-down-token\\>\\</box\\>\n",
    "- на случай нескольких границ для одного объекта есть токен \\<delim\\>\n",
    "- во всём остальном модель повторяет Kosmos-1\n",
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a32c6f",
   "metadata": {},
   "source": [
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-2_1.png)\n",
    "\n",
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/KOSMOS-2_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28201e6",
   "metadata": {},
   "source": [
    "- Модель основана на Kosmos-1, новые данные включены в старый датасет\n",
    "- SFT проводился на данных LLaVA, Unnatural Instructions и FLAN v2 + дополнительные задачи на данных GrIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc62a5",
   "metadata": {},
   "source": [
    "## Qwen-VL, 2023 (Alibaba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4704b5",
   "metadata": {},
   "source": [
    "- Адаптация Qwen-7B к изображениям с помощью Openclip ViT\n",
    "- Модель тоже умеет работать с границами объектов\n",
    "- Изображения переводятся в 256 векторов через cross-attention с латентными векторами-ключами (как в Perceiver Resampler Flamingo)\n",
    "- В cross-attention используется дополнительное 2D абсолютное позиционное кодирование\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a68f6c",
   "metadata": {},
   "source": [
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23102f44",
   "metadata": {},
   "source": [
    "Формат входа:\n",
    "- изображение отделяется спецтокенами \\<img\\> и \\</img\\>\n",
    "- координаты точек границ объектов (слева сверху и справа снизу) нормируются в \\[0, 1000) и подаются в виде строки «(A, B), (C, D)»\n",
    "- Для её выделения используются токены \\<box\\> и \\</box\\>\n",
    "- Текст, описывающий объект в границах, отделяется \\<ref\\> и \\</ref\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a318368c",
   "metadata": {},
   "source": [
    "**1-й шаг предобучения**:\n",
    "- на 1.4B парах «изображение»-«текст»\n",
    "- разрешение изображений 224 × 224\n",
    "- LLM заморожена, учатся ViT и адаптер\n",
    "\n",
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1a6f5",
   "metadata": {},
   "source": [
    "**2-й шаг предобучения**:\n",
    "- на >75M примерах из 20 наборов данных для 7 задач\n",
    "- разрешение изображений 448 × 448\n",
    "- Учится вся модель\n",
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd47d1c",
   "metadata": {},
   "source": [
    "**SFT**:\n",
    "- 350K инструкций, собранных разными способами\n",
    "- разрешение изображений 448 × 448\n",
    "- ViT заморожен, учатся LLM и адаптер\n",
    "![KOSMOS](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed9849",
   "metadata": {},
   "source": [
    "![QWEN](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e33117",
   "metadata": {},
   "source": [
    "![QWEN](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_1.png)\n",
    "\n",
    "![QWEN](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/QWEN_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747634ff",
   "metadata": {},
   "source": [
    "![QWEN](./data_m/QWEN_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c777646",
   "metadata": {},
   "source": [
    "# Этап 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcaee98",
   "metadata": {},
   "source": [
    "## FROMAGe, 2023 (Carnegie Mellon University)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087fb78",
   "metadata": {},
   "source": [
    "Замороженные CLIP-ViT и OPT + обучаемые проекции и доп. токен\n",
    "\n",
    "\n",
    "**Задача 1 (генерация описаний)**:\n",
    "- выходной вектор CLIP проецируется в векторы «токенов» (reshape большого выхода линейного слоя проекции на k элементов)\n",
    "- последовательность — набор нескольких случайных пар: модель учится для токена текста смотреть именно на нужное для него изображение\n",
    "![FROMAge](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/FROMAGe_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47aca5",
   "metadata": {},
   "source": [
    "**Задача 2 (сопоставление)**:\n",
    "- Новый спецтокен [RET] в конце текста добавляет шаг self-attention, учитывающий все токены в авторегрессионной модели\n",
    "- Выходной вектор [RET] улучшает качество сопоставления\n",
    "- Учатся по слою проекции для вектора [RET] и выходного вектора CLIP + эмбеддинг токена [RET]\n",
    "- Негативные примеры в loss — все неверные пары батча\n",
    "\n",
    "![FROMAge](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/FROMAGe_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aeb162",
   "metadata": {},
   "source": [
    "Мультимодальные (картиночные) ответы модели обеспечиваются поиском по базе, больше контекста — лучше результат\n",
    "\n",
    "![FROMAge](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/FROMAGe_3.png)\n",
    "![FROMAge](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/FROMAGe_4.png)\n",
    "![FROMAge](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/FROMAGe_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951970f",
   "metadata": {},
   "source": [
    "## SpeechGPT, 2023 (Fudan University)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649dd4a",
   "metadata": {},
   "source": [
    "Кодировщик HuBERT (c K-Means) + LLaMA + вокодер HiFi-GAN\n",
    "\n",
    "![SpeechGPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/SpeechGPT.png)\n",
    "\n",
    "![SpeechGPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/SpeechGPT_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b303b3",
   "metadata": {},
   "source": [
    "Собран собственный набор данных SpeechInstruct из двух частей\n",
    "\n",
    "**Cross-modal Instruction**:\n",
    "- из открытых датасетов отобраны и дедуплицированы пары «текст»-«аудио» (9М)\n",
    "- с помощью GPT-4 сгенерированы 100+ инструкций-запросов на транскрибацию или синтез речи\n",
    "- инструкции и данные случайно соединяются в тройки\n",
    "- тройки конкатенируются в «диалоги» по максимальной длине входа LLM\n",
    "\n",
    "\n",
    "**Chain-of-Modality Instruction**:\n",
    "- на Cross-modal обучена вспомогательная модель-кодировщик\n",
    "- с её помощью для 38К инструкционных примеров из moss-002-sft-data\n",
    "сгенерированы векторы аудио\n",
    "- сформированы 38К четвёрок «(текст входа, аудио входа, текст ответа, аудио ответа)»\n",
    "- на их основе формируются Chain-of-Modality инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a4742",
   "metadata": {},
   "source": [
    "Предобучение-адаптация к модальности:\n",
    "- в LLM добавляются новые токены, соответствующие дискретным аудио-токенам\n",
    "- предобучение LLM идёт на предсказание следующего аудио-токена на неразмеченных записях\n",
    "- Кодировщик аудио (скорее всего) заморожен\n",
    "\n",
    "\n",
    "Кросс-модальный SFT: вся модель учится на смеси текстовых и Cross-modal инструкций\n",
    "Chain-of-Modality SFT: замороженная модель с адаптером LoRA учится на Chain-of-Modality инструкциях\n",
    "Обучение идёт на предсказание следующего токена текстового выхода (видимо, после транскрибации результата)\n",
    "\n",
    "![SpeechGPT](./data_m/SpeechGPT_3.png)\n",
    "\n",
    "[DEMO](https://0nutation.github.io/SpeechGPT.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909ab4d",
   "metadata": {},
   "source": [
    "## NExT-GPT, 2023 (National University of Singapore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6ddae",
   "metadata": {},
   "source": [
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba528df2",
   "metadata": {},
   "source": [
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa7753",
   "metadata": {},
   "source": [
    "- Кодировщик ImageBind + LLM Vicuna + диффузионные генераторы (все предобученные и замороженные)\n",
    "- LLM получает на вход векторы всех входных сущностей + текст\n",
    "- При генерации LLM решает, нужно ли сгенерировать токен текста или объект какой-то иной модальности\n",
    "- Во втором случае генерируется один из спецтокенов этой модальности\n",
    "- После окончания работы LLM этот токен вместе с текстом передаётся в соответствующую диффузионнную модель\n",
    "- Один ответ модели может использовать несколько разных диффузий"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c84f77",
   "metadata": {},
   "source": [
    "- Обучение входных линейных проектирующих слоёв идёт на задачу генерации текста описания объекта\n",
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbf988",
   "metadata": {},
   "source": [
    "- Обучать полноценное сопоставление всех диффузионных декодировщиков и LLM долго и дорого\n",
    "- Диффузии обусловлены на текст описания\n",
    "- Выходные проектирующие слои учатся приближать представления текста и спецтокенов модальности к представлениям текста в диффузиях\n",
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec281d",
   "metadata": {},
   "source": [
    "- В инструктивном дообучении участвует LLM (с адаптером LoRA)\n",
    "- Модель дообучается на размеченных ММ-диалогах, в процессе дополнительно настраиваются выходные проекции\n",
    "- Обучение на открытых наборах данных + собственный датасет MosIT (Modality-switching Instruction Tuning)\n",
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcabaa",
   "metadata": {},
   "source": [
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_2.png)\n",
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_4.png)\n",
    "![GPT](https://raw.githubusercontent.com/sswt/dive2gai/main/unit10/data_m/GPT_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52086a",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "- ViT и CLIP оказались простыми и удачными моделями, используемыми\n",
    "повсеместно до сих пор\n",
    "- Общий тренд на добавление мультимодальности путём комбинирования\n",
    "предобученных моделей\n",
    "- Дообучение на задачи ММ обычно требует настройки только части\n",
    "параметров и/или адаптеров типа линейных проекций и Q-former\n",
    "- Лучший ММ-векторизатор на текущий момент — ImageBind, поверх него\n",
    "делаются свежие MLLM\n",
    "- LLaVA демонстрирует пример генерации качественного инструктивного\n",
    "ММ набора данных\n",
    "- Kosmos-2 — сильный пример перехода от понимания моделью\n",
    "изображения в целом до взаимодействия с объектами на нём\n",
    "- С помощью FROMAGe можно относительно просто и качественно\n",
    "привнести в модель понимание и поиск изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549fcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
