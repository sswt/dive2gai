{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHIGUmeO9vH2"
      },
      "source": [
        "# LeakGAN\n",
        "\n",
        "Цели:\n",
        "\n",
        "- Разобраться с основными архитектурными решениями и алгоритмом обучения LeakGAN\n",
        "- Реализация LeakGAN для задачи посимвольной генерации текстов\n",
        "\n",
        "Содержание:\n",
        "\n",
        "* [Введение](#intro)\n",
        "* [Загрузка данных](#load_data)\n",
        "* [Алгоритм](#algorithm)\n",
        "  * История возникновения иерархического обучения с подкреплением\n",
        "  * Нейросети для иерархического RL\n",
        "  * Методология LeakGAN\n",
        "* [Реализация](#implementation)\n",
        "  * [Загрузчики данных](#data_loaders)\n",
        "  * [Генератор](#generator)\n",
        "  * [Дискриминатор](#discriminator)\n",
        "  * [Предобучение генератора (MLE)](#gen_pretrain)\n",
        "  * [Предобучение дискриминатора](#disc_pretrain)\n",
        "  * [Обучение в состязательном режиме](#adversarial_train)\n",
        "\n",
        "Ссылки:\n",
        "\n",
        "* [Long Text Generation via Adversarial Training with Leaked Information](https://browse.arxiv.org/pdf/1709.08624v2.pdf)\n",
        "* [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1703.01161.pdf)\n",
        "* [Feudal Reinforcement Learning](https://proceedings.neurips.cc/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf)\n",
        "* [Adversarial Feature Matching for Text Generation (TextGAN)](https://arxiv.org/pdf/1706.03850.pdf)\n",
        "* [Реализация, адаптированная в этом ноутбуке](https://github.com/nurpeiis/LeakGAN-PyTorch/blob/master)\n",
        "* [Реализация в состaве framework с десятком разных GAN](https://github.com/williamSYSU/TextGAN-PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Введение"
      ],
      "metadata": {
        "id": "nzu6rdt0m8H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previously on\n",
        "\n",
        "Генерация текста моделируется как последовательный процесс принятия решений,\n",
        "где на каждом шаге принимаются решения относительно следующего слова. В этом процессе состояние представляет уже сгенерированные слова, действие - выбор следующего слова, а генератор - случайная стратегия (stochastic policy), которая отображает состояние в распределение вероятностей по следующим действиям (следующим словам).\n",
        "\n",
        "После завершения генерации текста, полученные тексты проходят через дискриминатор, это классификатор, который обучен отличать настоящий текст от сгенерированного. Это позволяет получить сигналы обратной связи для обновления параметров генератора.\n",
        "\n"
      ],
      "metadata": {
        "id": "thXX14V7m-U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предпосылки для LeakGAN\n",
        "\n",
        "Существующие методы генерации текстов (до конца 2017) имеют недостаток в том, что управляющий сигнал от дискриминатора приходит достаточно редко и доступен только после генерации всей последовательности текста. К тому же, скалярный управляющий сигнал для всего текста может быть недостаточно информативным. Нам (генератору) в общем-то для успешного обучения необходимо понять внутреннюю синтаксическую структуру и семантику сгенерированного текста, но дают одно число в час по чайной ложке (по окончанию генерации предложения).\n",
        "\n",
        "Для того, чтобы сделать управляющий сигнал более информативным, дискриминатор может предоставлять больше информации, чем просто финальное вознаграждение, так как сам дискриминатор также является моделью и не является \"чёрным ящиком\". Здесь отсылочка к тому, что в большинстве традиционных RL-сред мы не понимаем внутренней механизмов или динамики среды и как формируется вознаграждение - оно нам просто выдаётся по окончанию эпизода. Например в робототехнике, играх (Dota 2, Starcraft) или автономном вождении.\n",
        "\n",
        "Именно на идее более информативного сигнала основан метод [TextGAN](https://arxiv.org/pdf/1706.03850.pdf). В TextGAN при обучении генератора скрытые представления настоящих и сгенерированных текстов вынуждаются быть похожими. Для генерации коротких текстов такой подход может быть эффективным, но управляющие сигналы по прежнему появляются только после генерации текста целиком.\n",
        "\n",
        "С другой стороны для решения проблемы разреженности управляющего сигнала (редкого вознаграждения) возникает идея иерархии, когда мы декомпозируем сложную задачу генерации на более мелкие подзадачи и модели становится легче их выучить (своего рода разделяй-и-властвуй).\n"
      ],
      "metadata": {
        "id": "xuoZZ0vyMNlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обобщённая архитектура LeakGAN\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w4/leakgan_arch.png\" alt=\"SeqGAN\" width=\"60%\" />\n",
        "\n",
        "LeakGAN должен устранить как слабую информативность вознаграждения, так и разреженность сигнала. Из идей иерархического RL вводится иерархический генератор, который состоит из высокоуровневого модуля MANAGER и низкоуровневого модуля WORKER.\n",
        "\n",
        "MANAGER - LSTM, работает как посредник. На каждом шаге, он принимает признаковое представление из дискриминатора и использует для формирования цель-ориентир (guiding goal) для WORKER на текущем шаге. Это внутренняя информация дискриминатора и неявно содержит часть информации о следующем токене, поэтому это назвается ликом. На kaggle обычно борются с ликами, а здесь их используют во благо.\n",
        "\n",
        "Затем, на основе эмбеддинга цели от менеджера, работник кодирует уже сгенерированные слова своей внутренней LSTM и совмещает выход с эмбеддингом цели, чтобы принять решение о действии в текущем состоянии.\n",
        "\n",
        "Таким образом, управляющие сигналы от дискриминатора доступны генератору не только в конце, как скалярное вознаграждение, но и в качестве вектора эмбеддинга цели непосредственно в процессе генерации, чтобы генератору было легче понять, как улучшаться."
      ],
      "metadata": {
        "id": "okUZZFFjMOl2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KUKbAAq10hR"
      },
      "source": [
        "<a name=\"load_data\"></a>\n",
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USX7LvX0Gpzm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAnfc4vvFAa8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!wget -q https://huggingface.co/datasets/sswt/arxiv_sample_50K/resolve/main/title_summary_ascii.txt.tar.gz -O data.tar.gz && tar -xzf data.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAxjSxf-FRan",
        "tags": []
      },
      "outputs": [],
      "source": [
        "max_len = 64\n",
        "with open('title_summary_ascii.txt') as fp:\n",
        "    lines = [line[:max_len] for line in fp.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8GEnsxMFtMS",
        "outputId": "6cfc1c71-985d-4fb9-eddd-c9c8d85abc6f",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' On Finitely Generated Models of Theories with at Most Countably',\n",
              " ' Generalized modeling of ecological population dynamics ; Over t',\n",
              " ' Generating Subsurface Earth Models using Discrete Representatio']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CISnZgL-GHxD",
        "outputId": "f014c342-d4b9-4f85-a774-fc307153bc0a",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {c: i for i, c in enumerate(sorted(set([c for l in lines for c in l])))}\n",
        "inv_vocab = {i: c for c, i in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V0-6kK_lGOzd",
        "outputId": "abb62476-8005-4178-8359-0dca2c6ed491",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n ',.0123456789;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''.join(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZiHHkcNmtGq",
        "outputId": "9f434769-5cc7-4773-d8c0-205def1b92eb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Foqi-h9Oeig",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def to_tensor(line):\n",
        "    t = torch.LongTensor([vocab[line[li]] for li in range(len(line[:max_len]))])\n",
        "    if len(line) < max_len:\n",
        "        t = nn.ConstantPad1d((0, max_len - len(line)), 0)(t)\n",
        "    return t\n",
        "\n",
        "def to_text(t):\n",
        "    return ''.join(inv_vocab[i] for i in t.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQgH1ei8Ok-U",
        "tags": []
      },
      "outputs": [],
      "source": [
        "sequences = torch.stack([to_tensor(lines[i]) for i in range(6400)])\n",
        "np.save('./data/train_corpus.npy', sequences.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz48kdTc15E7"
      },
      "source": [
        "<a name=\"algorithm\"></a>\n",
        "## Алгоритм\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### История возникновения иерархического обучения с подкреплением\n",
        "\n",
        "Первые успехи нейросетей на поприще RL были достаточно давно. В 1992 году Джон Тезауро разработал программу TD-Gammon, которая играла в нард. Так как в нардах ход игры зависит от броска кубиков и в стратегию игры всегда вносится элемент случайности, что во многом определяет стратегию, то алгоритму можно было тренировать модель просто играя самому с собой. Обобщить это на другие игры тогда не удалось и в качестве одной из идей в работе Feudal Reinforcement Learning (в соавторстве с Хинтоном) было озвучено иерархическое RL.\n",
        "\n",
        "Выдержка из текста статьи:\n",
        "\n",
        "> Мы стремились создать систему, которая отражает иерархические аспекты феодальной вассальной феодальной системы, как один из крайних случаев моделей управления. Менеджерам предоставляется абсолютная власть над своими подчиненными менеджерами - они могут назначать им задания и награждать или наказывать их так, как им угодно. Однако менеджерам в конечном итоге приходится удовлетворять своих собственных вышестоящих менеджеров, иначе они сами подвергаются наказанию - и таким образом, происходит рекурсивное укрепление и отбор, пока вся система не достигнет цели высшего уровня менеджера. Это можно добиться без того, чтобы подчиненные менеджеры сначала \"понимали\" подзадачи, которые им назначают. Каждый компонент просто действует, чтобы максимизировать ожидаемое вознаграждение, поэтому после обучения значение, которое он прикрепляет к описанию подзадачи, заключается в том, как это описание влияет на его выбор под-под-менеджеров и под-под-задач.\n",
        "\n",
        "Два принципа играют ключевую роль:\n",
        "\n",
        "* Скрытие вознаграждения\n",
        "  \n",
        "  Менеджеры должны награждать подчиненных менеджеров за выполнение своих заданий, независимо от того, удовлетворяют ли они команды вышестоящих менеджеров. Подчиненные менеджеры должны просто учиться подчиняться своим менеджерам и позволить им решить, что лучше всего делать на следующем уровне. Таким образом, если подчиненный менеджер не достигает подцели, установленной его менеджером, его не награждают, даже если его действия приводят к удовлетворению цели менеджера. В начальной стадии обучения менеджеры низкого уровня могут стать довольно компетентными в достижении целей низкого уровня, даже если цель высшего уровня никогда не была удовлетворена.\n",
        "\n",
        "* Скрытие информации\n",
        "\n",
        "  Менеджерам нужно знать состояние системы с точностью до их собственного выбора задач. Предоставление некоторых решений на более грубом уровне - одна из основных целей иерархической декомпозиции. Информация скрывается как вниз по иерархии - подчиненные менеджеры не знают о задаче, которую высший менеджер назначил менеджеру, так и вверх по иерархии - высший менеджер не знает, какие решения принял его менеджер для выполнения его команд. Однако менеджерам нужно знать условия удовлетворения для задач, которые они устанавливают, а также некоторую меру фактической стоимости для системы при их выполнении с использованием подчиненных менеджеров и задач, которые они выбрали в конкретный момент.\n",
        "  Для особого случая, рассматриваемого здесь, когда менеджерам не предоставляется выбора, какого подчиненного менеджера использовать в данном состоянии, их выбор задачи очень похож на действие в стандартной системе Q-обучения. Если задача успешно выполнена, стоимость определяется высшим менеджером в зависимости от того, насколько хорошо (например, насколько быстро или вообще) менеджер удовлетворяет его супер-задачи. В зависимости от того, как выполняется их собственная задача, менеджер награждает или наказывает подчиненного менеджера. Когда менеджер выбирает действие, управление передается подчиненному менеджеру и возвращается только тогда, когда состояние меняется на уровне управления.\n",
        "\n",
        "Феодальная архитектура частично решает одну из основных проблем в обучении с подкреплением - вопрос о том, как разбить одну задачу на подзадачи на нескольких уровнях.\n",
        "\n",
        "Это позволило улучшить общие метрики - менеджеры не задумывались о том, как подчинённые будут выполнять цели, а сосредотачивались на своём уровне гранулярности задач. Они могут понимать, что успех может быть в какой-то области и направлять туда агентов. То есть пространство исследуется не равномерно, а сокрытие информации уменьшает размер пространства состояний для поиска.\n",
        "\n"
      ],
      "metadata": {
        "id": "pomnzkfuNOSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Нейросети для иерархического RL\n",
        "\n",
        "Идея выше была переиспользована в недрах DeepMind. В 2017 году вышла статья \"FeUdal Networks for Hierarchical Reinforcement Learning\", где предложили нейросеть с одноимённой архитектурой FeUdal Networks (FuNs).\n",
        "\n",
        "В то время был бум обучения с подкреплением на наборе игр ATARI, так как он предоставлял разнообразные среды и задачи для тестирования и сравнения различных алгоритмов.\n",
        "\n",
        "В простых играх типа пинг-понга удалось достичь успеха, но в более сложных играх (Месть Монтесумы) не удалось. Проблемы были когда вознаграждение поступало редко (sparse reward signal) и нужно было долгосрочно планировать. Или если нужна была память, чтобы агент обучался, какую часть опыта ему следует запомнить, только на основе редкого вознаграждения. В иерархическом (feudal) RL уровни иерархии внутри агента взаимодействуют через явные цели, которые спускаются сверху вниз и установка целей не зависит от их достижения. Уровень иерархии взаимодействует с нижележащим уровнем, указывая, что должно быть достигнуто, но не каким образом. Объявление целей высокого уровня в более низкоуровневом представлении естественным образом структурирует поведение агентов в протяженные во времени под-политики.\n",
        "\n",
        "Ух забористо вышло выше... надо подкинуть ещё.\n",
        "\n",
        "В данной статье высокоуровневый менеджер устанавливает цели в менее гранулярном латентном пространстве состояние-действие, которое обучается им самим. Низкоуровневый рабочий, оперирует в более детализированном пространстве и производит простые действия, в соответствии с целями, установленными менеджером. Мотивация рабочего следовать этим целям достигается через внутреннюю награду. Важно отметить, что градиенты не протекают между рабочим и менеджером. Менеджер получает свой обучающий сигнал исключительно из окружающей среды. Другими словами менеджер обучается выбирать скрытые цели, которые максимизируют внешнее вознаграждение.\n",
        "\n",
        "Основные вклады исследования:\n",
        "\n",
        "* сквозное дифференцирование, обобщающее принципы иерархического RL\n",
        "* цели являются не абсолютными, а лишь задают направление\n",
        "* LSTM для менеджера позволяет удерживать более длинный контекст\n",
        "\n",
        "В LeakGAN была заимствована концепция из FuNs, однако, в отличие от FuNs, где функция вознаграждения является неявной (чёрным ящиком), LeakGAN использует модель дискриминатора и высокоуровневые признаки из внутреннего представления дискриминатора, передаются менеджеру. Таким образом менеджер является \"шпионом\", который \"сливает\" информацию от дискриминатора, чтобы помогать генератору.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w4/leakgan_algorithm.png\" alt=\"SeqGAN\" width=\"75%\" />\n",
        "\n"
      ],
      "metadata": {
        "id": "BmShokY1NSnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeakGAN-методология более детально\n",
        "\n",
        "**TODO:** Избавиться от повторений одного и того же, некоторые мысли сейчас дублируются.\n",
        "\n",
        "Есть последовательность $s_t = (x_1, ..., x_i, ..., x_t)$, где $x_i$ - токен из словаря $V$. Генератор $G_\\theta$ соответстсвует стохастической стратегии (stochastic policy), отображает $s_t$ в распределение вероятностей по словарю, откуда сэмплируется следующий токен $s_{t+1}$.\n",
        "\n",
        "Дискриминатор $D_\\phi$ даёт скалярный управляющий сигнал $D_\\phi(s_T)$ генератору для подстройки его параметров, когда вся последовательность $s_T$ уже сгенерирована. При длинных последовательностях скалярного сигнала недостаточно. Поэтому дискриминатор выдаёт генератору дополнительно вектор признаков $f_t$ в добавок к его текущему внутреннему состоянию $s_t$.\n",
        "\n",
        "#### Фичи дискриминатора как управляющие сигналы\n",
        "\n",
        "Обычно в RL функция вознаграждения - чёрный ящик. В LeakGAN дискриминатор используется как обучаемая функция вознаграждения. Вообще там нейросетка, которая тоже в целом чёрный ящик, но мы можем её разложить на feature extractor $\\mathcal{F}(·; \\phi_f)$ and a последний слой классификатора с сигмоидой и весами $\\phi_l$.\n",
        "\n",
        "$$D_\\phi(s)=\\sigma(\\phi_l^T\\mathcal{F}(s;\\phi_f))=\\sigma(\\phi_l^Tf), \\tag{1}$$ где $\\phi=(\\phi_f, \\phi_l), \\sigma(x)=1/(1+e^{-z})$, $f$ - вектор признаков, который передаётся в генератор.\n",
        "\n",
        "Есть известное наблюдение, что нейросети для задачи классификации стремятся создать легко разделимое пространство в на выходе предпоследнего слоя. Как видно из формулы (1) вознаграждение на выходе зависит от $f$, поэтому цель получить высокое вознаграждение эквивалентно цели найти область с высоким вознаграждением в пространстве $\\mathcal{F}(S; \\phi_f) = \\{\\mathcal{F}(s; \\phi_f)\\}_{s \\in S}$.\n",
        "\n",
        "Стоит отметить, что сравнению со скалярным сигналом, вектор $f$ гораздо более информативен.\n",
        "\n",
        "#### Иерархическая структура генератора\n",
        "\n",
        "На каждом шаге $t$ во время генерации для эффективного использования $f_t$ генератором используются принципы иерархического RL. Он состоит из модуля MANAGER, который представляет собой LSTM, принимающую на вход $f_t$ на шаге $t$ и выдающую вектор целей $g_t$, который передаётся на вход модуля WORKER для управления генерацией следующего слова так, чтобы попасть в область высокого вознаграждения в пространстве $\\mathcal{F}(·; \\phi_f)$.\n",
        "\n",
        "#### Процесс генерации\n",
        "\n",
        "$M$ и $W$ начинают с нулевых скрытых состояний $h_0^W$ и $h_0^M$. На каждом шаге M из вектора $f_t$ от дискриминатора и своего скрытого состояния производит вектор цели $g_t$\n",
        "$$\\hat g_t, h^M_t = M(f_t, h^M_{t−1}; \\theta_m), \\tag{2}$$\n",
        "$$g_t = \\hat g_t/‖\\hat g_t‖,$$\n",
        "где $\\theta_m$ - параметры LSTM-сети модуля $M$.\n",
        "\n",
        "Для интеграции целей полученных $M$, линейная трансформация $\\psi$ с весами $W_\\psi$ выполняется над суммой последних $c$ целей, для того, чтобы получить $k$-мерный вектор эмбеддингов целей\n",
        "$$w_t = \\psi\\Big(\\sum_{i=1}^c g_{t-i}\\Big)=W_\\psi\\Big(\\sum_{i=1}^c g_{t-i}\\Big)$$\n",
        "Модуль $W$ получая на вход текущее слово $x_t$ и выход $O_t$, который комбинируется с вектором эмбеддингов $w_t$ с помощью матричного произведения для определения финального распределения вероятностей следующих слов через softmax:\n",
        "$$O_t, h^W_t = W(x_t, h^W_{t−1}; \\theta_w),$$\n",
        "$$G_\\theta(·|s_t) = softmax(O_t · w_t/α),$$\n",
        "где $\\theta_w$ - параметры LSTM-сети модуля $W$,  $O_t$ матрица $|V |×k$, $O_t · w_t$ - логиты для всех слов, $\\alpha$ - температура для контроля энтропии генерации.\n",
        "\n",
        "#### Обучение генератора\n",
        "\n",
        "Процедура выше полностью дифференцируема. Мы можем обучать генератор от начала до конца с использованием gradient policy алгоритма, например REINFORCE. При этом нам бы хотелось, чтобы $M$ выучил какие-то значимые закономерности, поэтому, следуя алгоритму из FeUdal Networks $M$ и $W$ обучаются раздельно. $M$ обучается предсказать выгодные напаравления в дискриминативном пространстве, а $W$ получает внутреннее вознаграждение за следование этим направлениям.\n",
        "\n",
        "Градиент $M$ определяется как\n",
        "\n",
        "$$∇^{adv}_{θ_m} g_t = −Q_{\\mathcal{F}} (s_t, g_t)∇_{θ_m} d_{cos}(f_{t+c} − f_t, g_t(θ_m)), \\tag{7}$$\n",
        "\n",
        "где $Q_{\\mathcal{F}} (s_t, g_t) = Q(F(st), gt) = Q(ft, gt) = E[rt]$ - ожидаемая награда при текущей стратегии, которая может быть оценена по методу Монте-Карло; $d_{cos}$ - косинусная мера схожести между изменением $f$ за $c$-шагов ($f_{t+c} − f_t$) и вектор цели $g_t(θ_m)$ производится менеджером по формуле (2).\n",
        "\n",
        "Интуитивно, функция потерь используется для принуждения соответствия вектора цели переходу в пространстве признаков, при этом достигая высокого вознаграждения.\n",
        "\n",
        "В то же время, рабочий обучается максимизировать вознаграждение с использованием алгоритма REINFORCE, как это сделано в SeqGAN:\n",
        "\n",
        "$$∇_{θ_w}\\mathbb{E}_{s_{t−1}∼G}\\Big[\\sum_{x_t}r^I_tW(x_t|s_{t−1}; θ_w)\\Big]\n",
        "=\\mathbb{E}_{s_{t−1}∼G,x_t∼W(xt_|s_{t−1})}\\Big[r^I_t ∇_{θ_w} log W(x_t|s_{t−1}; θ_w)\\Big], \\tag{8}$$\n",
        "\n",
        "что можно аппроксимировать сэмплируя состояние $s_{t-1}$ и действие $x_t$, предпринимаемое рабочим. Так как рабочий поощряется за следование направлениям, задаваемым менеджером, внутреннее вознаграждение рабочего определяется как:\n",
        "\n",
        "$$r^I_t = \\frac{1}{c} \\sum_{i=1}^{c} d_{cos}(f_t - f_{t-i}, g_{t-i}). \\tag{9}$$\n",
        "\n",
        "На практике, до состязательного обучения нам нужно предобучить $G_θ$. Будем последовательными - при предобучении используется немного другая схема, где градиент менеджера выражается как\n",
        "\n",
        "$$∇^{pre}_{θ_m}g_t = −∇_{θ_m}d_{cos}(\\hat{f}_{t+c} − \\hat{f}_t, g_t(θ_m)), \\tag{10}$$\n",
        "\n",
        "где $\\hat{f}_t = \\mathcal{F}(\\hat{s}_t)$, $\\hat{s}_t$ и $\\hat{s}_{t+c}$ -  состояния реального текста, а значение функции состояние-действие $Q_{\\mathcal{F}}(s_t, g_t)$ из выражения (7) устанавливается равным 1, так как данные примеры, используемые в предварительном обучении, представляют собой реальные предложения.\n",
        "\n",
        "Таким образом, менеджер обучается имитировать \"переходы\" реальных текстовых образцов в пространстве признаков, в то время как работник обучается с использованием метода максимального правдоподобия (MLE).\n",
        "\n",
        "В процессе обучения генератор $G_θ$ и дискриминатор $D_φ$ обучаются поочередно. В генераторе менеджер $M(· ; θ_m)$ и работник $W(· ; θ_w)$ (включая $ψ$ и softmax) обучаются поочередно, при этом фиксируется другой компонент.\n",
        "\n"
      ],
      "metadata": {
        "id": "OivOKskQNX-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSvozJf6C79T"
      },
      "source": [
        "График из статьи - NLL на синтетических данных, сгенерированных случайно инициализированным LSTM-оракулом.\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w4/leakgan_nll.png\" alt=\"SeqGAN\" width=\"60%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqwIKsIdgfJe"
      },
      "source": [
        "<a name=\"implementation\"></a>\n",
        "## Реализация"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"data_loaders\"></a>\n",
        "### Data loaders"
      ],
      "metadata": {
        "id": "pyT9uILbMATT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_23uW5egg3_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Real_Dataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.data = np.load(filepath)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.data[idx]).long()\n",
        "\n",
        "class Dis_Dataset(Dataset):\n",
        "    def __init__(self, positive_filepath, negative_filepath):\n",
        "        pos_data = np.load(positive_filepath, allow_pickle=True)\n",
        "        neg_data = np.load(negative_filepath, allow_pickle=True)\n",
        "        #print(\"Pos data: {}\".format(len(pos_data)))\n",
        "        #print(\"Neg data: {}\".format(len(neg_data)))\n",
        "        pos_label = np.array([1 for _ in pos_data])\n",
        "        neg_label = np.array([0 for _ in neg_data])\n",
        "        self.data = np.concatenate([pos_data, neg_data])\n",
        "        self.label = np.concatenate([pos_label, neg_label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.from_numpy(self.data[idx]).long()\n",
        "        label = torch.nn.init.constant_(torch.zeros(1), int(self.label[idx])).long()\n",
        "        return {\"data\": data, \"label\": label}\n",
        "\n",
        "\n",
        "def real_data_loader(filepath, batch_size, shuffle, num_workers, pin_memory):\n",
        "    dataset = Real_Dataset(filepath)\n",
        "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "def dis_data_loader(positive_filepath, negative_filepath, batch_size, shuffle, num_workers, pin_memory):\n",
        "    dataset = Dis_Dataset(positive_filepath, negative_filepath)\n",
        "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhILA2KsaEIJ"
      },
      "source": [
        "<a name=\"generator\"></a>\n",
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDolaTTPaExM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from scipy.stats import truncnorm\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# A truncated distribution has its domain (the x-values) restricted to a certain range of values.\n",
        "# For example, you might restrict your x-values to between 0 and 100, written in math terminology as {0 > x > 100}.\n",
        "# There are several types of truncated distributions:\n",
        "def truncated_normal(shape, lower=-0.2, upper=0.2):\n",
        "    size = 1\n",
        "    for dim in shape:\n",
        "        size *= dim\n",
        "    w_truncated = truncnorm.rvs(lower, upper, size=size)\n",
        "    w_truncated = torch.from_numpy(w_truncated).float()\n",
        "    w_truncated = w_truncated.view(shape)\n",
        "    return w_truncated\n",
        "\n",
        "class Manager(nn.Module):\n",
        "    def __init__(self, batch_size, hidden_dim, goal_out_size):\n",
        "        super(Manager, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.goal_out_size = goal_out_size\n",
        "        self.recurrent_unit = nn.LSTMCell(\n",
        "            self.goal_out_size, #input size\n",
        "            self.hidden_dim #hidden size\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            self.hidden_dim, #in_features\n",
        "            self.goal_out_size #out_features\n",
        "        )\n",
        "        self.goal_init = nn.Parameter(torch.zeros(self.batch_size, self.goal_out_size))\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.normal_(param, std=0.1)\n",
        "        self.goal_init.data = truncated_normal(\n",
        "            self.goal_init.data.shape\n",
        "        )\n",
        "    def forward(self, f_t, h_m_t, c_m_t):\n",
        "        \"\"\"\n",
        "        f_t = feature of CNN from discriminator leaked at time t, it is input into LSTM\n",
        "        h_m_t = ouput of previous LSTMCell\n",
        "        c_m_t = previous cell state\n",
        "        \"\"\"\n",
        "        #print(\"H_M size: {}\".format(h_m_t.size()))\n",
        "        #print(\"C_M size: {}\".format(c_m_t.size()))\n",
        "        #print(\"F_t size: {}\".format(f_t.size()))\n",
        "        h_m_tp1, c_m_tp1 = self.recurrent_unit(f_t, (h_m_t, c_m_t))\n",
        "        sub_goal = self.fc(h_m_tp1)\n",
        "        sub_goal = torch.renorm(sub_goal, 2, 0, 1.0) #Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm\n",
        "        return sub_goal, h_m_tp1, c_m_tp1\n",
        "\n",
        "class Worker(nn.Module):\n",
        "    def __init__(self, batch_size, vocab_size, embed_dim, hidden_dim,\n",
        "                    goal_out_size, goal_size):\n",
        "        super(Worker, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.goal_out_size = goal_out_size\n",
        "        self.goal_size = goal_size\n",
        "\n",
        "        self.emb = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.recurrent_unit = nn.LSTMCell(self.embed_dim, self.hidden_dim)\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.goal_size*self.vocab_size)\n",
        "        self.goal_change = nn.Parameter(torch.zeros(self.goal_out_size, self.goal_size))\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.normal_(param, std=0.1)\n",
        "    def forward(self, x_t, h_w_t, c_w_t):\n",
        "        \"\"\"\n",
        "            x_t = last word\n",
        "            h_w_t = last output of LSTM in Worker\n",
        "            c_w_t = last cell state of LSTM in Worker\n",
        "        \"\"\"\n",
        "        x_t_emb = self.emb(x_t)\n",
        "        h_w_tp1, c_w_tp1 = self.recurrent_unit(x_t_emb, (h_w_t, c_w_t))\n",
        "        output_tp1 = self.fc(h_w_tp1)\n",
        "        output_tp1 = output_tp1.view(self.batch_size, self.vocab_size, self.goal_size)\n",
        "        return output_tp1, h_w_tp1, c_w_tp1\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, worker_params, manager_params, step_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.step_size = step_size\n",
        "        self.worker = Worker(**worker_params)\n",
        "        self.manager = Manager(**manager_params)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h = Variable(torch.zeros(self.worker.batch_size, self.worker.hidden_dim))\n",
        "        c = Variable(torch.zeros(self.worker.batch_size, self.worker.hidden_dim))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal, t, temperature):\n",
        "        sub_goal, h_m_tp1, c_m_tp1 = self.manager(f_t, h_m_t, c_m_t)\n",
        "        output, h_w_tp1, c_w_tp1 = self.worker(x_t, h_w_t, c_w_t)\n",
        "        last_goal_temp = last_goal + sub_goal\n",
        "        w_t = torch.matmul(\n",
        "            real_goal, self.worker.goal_change\n",
        "        )\n",
        "        w_t = torch.renorm(w_t, 2, 0, 1.0)\n",
        "        w_t = torch.unsqueeze(w_t, -1)\n",
        "        logits = torch.squeeze(torch.matmul(output, w_t))\n",
        "        probs = F.softmax(temperature * logits, dim=1)\n",
        "        x_tp1 = Categorical(probs).sample()\n",
        "        return x_tp1, h_m_tp1, c_m_tp1, h_w_tp1, c_w_tp1,\\\n",
        "                last_goal_temp, real_goal, sub_goal, probs, t + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY1gWAK4aX7T"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfAOsxYSaFAY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Highway(nn.Module):\n",
        "    #Highway Networks = Gating Function To Highway = y = xA^T + b\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(Highway, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_size, out_size)\n",
        "        self.fc2 = nn.Linear(in_size, out_size)\n",
        "    def forward(self, x):\n",
        "        # highway = F.sigmoid(highway)*F.relu(highway) + (1. - transform)*pred # sets C = 1 - T\n",
        "        g = F.relu(self.fc1)\n",
        "        t = torch.sigmoid(self.fc2)\n",
        "        out = g*t + (1. - t)*x\n",
        "        return out\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN for text classification\n",
        "    num_filters (int): This is the output dim for each convolutional layer, which is the number\n",
        "          of \"filters\" learned by that layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, num_classes, vocab_size, dis_emb_dim,\n",
        "                    filter_sizes, num_filters, start_token, goal_out_size, step_size, dropout_prob, l2_reg_lambda):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_classes = num_classes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dis_emb_dim = dis_emb_dim\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.num_filters = num_filters\n",
        "        self.start_token = start_token\n",
        "        self.goal_out_size = goal_out_size\n",
        "        self.step_size = step_size\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.num_filters_total = sum(self.num_filters)\n",
        "\n",
        "        #Building up layers\n",
        "        self.emb = nn.Embedding(self.vocab_size + 1, self.dis_emb_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_f, (f_size, self.dis_emb_dim)) for f_size, num_f in zip(self.filter_sizes, self.num_filters)\n",
        "        ])\n",
        "        self.highway = nn.Linear(self.num_filters_total, self.num_filters_total)\n",
        "        #in_features = out_features = sum of num_festures\n",
        "        self.dropout = nn.Dropout(p = self.dropout_prob)\n",
        "        #Randomly zeroes some of the elements of the input tensor with probability p using Bernouli distribution\n",
        "        #Each channel will be zeroed independently onn every forward call\n",
        "        self.fc = nn.Linear(self.num_filters_total, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "            x: shape(batch_size * self.seq_len)\n",
        "               type(Variable containing torch.LongTensor)\n",
        "        Return:\n",
        "            pred: shape(batch_size * 2)\n",
        "                  For each sequence in the mini batch, output the probability\n",
        "                  of it belonging to positive sample and negative sample.\n",
        "            feature: shape(batch_size * self.num_filters_total)\n",
        "                     Corresponding to f_t in original paper\n",
        "            score: shape(batch_size, self.num_classes)\n",
        "\n",
        "        \"\"\"\n",
        "        #1. Embedding Layer\n",
        "        #2. Convolution + maxpool layer for each filter size\n",
        "        #3. Combine all the pooled features into a prediction\n",
        "        #4. Add highway\n",
        "        #5. Add dropout. This is when feature should be extracted\n",
        "        #6. Final unnormalized scores and predictions\n",
        "        emb = self.emb(x).unsqueeze(1)\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len]\n",
        "        pooled_out = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n",
        "        pred = torch.cat(pooled_out, 1) # batch_size * sum(num_filters)\n",
        "        #print(\"Pred size: {}\".format(pred.size()))\n",
        "        highway = self.highway(pred)\n",
        "        #print(\"highway size: {}\".format(highway.size()))\n",
        "        highway = torch.sigmoid(highway)* F.relu(highway) + (1.0 - torch.sigmoid(highway))*pred\n",
        "        features = self.dropout(highway)\n",
        "        score = self.fc(features)\n",
        "        pred = F.log_softmax(score, dim=1) #batch * num_classes\n",
        "        return {\"pred\":pred, \"feature\":features, \"score\": score}\n",
        "\n",
        "    def l2_loss(self):\n",
        "        W = self.fc.weight\n",
        "        b = self.fc.bias\n",
        "        l2_loss = torch.sum(W*W) + torch.sum(b*b)\n",
        "        l2_loss = self.l2_reg_lambda * l2_loss\n",
        "        return l2_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U8fgg-Tabcx"
      },
      "source": [
        "### Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btvAwJnoihmp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=False):\n",
        "    file_name = \"checkpoint\" + str(ckpt_num) + \".pth.tar\"\n",
        "    torch.save({\"model_dict\": model_dict, \"optimizer_dict\": optimizer_dict, \"scheduler_dict\": scheduler_dict, \"ckpt_num\": ckpt_num}, file_name)\n",
        "    if replace:\n",
        "        ckpts = glob.glob(\"checkpoint*\")\n",
        "        ckpt_nums = [int(x.split('.')[0][10:]) for x in ckpts]\n",
        "        oldest_ckpt = \"checkpoint\" + str(min(ckpt_nums)) + \".pth.tar\"\n",
        "        os.remove(oldest_ckpt)\n",
        "\n",
        "def restore_checkpoint(ckpt_path):\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    return checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV8HgS4DDJ0N",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_params = {\n",
        "    \"lr_dict\": {\"worker\": 0.0015, \"manager\": 0.0015, \"discriminator\": 5e-05},\n",
        "    \"decay_step_size\": 200,\n",
        "    \"decay_rate\": 0.99,\n",
        "    \"total_epoch\": 80,  # 800,\n",
        "    \"generated_num\": 156,\n",
        "    \"save_num\": 10,\n",
        "    \"replace_num\": 5,\n",
        "    \"pre_dis_epoch_num\": 20,  # 50,\n",
        "    \"pre_gen_epoch_num\": 30,  # 80,\n",
        "    \"pos_filepath\": \"./data/train_corpus.npy\",\n",
        "    \"neg_filepath\": \"./data/gen_data.npy\",\n",
        "    \"eval_filepath\": \"./data/eval_data.npy\",\n",
        "    \"model_path\": \"./ckpts/\",\n",
        "    \"seed\": 233,\n",
        "    \"checkpoint_path\": None\n",
        "}\n",
        "\n",
        "target_params = {\n",
        "    \"vocab_size\": 5000,\n",
        "    \"batch_size\": 64,\n",
        "    \"embed_dim\": 32,\n",
        "    \"hidden_dim\": 32,\n",
        "    \"seq_len\": max_len,  # 20\n",
        "    \"start_token\": 0\n",
        "}\n",
        "\n",
        "dis_data_params = {\n",
        "    \"positive_filepath\": \"./data/train_corpus.npy\",\n",
        "    \"negative_filepath\": \"./data/gen_corpus.npy\",\n",
        "    \"batch_size\": 64,\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 4,\n",
        "    \"pin_memory\": False\n",
        "}\n",
        "\n",
        "real_data_params = {\n",
        "    \"filepath\": \"./data/train_corpus.npy\",\n",
        "    \"batch_size\": 64,\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 4,\n",
        "    \"pin_memory\": False\n",
        "}\n",
        "\n",
        "discriminator_params = {\n",
        "  \"seq_len\": max_len,  # 20\n",
        "  \"num_classes\": 2,\n",
        "#   \"vocab_size\": 5258,\n",
        "  \"dis_emb_dim\": 64,\n",
        "  \"filter_sizes\": [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20 ],\n",
        "  \"num_filters\": [ 100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160 ],\n",
        "  \"start_token\": 0,\n",
        "  \"goal_out_size\": None,\n",
        "  \"step_size\": 5,\n",
        "  \"dropout_prob\": 0.8,\n",
        "  \"l2_reg_lambda\": 0.2\n",
        "}\n",
        "\n",
        "# generator_params\n",
        "manager_params = {\"batch_size\": 64, \"hidden_dim\": 32, \"goal_out_size\": None}\n",
        "worker_params = {\"batch_size\": 64, \"vocab_size\": vocab_size, \"embed_dim\": 32, \"hidden_dim\": 32, \"goal_out_size\": None, \"goal_size\": 16}\n",
        "step_size = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0GlfG3-XFKK",
        "outputId": "aecde018-0df0-4a8f-bf89-c35ac69ff8a1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f496c11bc50>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(train_params[\"seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc9Di2YpXin3",
        "outputId": "3fc88bae-27bc-41c8-fa04-6f508a2774c5",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(device(type='cuda'), True)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "use_cuda = device == torch.device('cuda')\n",
        "device, use_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u67MdxVSXaKs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "discriminator_params[\"goal_out_size\"] = sum(discriminator_params[\"num_filters\"])\n",
        "worker_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
        "manager_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
        "discriminator = Discriminator(**discriminator_params, vocab_size=vocab_size)\n",
        "generator = Generator(worker_params, manager_params, step_size)\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "model_dict = {\"generator\": generator, \"discriminator\": discriminator}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T_nghNYXOdq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "lr_dict = train_params[\"lr_dict\"]\n",
        "w_optimizer = torch.optim.Adam(generator.worker.parameters(), lr=lr_dict[\"worker\"])\n",
        "m_optimizer = torch.optim.Adam(generator.manager.parameters(), lr=lr_dict[\"manager\"])\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr_dict[\"discriminator\"])\n",
        "optimizer_dict = {\"worker\": w_optimizer, \"manager\": m_optimizer, \"discriminator\": d_optimizer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRoLNU7xXFld",
        "tags": []
      },
      "outputs": [],
      "source": [
        "gamma = train_params[\"decay_rate\"]\n",
        "step_size = train_params[\"decay_step_size\"]\n",
        "\n",
        "w_scheduler = torch.optim.lr_scheduler.StepLR(w_optimizer, step_size=step_size, gamma=gamma)\n",
        "m_scheduler = torch.optim.lr_scheduler.StepLR(m_optimizer, step_size=step_size, gamma=gamma)\n",
        "d_scheduler = torch.optim.lr_scheduler.StepLR(d_optimizer, step_size=step_size, gamma=gamma)\n",
        "scheduler_dict = {\"worker\": w_scheduler, \"manager\": m_scheduler, \"discriminator\": d_scheduler}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftZg6_p2cptl"
      },
      "source": [
        "<a name=\"gen_pretrain\"></a>\n",
        "#### Pretrain discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4jsEXiec6pI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def pretrain_discriminator(model_dict, optimizer_dict, scheduler_dict,\n",
        "                           dis_dataloader_params, vocab_size, positive_file,\n",
        "                           negative_file, batch_size, epochs, use_cuda=False, temperature=1.0):\n",
        "    discriminator = model_dict[\"discriminator\"]\n",
        "\n",
        "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
        "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
        "\n",
        "    generate_samples(model_dict, negative_file, batch_size, use_cuda, temperature)\n",
        "    dis_dataloader_params[\"positive_filepath\"] = positive_file\n",
        "    dis_dataloader_params[\"negative_filepath\"] = negative_file\n",
        "    #print(dis_dataloader_params)\n",
        "    dataloader = dis_data_loader(**dis_dataloader_params) #this is where data iterator is used\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss() #this one is similar to NLL (negative log likelihood)\n",
        "    if use_cuda:\n",
        "        cross_entropy = cross_entropy.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, sample in enumerate(dataloader):\n",
        "            d_optimizer.zero_grad()\n",
        "            data, label = sample[\"data\"], sample[\"label\"] #initialize sample variables\n",
        "            data = Variable(data)\n",
        "            label = Variable(label)\n",
        "            if use_cuda:\n",
        "                data = data.cuda()\n",
        "                label = label.cuda()\n",
        "            outs = discriminator(data)\n",
        "            loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
        "            d_lr_scheduler.step()\n",
        "            loss.backward()\n",
        "            d_optimizer.step()\n",
        "            if i == 63:\n",
        "                print(\"Pre-Discriminator loss: {:.5f}\".format(loss))\n",
        "\n",
        "    model_dict[\"discriminator\"] = discriminator\n",
        "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
        "    scheduler_dict[\"discriminator\"] = d_lr_scheduler\n",
        "    return model_dict, optimizer_dict, scheduler_dict\n",
        "\n",
        "def generate_samples(model_dict, negative_file, batch_size,\n",
        "                     use_cuda=False, temperature=1.0):\n",
        "    neg_data = []\n",
        "    for _ in range(batch_size):\n",
        "        sample = get_sample(model_dict, use_cuda, temperature)\n",
        "        sample = sample.cpu()\n",
        "        neg_data.append(sample.data.numpy())\n",
        "    neg_data = np.concatenate(neg_data, axis=0)\n",
        "    np.save(negative_file, neg_data)\n",
        "\n",
        "def get_sample(model_dict, use_cuda=False, temperature=1.0):\n",
        "    return recurrent_func(\"gen\")(model_dict, use_cuda, temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "U61bb3c8lyEm"
      },
      "outputs": [],
      "source": [
        "def recurrent_func(f_type = \"pre\"):\n",
        "    \"\"\"\n",
        "    There are 3 types of recurrent function:\n",
        "        1. pre = pretrain\n",
        "        2. adv = adversarial train\n",
        "        3. rollout = rollout for evaluate reward\n",
        "\n",
        "    Each kind of training has its own function\n",
        "    \"\"\"\n",
        "    if f_type == \"pre\":\n",
        "        def func(model_dict, real_data, use_cuda, temperature = 1.0):\n",
        "            \"\"\"\n",
        "                Get generator and discriminator\n",
        "            \"\"\"\n",
        "            #print(\"After sample size: {}\".format(real_data.size()))\n",
        "            generator = model_dict[\"generator\"]\n",
        "            discriminator = model_dict[\"discriminator\"]\n",
        "            '''\n",
        "            Initialize variables and lists for forward step.\n",
        "            '''\n",
        "            h_w_t, c_w_t, h_m_t, c_m_t, last_goal, real_goal, x_t = \\\n",
        "                init_vars(generator, discriminator, use_cuda)\n",
        "            t = 0\n",
        "            feature_list = []\n",
        "            delta_feature_list = [] #F(St+c) - F(St) = used to calculate the gradient of manager module\n",
        "            prediction_list = []\n",
        "            real_goal_list = []\n",
        "            batch_size = generator.worker.batch_size\n",
        "            seq_len = discriminator.seq_len\n",
        "            step_size = generator.step_size\n",
        "            goal_out_size = generator.worker.goal_out_size\n",
        "            vocab_size = discriminator.vocab_size\n",
        "            \"\"\"\n",
        "                Forward step for pretrainning G & D\n",
        "            \"\"\"\n",
        "            while t < seq_len + 1:\n",
        "                #Extract Feature from D\n",
        "                if t == 0:\n",
        "                    cur_sen = Variable(nn.init.constant_(\n",
        "                        torch.zeros(batch_size, seq_len), vocab_size\n",
        "                    )).long()\n",
        "                    #print(\"Batch Size: {}\".format(batch_size))\n",
        "                    #print(\"Real Data: {}\".format(cur_sen.size()))\n",
        "                else:\n",
        "                    cur_sen = real_data[:,:t]\n",
        "                    #print(\"Real Data: {}\".format(real_data.size()))\n",
        "                    #print(\"t: {}\".format(t))\n",
        "                    cur_sen = cur_sen.contiguous()\n",
        "                    cur_sen = F.pad(cur_sen.view(-1, t), (0, seq_len - t), value=vocab_size)\n",
        "                cur_sen.to(device, non_blocking=True)\n",
        "                #print(\"Current sentence:{}\".format(cur_sen))\n",
        "                #print(\"Current sentence size:{}\".format(cur_sen.size()))\n",
        "                f_t= discriminator(cur_sen)[\"feature\"]\n",
        "                #print(\"F_t from discr: {}\".format(f_t))\n",
        "                #print(\"F_t from discr: {}\".format(f_t.size()))\n",
        "                #G forward tep\n",
        "                x_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal,\\\n",
        "                sub_goal, probs, t_ = generator(\n",
        "                        x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal,\n",
        "                        real_goal, t, temperature\n",
        "                    )\n",
        "                if t % step_size == 0:\n",
        "                    if t>0:\n",
        "                        real_goal = last_goal\n",
        "                    last_goal = Variable(torch.zeros(batch_size, goal_out_size))\n",
        "                    last_goal.to(device, non_blocking=True)\n",
        "                    real_goal_list.append(real_goal)\n",
        "                \"\"\"\n",
        "                Store needed information for calculating loss function\n",
        "                \"\"\"\n",
        "                feature_list.append(f_t)\n",
        "                prediction_list.append(probs)\n",
        "                if t > 0:\n",
        "                    if t % step_size == 0:\n",
        "                        delta_feature_list.append(f_t-feature_list[t - step_size])\n",
        "                t = t_\n",
        "            \"\"\"\n",
        "            Post process and return variables needed for calculating loss\n",
        "            \"\"\"\n",
        "            if len(real_goal_list) == len(delta_feature_list) + 1:\n",
        "                real_goal_list = real_goal_list[:-1] #exclude the last element\n",
        "            prediction_list = prediction_list[:-1]\n",
        "            real_goal_var = torch.stack(real_goal_list).permute(1,0,2)#stack = turn a list of PyTorch Tensors into one tensor, permute = rotating in regards to z axis\n",
        "            #print(\"Prediction stack before stacking: {}\".format(torch.stack(prediction_list).size()))\n",
        "            prediction_var = torch.stack(prediction_list).permute(1,0,2)\n",
        "            delta_feature_var = torch.stack(delta_feature_list).permute(1,0,2)\n",
        "            \"\"\"\n",
        "            real_goal = g_t, prediction = generator sentence, delta_feature = F(s_(t+c))-F(s_t)\n",
        "            \"\"\"\n",
        "            results = {\"real_goal\": real_goal_var,\"prediction\": prediction_var, \"delta_feature\": delta_feature_var}\n",
        "            for result in results.values():\n",
        "                if result.is_contiguous():\n",
        "                    result = result.contiguous()\n",
        "            return results\n",
        "        return func\n",
        "\n",
        "    #Adversarial Training\n",
        "    elif f_type == \"adv\":\n",
        "        def func(model_dict, use_cuda=False, temperature = 1.0):\n",
        "            \"\"\"\n",
        "            Get G and D\n",
        "            \"\"\"\n",
        "            generator = model_dict[\"generator\"]\n",
        "            discriminator = model_dict[\"discriminator\"]\n",
        "            h_w_t, c_w_t, h_m_t, c_m_t, last_goal, real_goal, x_t = \\\n",
        "                init_vars(generator, discriminator, use_cuda)\n",
        "            t = 0\n",
        "            feature_list = []\n",
        "            delta_feature_list = [] # f_(t+c) - f_t\n",
        "            delta_feature_for_worker_list = [] # f_t - f_(t-i)\n",
        "            prediction_list = []\n",
        "            real_goal_list = []\n",
        "            all_goal_list = []\n",
        "            gen_token_list = []\n",
        "            batch_size = generator.worker.batch_size\n",
        "            seq_len = discriminator.seq_len\n",
        "            step_size = generator.step_size\n",
        "            goal_out_size = generator.worker.goal_out_size\n",
        "            vocab_size = discriminator.vocab_size\n",
        "            \"\"\"\n",
        "            Perform forward step for adversarial training for discriminator and generator\n",
        "            \"\"\"\n",
        "            while t < seq_len + 1:\n",
        "                #Extract Feature from D\n",
        "                if t == 0:\n",
        "                    cur_sen = Variable(nn.init.constant_(\n",
        "                        torch.zeros(batch_size, seq_len), vocab_size\n",
        "                    )).long()\n",
        "                else:\n",
        "                    #print(\"Cur sen size before permute: {}\".format(cur_sen.size()))\n",
        "                    cur_sen = torch.stack(gen_token_list).permute(1,0)\n",
        "                    #print(\"Cur sen size: {}\".format(cur_sen.size()))\n",
        "                    cur_sen = F.pad(cur_sen, (0, seq_len - t), value=vocab_size)\n",
        "                #Why no cuda here: CHECK: ADD CUDA!!!!\n",
        "                cur_sen = cur_sen.to(device, non_blocking=True)\n",
        "                f_t = discriminator(cur_sen)[\"feature\"]\n",
        "                #Generator forward step\n",
        "                x_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal, sub_goal, probs, t_ = generator(x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal, t, temperature)\n",
        "                if t % step_size == 0:\n",
        "                    if t > 0:\n",
        "                        real_goal = last_goal\n",
        "                    last_goal = Variable(torch.zeros(batch_size, goal_out_size)).to(device, non_blocking=True)\n",
        "                    real_goal_list.append(real_goal)\n",
        "                #Store info for calculating loss function\n",
        "                feature_list.append(f_t)\n",
        "                prediction_list.append(probs)\n",
        "                if t > 0:\n",
        "                    if t % step_size == 0:\n",
        "                        delta_feature_list.append(f_t-feature_list[t-step_size])\n",
        "                        delta_feature_for_worker_list.append(f_t - feature_list[t - step_size])\n",
        "                    else:\n",
        "                        delta_feature_for_worker_list.append(f_t - feature_list[t - t%step_size])\n",
        "                    all_goal_list.append(real_goal)\n",
        "                gen_token_list.append(x_t) #next token generated by G\n",
        "                t = t_\n",
        "                #print(\"X size: {}\".format(x_t.size()))\n",
        "            #Post Process and return variables\n",
        "            if len(real_goal_list) == len(delta_feature_list) + 1:\n",
        "                real_goal_list = real_goal_list[:-1]\n",
        "            prediction_list = prediction_list[:-1]\n",
        "            gen_token_list = gen_token_list[:-1]\n",
        "            real_goal_var = torch.stack(real_goal_list).permute(1,0,2)\n",
        "            all_goal_var = torch.stack(all_goal_list).permute(1,0,2)\n",
        "            prediction_var = torch.stack(prediction_list).permute(1,0,2)\n",
        "            delta_feature_var = torch.stack(delta_feature_list).permute(1,0,2)\n",
        "            #print(delta_feature_var)\n",
        "            #print(\"Delta feature list size: {}\".format(len(delta_feature_list)))\n",
        "            #print(\"Gen token list size: {}\".format(len(gen_token_list)))\n",
        "            gen_token_var = torch.stack(gen_token_list).permute(1,0)\n",
        "            #print(\"Gen token var after correct permute: {}\".format(gen_token_var.size()))\n",
        "            delta_feature_for_worker_var = torch.stack(delta_feature_for_worker_list).permute(1,0,2)\n",
        "            results = {\"real_goal\": real_goal_var,\n",
        "                        \"all_goal\": all_goal_var,\n",
        "                        \"prediction\": prediction_var,\n",
        "                        \"delta_feature\": delta_feature_var,\n",
        "                        \"delta_feature_for_worker\": delta_feature_for_worker_var,\n",
        "                        \"gen_token\": gen_token_var}\n",
        "            for result in results.values():\n",
        "                if result.is_contiguous():\n",
        "                    result = result.contiguous()\n",
        "            return results\n",
        "        return func\n",
        "\n",
        "    elif f_type == \"rollout\":\n",
        "        def func(model_dict, input_x, given_num, use_cuda=False, temperature=1.0):\n",
        "            #Get G and D\n",
        "            generator = model_dict[\"generator\"]\n",
        "            discriminator = model_dict[\"discriminator\"]\n",
        "            #Init vairables and lists for forward step\n",
        "            h_w_t, c_w_t, h_m_t, c_m_t, last_goal, real_goal, x_t = \\\n",
        "                init_vars(generator, discriminator, use_cuda)\n",
        "            t = 0\n",
        "            gen_token_list = []\n",
        "            batch_size = generator.worker.batch_size\n",
        "            seq_len = discriminator.seq_len\n",
        "            step_size = generator.step_size\n",
        "            goal_out_size = generator.worker.goal_out_size\n",
        "            vocab_size = discriminator.vocab_size\n",
        "            #Use input_x to perform G forward step\n",
        "            while t < given_num +1:\n",
        "                #Extract f_t\n",
        "                if t == 0:\n",
        "                    cur_sen = Variable(nn.init.constant_(torch.zeros(batch_size, seq_len), vocab_size)).long().to(device, non_blocking=True)\n",
        "                else:\n",
        "                    cur_sen = torch.stack(gen_token_list).permute(1,0)\n",
        "                    cur_sen = F.pad(cur_sen, (0, seq_len - t), value=vocab_size)\n",
        "                f_t = discriminator(cur_sen)[\"feature\"]\n",
        "                #G forward step now that you have f\n",
        "                _, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal,\\\n",
        "                sub_goal, probs, t_ = generator( x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal, t, temperature)\n",
        "                if t % step_size == 0:\n",
        "                    if t > 0:\n",
        "                        real_goal = last_goal\n",
        "                    last_goal = Variable(torch.zeros(batch_size, goal_out_size)).to(device, non_blocking=True)\n",
        "                if t < given_num:\n",
        "                    x_t = input_x[:, t].contiguous()\n",
        "                    gen_token_list.append(x_t)\n",
        "                t = t_\n",
        "                #Perform Rollout\n",
        "            while t < seq_len + 1:\n",
        "                #Extract feature f_t\n",
        "                if len(gen_token_list) == 0:\n",
        "                    cur_sen = Variable(nn.init.constant_(torch.zeros(batch_size, seq_len), vocab_size)).long().to(device, non_blocking=True)\n",
        "                else:\n",
        "                    cur_sen = torch.stack(gen_token_list).permute(1,0)\n",
        "                    cur_sen = F.pad(cur_sen, (0, seq_len - t + 1), value=vocab_size)\n",
        "                f_t = discriminator(cur_sen)[\"feature\"]\n",
        "                #Generator forward step\n",
        "                x_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal,sub_goal, probs, t_ = generator(x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal,\n",
        "                    real_goal, t, temperature)\n",
        "                if t % step_size == 0:\n",
        "                    real_goal = last_goal\n",
        "                last_goal = Variable(torch.zeros(\n",
        "                    batch_size, goal_out_size\n",
        "                )).to(device, non_blocking=True)\n",
        "                gen_token_list.append(x_t)\n",
        "                t = t_\n",
        "            gen_token = torch.stack(gen_token_list).permute(1, 0)\n",
        "            return gen_token\n",
        "        return func\n",
        "    elif f_type == \"gen\":\n",
        "        def func(model_dict, use_cuda=False, temperature=1.0):\n",
        "            generator = model_dict[\"generator\"]\n",
        "            discriminator = model_dict[\"discriminator\"]\n",
        "            h_w_t, c_w_t, h_m_t, c_m_t, last_goal, real_goal, x_t = \\\n",
        "                init_vars(generator, discriminator, use_cuda)\n",
        "            t = 0\n",
        "            gen_token_list = []\n",
        "            batch_size = generator.worker.batch_size\n",
        "            seq_len = discriminator.seq_len\n",
        "            step_size = generator.step_size\n",
        "            goal_out_size = generator.worker.goal_out_size\n",
        "            vocab_size = discriminator.vocab_size\n",
        "            #G forward\n",
        "            while t < seq_len:\n",
        "                #Extract f_t\n",
        "                if t == 0:\n",
        "                    cur_sen = Variable(nn.init.constant_(\n",
        "                        torch.zeros(batch_size, seq_len), vocab_size)\n",
        "                    ).long().to(device, non_blocking=True)\n",
        "                else:\n",
        "                    cur_sen = torch.stack(gen_token_list).permute(1, 0)\n",
        "                    cur_sen = F.pad(\n",
        "                        cur_sen, (0, seq_len - t), value=vocab_size\n",
        "                    )\n",
        "                # print('>', t, device, cur_sen.device)\n",
        "                # print('>>', discriminator.emb.weight.device)\n",
        "                f_t = discriminator(cur_sen)[\"feature\"]\n",
        "                #G forward step\n",
        "                # for i, x in enumerate([x_t, f_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal]):\n",
        "                #     print(i, x.device, end=', ')\n",
        "                x_t, h_m_t, c_m_t, h_w_t, c_w_t, last_goal, real_goal, sub_goal, probs, t_ = generator(x_t, f_t, h_m_t, c_m_t,\n",
        "                        h_w_t, c_w_t, last_goal,real_goal, t, temperature)\n",
        "                if t % step_size == 0:\n",
        "                    if t > 0:\n",
        "                        real_goal = last_goal\n",
        "                        last_goal = Variable(torch.zeros(batch_size, goal_out_size)).to(device)\n",
        "                    last_goal.to(device, non_blocking=True)\n",
        "                gen_token_list.append(x_t)\n",
        "                t = t_\n",
        "            gen_token = torch.stack(gen_token_list).permute(1,0)\n",
        "            return gen_token\n",
        "        return func\n",
        "    else:\n",
        "        raise(\"Invalid function type\")\n",
        "\n",
        "def init_vars(generator, discriminator, use_cuda=False):\n",
        "    h_w_t, c_w_t = generator.init_hidden() #worker unit of gen\n",
        "    h_m_t, c_m_t = generator.init_hidden() #manager unit of gen\n",
        "    last_goal = Variable(torch.zeros(generator.worker.batch_size, generator.worker.goal_out_size)) #bach_size * goal_out_size\n",
        "    real_goal = generator.manager.goal_init\n",
        "    x_t = Variable(nn.init.constant_(torch.Tensor(\n",
        "        generator.worker.batch_size\n",
        "    ), discriminator.start_token)).long()\n",
        "    variables_ = [h_w_t, c_w_t, h_m_t, c_m_t, last_goal, real_goal, x_t]\n",
        "    vs = []\n",
        "    for var in variables_:\n",
        "        var = var.to(device, non_blocking=True)\n",
        "        vs.append(var)\n",
        "    return vs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXKt0hY0fhgD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4l0qqbIcQSU",
        "outputId": "c66ae039-c66f-4407-c013-eb8384ceca43",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################################################################\n",
            "Start Pretraining Discriminator...\n",
            "Epoch: 0/20  Pre-Discriminator\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-Discriminator loss: 0.28354\n",
            "elapsed: 147.7s\n",
            "Epoch: 1/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.12144\n",
            "elapsed: 131.7s\n",
            "Epoch: 2/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.08997\n",
            "elapsed: 106.0s\n",
            "Epoch: 3/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.06707\n",
            "elapsed: 154.1s\n",
            "Epoch: 4/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.04591\n",
            "elapsed: 160.1s\n",
            "Epoch: 5/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.03026\n",
            "elapsed: 121.4s\n",
            "Epoch: 6/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01947\n",
            "elapsed: 166.7s\n",
            "Epoch: 7/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01401\n",
            "elapsed: 166.0s\n",
            "Epoch: 8/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00858\n",
            "elapsed: 131.7s\n",
            "Epoch: 9/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00568\n",
            "elapsed: 121.0s\n",
            "Epoch: 10/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00448\n",
            "elapsed: 111.1s\n",
            "Epoch: 11/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00328\n",
            "elapsed: 178.3s\n",
            "Epoch: 12/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00306\n",
            "elapsed: 132.5s\n",
            "Epoch: 13/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00236\n",
            "elapsed: 143.3s\n",
            "Epoch: 14/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00232\n",
            "elapsed: 184.6s\n",
            "Epoch: 15/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00171\n",
            "elapsed: 128.3s\n",
            "Epoch: 16/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00362\n",
            "elapsed: 134.1s\n",
            "Epoch: 17/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00161\n",
            "elapsed: 87.6s\n",
            "Epoch: 18/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00128\n",
            "elapsed: 172.7s\n",
            "Epoch: 19/20  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00112\n",
            "elapsed: 142.2s\n"
          ]
        }
      ],
      "source": [
        "print(\"#########################################################################\")\n",
        "print(\"Start Pretraining Discriminator...\")\n",
        "if use_cuda:\n",
        "    dis_data_params[\"pin_memory\"] = True\n",
        "pos_file = dis_data_params[\"positive_filepath\"]\n",
        "neg_file = dis_data_params[\"negative_filepath\"]\n",
        "batch_size = train_params[\"generated_num\"]\n",
        "for i in range(train_params[\"pre_dis_epoch_num\"]):\n",
        "    t0 = time()\n",
        "    print(\"Epoch: {}/{}  Pre-Discriminator\".format(i, train_params[\"pre_dis_epoch_num\"]))\n",
        "    # break\n",
        "    model_dict, optimizer_dict, scheduler_dict = pretrain_discriminator(\n",
        "        model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size,\n",
        "        positive_file=pos_file, negative_file=neg_file, batch_size=batch_size, epochs=1, use_cuda=use_cuda)\n",
        "    print(f'elapsed: {time() - t0:.1f}s')\n",
        "ckpt_num = 0\n",
        "# save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEEeh2vBf6qD"
      },
      "source": [
        "<a name=\"disc_pretrain\"></a>\n",
        "#### Pretrain generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOwgBm5pf7Lx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def pretrain_generator(model_dict, optimizer_dict, scheduler_dict, dataloader, vocab_size, max_norm=5.0, use_cuda=False, epoch=1, tot_epochs=100):\n",
        "    #get the models of generator\n",
        "    generator = model_dict[\"generator\"]\n",
        "    worker = generator.worker\n",
        "    manager = generator.manager\n",
        "    #get the optimizers\n",
        "    m_optimizer = optimizer_dict[\"manager\"]\n",
        "    w_optimizer = optimizer_dict[\"worker\"]\n",
        "\n",
        "    m_optimizer.zero_grad()\n",
        "    w_optimizer.zero_grad()\n",
        "\n",
        "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
        "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
        "    \"\"\"\n",
        "     Perform pretrain step for real data\n",
        "    \"\"\"\n",
        "\n",
        "    for i, sample in enumerate(dataloader):\n",
        "        #print(\"DataLoader: {}\".format(dataloader))\n",
        "        m_lr_scheduler.step()\n",
        "        w_lr_scheduler.step()\n",
        "\n",
        "        sample = Variable(sample)\n",
        "        sample.to(device, non_blocking=True)\n",
        "\n",
        "        # Calculate pretrain loss\n",
        "        if (sample.size() == torch.zeros([64, 20]).size()): #sometimes smaller than 64 (16) is passed, so this if statement disables it\n",
        "            #print(\"Sample size: {}\".format(sample.size()))\n",
        "            pre_rets = recurrent_func(\"pre\")(model_dict, sample, use_cuda)\n",
        "            real_goal = pre_rets[\"real_goal\"]\n",
        "            prediction = pre_rets[\"prediction\"]\n",
        "            delta_feature = pre_rets[\"delta_feature\"]\n",
        "\n",
        "            m_loss = loss_func(\"pre_manager\")(real_goal, delta_feature)\n",
        "            torch.autograd.grad(m_loss, manager.parameters())\n",
        "            clip_grad_norm_(manager.parameters(), max_norm=max_norm)\n",
        "            m_optimizer.step()\n",
        "            m_optimizer.zero_grad()\n",
        "\n",
        "            w_loss = loss_func(\"pre_worker\")(sample, prediction, vocab_size, use_cuda)\n",
        "            torch.autograd.grad(w_loss, worker.parameters())\n",
        "            clip_grad_norm_(worker.parameters(), max_norm=max_norm)\n",
        "            w_optimizer.step()\n",
        "            w_optimizer.zero_grad()\n",
        "            if i >= 0:\n",
        "                print(\"Pre-Manager Loss: {:.5f}, Pre-Worker Loss: {:.5f}\\n\".format(m_loss, w_loss))\n",
        "    \"\"\"\n",
        "    Update model_dict, optimizer_dict, and scheduler_dict\n",
        "    \"\"\"\n",
        "\n",
        "    generator.woroker = worker\n",
        "    generator.manager = manager\n",
        "    model_dict[\"generator\"] = generator\n",
        "\n",
        "    optimizer_dict[\"manager\"] = m_optimizer\n",
        "    optimizer_dict[\"worker\"] = w_optimizer\n",
        "\n",
        "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
        "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
        "\n",
        "    return model_dict, optimizer_dict, scheduler_dict\n",
        "\n",
        "def loss_func(f_type=\"pre_worker\"):\n",
        "    \"\"\"\n",
        "    5 kind of loss function: pre_worker, pre_manager, adv_worker, adv_manager, dis\n",
        "    \"\"\"\n",
        "    if f_type == \"pre_worker\":\n",
        "        def func(real_data, prediction, vocab_size, use_cuda=False):\n",
        "            #print(\"Prediction shape before: {}\".format(prediction.size()))\n",
        "            prediction = torch.clamp(prediction, 1e-20, 1.0) # put min and max boundaries\n",
        "            #print(\"One Hot: {}\".format(one_hot(real_data, vocab_size, use_cuda).size()))\n",
        "            #print(\"Real data size: {}\".format(real_data.size()))\n",
        "            #print(\"Log Prediction: {}\".format(torch.log(prediction).size()))\n",
        "            hot_one = one_hot(real_data, vocab_size, use_cuda)\n",
        "            #print(\"Pred after reshape: {}\".format(prediction.size()))\n",
        "            #print(\"One Hot after reshape: {}\".format(hot_one.size()))\n",
        "            loss = -torch.mean(one_hot(real_data, vocab_size, use_cuda) * torch.log(prediction))\n",
        "            return loss\n",
        "        return func\n",
        "    elif f_type == \"pre_manager\":\n",
        "        def func(real_goal, delta_feature):\n",
        "            loss = -torch.mean(1.0 - F.cosine_similarity(real_goal, delta_feature))\n",
        "            return loss\n",
        "        return func\n",
        "    elif f_type == \"adv_worker\":\n",
        "        def func(all_goal, delta_feature_for_worker, gen_token, prediction, vocab_size, use_cuda=False):\n",
        "            intrinsic_rewards = 1.0 - F.cosine_similarity(all_goal, delta_feature_for_worker, dim=2)\n",
        "            prediction = torch.clamp(prediction, 1e-20, 1.0)\n",
        "            loss = -torch.mean(intrinsic_rewards * torch.sum(one_hot(gen_token, vocab_size, use_cuda)* torch.log(prediction), dim=2))\n",
        "            return loss\n",
        "        return func\n",
        "    elif f_type == \"adv_manager\":\n",
        "        def func(rewards, real_goal, delta_feature):\n",
        "            loss = -torch.mean(rewards*(1.0 - F.cosine_similarity(delta_feature, real_goal, dim=2)))\n",
        "            return loss\n",
        "        return func\n",
        "    elif f_type == \"dis\":\n",
        "        def func(discriminator, input_x, score, use_cuda=False):\n",
        "            \"\"\"\n",
        "            input_x:\n",
        "                size(batch_size*seq_len)\n",
        "                type(torch.LongTensor)\n",
        "            score:\n",
        "                size(batch_size * seq_len * vocab_size)\n",
        "                type(torch.FloatTensor)\n",
        "            \"\"\"\n",
        "            loss_func = nn.CrossEntropyLoss()\n",
        "            if use_cuda:\n",
        "                loss_func = loss_func.cuda()\n",
        "            input_x = input_x.view(-1) #last dim\n",
        "            batch_size, seq_len, vocab_size = score.size()\n",
        "            score = score.view(batch_size * seq_len, -1) #reshape\n",
        "            loss = loss_func(score, input_x) + discriminator.l2_loss()\n",
        "            return loss\n",
        "        return func\n",
        "    else:\n",
        "        raise(\"Invalid loss function type\")\n",
        "\n",
        "def one_hot(x, vocab_size, use_cuda=False):\n",
        "    batch_size, seq_len = x.size()\n",
        "    #print(x.device)\n",
        "    out = torch.zeros(batch_size* seq_len, vocab_size, device=x.device)\n",
        "    #\n",
        "    # print(out.size())\n",
        "    x = x.contiguous()\n",
        "    x = x.view(-1, 1)\n",
        "    #print(\"X size: {}\".format(x.size()))\n",
        "    #print(\"Out size: {}\".format(out.size()))\n",
        "    #print(\"Out size at dim 1: {}\".format(out.size(1)))\n",
        "    if (x.data < vocab_size).all() == 0:\n",
        "        for i, d in enumerate(x.data):\n",
        "            if x[i].item() > vocab_size - 1 :\n",
        "                x[i] = 0\n",
        "                #print(x[i])\n",
        "                #print (i)\n",
        "    out = out.scatter_(1, x.data, 1.0) #setting particular values of a tensor at the provided indices, one hot vector at positions where there is word\n",
        "    \"\"\"\n",
        "        check places with 1.0 in out\n",
        "        a = (out == 1.0).nonzero()\n",
        "        print(a)\n",
        "    \"\"\"\n",
        "    out = out.view(batch_size, seq_len, vocab_size)\n",
        "    out = Variable(out)\n",
        "    out.to(device, non_blocking=True)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "VMfEANUPfsZ9",
        "outputId": "1d9ea277-8487-4977-bfae-faa523f0e760",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################################################################\n",
            "Start Pretraining Generator...\n",
            "Epoch: 0/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 1/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 2/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 3/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 4/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 5/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 6/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 7/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 8/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 9/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 10/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 11/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 12/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 13/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 14/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 15/30  Pre-Generator\n",
            "elapsed: 0.4s\n",
            "Epoch: 16/30  Pre-Generator\n",
            "elapsed: 0.4s\n",
            "Epoch: 17/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 18/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 19/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 20/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 21/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 22/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 23/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 24/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 25/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 26/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 27/30  Pre-Generator\n",
            "elapsed: 0.6s\n",
            "Epoch: 28/30  Pre-Generator\n",
            "elapsed: 0.5s\n",
            "Epoch: 29/30  Pre-Generator\n",
            "elapsed: 0.6s\n"
          ]
        }
      ],
      "source": [
        "#Pretrain generator\n",
        "print (\"#########################################################################\")\n",
        "print (\"Start Pretraining Generator...\")\n",
        "if use_cuda:\n",
        "    real_data_params[\"pin_memory\"] = True\n",
        "r_dataloader = real_data_loader(**real_data_params)\n",
        "for epoch in range(train_params[\"pre_gen_epoch_num\"]):\n",
        "    t0 = time()\n",
        "    # break\n",
        "    print(\"Epoch: {}/{}  Pre-Generator\".format(epoch, train_params[\"pre_gen_epoch_num\"]))\n",
        "    model_dict, optimizer_dict, scheduler_dict = pretrain_generator(\n",
        "        model_dict, optimizer_dict, scheduler_dict, r_dataloader, vocab_size=vocab_size, use_cuda=use_cuda,\n",
        "        epoch=epoch, tot_epochs=range(train_params[\"pre_gen_epoch_num\"]))\n",
        "    print(f'elapsed: {time() - t0:.1f}s')\n",
        "#Finish pretrain and save the checkpoint\n",
        "# save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEwuIkQ9g0r7"
      },
      "source": [
        "<a name=\"adversarial_train\"></a>\n",
        "#### Adversarial train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNv0MpFdhV-9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def adversarial_train(model_dict, optimizer_dict, scheduler_dict, dis_dataloader_params,\n",
        "                      vocab_size, pos_file, neg_file, batch_size, gen_train_num=1,\n",
        "                      dis_train_epoch=5, dis_train_num=3, max_norm=5.0,\n",
        "                      rollout_num=4, use_cuda=False, temperature=1.0, epoch=1, tot_epoch=100):\n",
        "    \"\"\"\n",
        "        Get all the models, optimizer and schedulers\n",
        "    \"\"\"\n",
        "    generator = model_dict[\"generator\"]\n",
        "    discriminator = model_dict [\"discriminator\"]\n",
        "    worker = generator.worker\n",
        "    manager = generator.manager\n",
        "\n",
        "    m_optimizer = optimizer_dict[\"manager\"]\n",
        "    w_optimizer = optimizer_dict[\"worker\"]\n",
        "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
        "\n",
        "    #Why zero grad only m and w?\n",
        "    m_optimizer.zero_grad()\n",
        "    w_optimizer.zero_grad()\n",
        "\n",
        "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
        "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
        "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
        "\n",
        "    #Adversarial training for generator\n",
        "    for _ in range(gen_train_num):\n",
        "        m_lr_scheduler.step()\n",
        "        w_lr_scheduler.step()\n",
        "\n",
        "        m_optimizer.zero_grad()\n",
        "        w_optimizer.zero_grad()\n",
        "\n",
        "        #get all the return values\n",
        "        adv_rets = recurrent_func(\"adv\")(model_dict, use_cuda)\n",
        "        real_goal = adv_rets[\"real_goal\"]\n",
        "        all_goal = adv_rets[\"all_goal\"]\n",
        "        prediction = adv_rets[\"prediction\"]\n",
        "        delta_feature = adv_rets[\"delta_feature\"]\n",
        "        delta_feature_for_worker = adv_rets[\"delta_feature_for_worker\"]\n",
        "        gen_token = adv_rets[\"gen_token\"]\n",
        "\n",
        "        rewards = get_rewards(model_dict, gen_token, rollout_num, use_cuda)\n",
        "        # NOTE: The same fix as in recurrent_func(\"adv\") for real_goal to prevent shape mismatch\n",
        "        # TODO: Why this fix is needed?\n",
        "        if rewards.size()[1] == delta_feature.size()[1] + 1:\n",
        "            rewards = rewards[:, :-1]\n",
        "        m_loss = loss_func(\"adv_manager\")(rewards, real_goal, delta_feature)\n",
        "        w_loss = loss_func(\"adv_worker\")(all_goal, delta_feature_for_worker, gen_token, prediction, vocab_size, use_cuda)\n",
        "\n",
        "        torch.autograd.grad(m_loss, manager.parameters()) #based on loss improve the parameters\n",
        "        torch.autograd.grad(w_loss, worker.parameters())\n",
        "        clip_grad_norm_(manager.parameters(), max_norm)\n",
        "        clip_grad_norm_(worker.parameters(), max_norm)\n",
        "        m_optimizer.step()\n",
        "        w_optimizer.step()\n",
        "        print(\"Adv-Manager loss: {:.5f} Adv-Worker loss: {:.5f}\".format(m_loss, w_loss))\n",
        "\n",
        "    del adv_rets\n",
        "    del real_goal\n",
        "    del all_goal\n",
        "    del prediction\n",
        "    del delta_feature\n",
        "    del delta_feature_for_worker\n",
        "    del gen_token\n",
        "    del rewards\n",
        "\n",
        "    #Adversarial training for discriminator\n",
        "    for n in range(dis_train_epoch):\n",
        "        generate_samples(model_dict, neg_file, batch_size, use_cuda, temperature)\n",
        "        dis_dataloader_params[\"positive_filepath\"] = pos_file\n",
        "        dis_dataloader_params[\"negative_filepath\"] = neg_file\n",
        "        dataloader = dis_data_loader(**dis_dataloader_params)\n",
        "\n",
        "        cross_entropy = nn.CrossEntropyLoss()\n",
        "        if use_cuda:\n",
        "            cross_entropy = cross_entropy.cuda()\n",
        "        \"\"\"\n",
        "        for d-steps do\n",
        "            Use current G, θm,θw to generate negative examples and combine with given positive examples S\n",
        "            Train discriminator Dφ for k epochs by Eq. (2)\n",
        "        end for\n",
        "        \"\"\"\n",
        "        for _ in range(dis_train_num):\n",
        "            for i, sample in enumerate(dataloader):\n",
        "                data, label = sample[\"data\"], sample[\"label\"]\n",
        "                data = Variable(data).to(device, non_blocking=True)\n",
        "                label = Variable(label).to(device, non_blocking=True)\n",
        "                outs = discriminator(data)\n",
        "                loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
        "                d_optimizer.zero_grad()\n",
        "                d_lr_scheduler.step()\n",
        "                loss.backward()\n",
        "                d_optimizer.step()\n",
        "        print(\"{}/{} Adv-Discriminator Loss: {:.5f}\".format(n, range(dis_train_epoch),loss))\n",
        "    #Save all changes\n",
        "    model_dict[\"discriminator\"] = discriminator\n",
        "    generator.worker = worker\n",
        "    generator.manager = manager\n",
        "    model_dict[\"generator\"] = generator\n",
        "\n",
        "    optimizer_dict[\"manager\"] = m_optimizer\n",
        "    optimizer_dict[\"worker\"] = w_optimizer\n",
        "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
        "\n",
        "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
        "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
        "    scheduler_dict[\"disciminator\"] = d_lr_scheduler\n",
        "\n",
        "    return model_dict, optimizer_dict, scheduler_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yVh0RcmGlyEq"
      },
      "outputs": [],
      "source": [
        "def get_rewards(model_dict, input_x, rollout_num, use_cuda=False, temperature=1.0, delta=16.0):\n",
        "    #Get G and D\n",
        "    generator = model_dict[\"generator\"]\n",
        "    discriminator = model_dict[\"discriminator\"]\n",
        "    discriminator = discriminator.eval()\n",
        "    #Prepare constants\n",
        "    seq_len = discriminator.seq_len\n",
        "    step_size = generator.step_size\n",
        "    #Perform rollout and calculate reward\n",
        "    rewards = []\n",
        "    rollout_func = recurrent_func(\"rollout\")\n",
        "    for i in range(rollout_num):\n",
        "        given_num = 0\n",
        "        #print(\"Sequence length: {}\".format(seq_len))\n",
        "        #print(\"i stage in rollout: {}\".format(i))\n",
        "        while given_num < seq_len:\n",
        "            sample_for_reward = rollout_func(model_dict, input_x, given_num, use_cuda, temperature)\n",
        "            pred = discriminator(sample_for_reward)[\"pred\"]\n",
        "            pred = pred[:, 1].data\n",
        "            if use_cuda:\n",
        "                pred = pred.cpu()\n",
        "            pred = pred.numpy()\n",
        "            pred = pred.reshape(-1)\n",
        "            if i == 0:\n",
        "                rewards.append(pred)\n",
        "            else:\n",
        "                rewards[int(given_num/step_size -1)] += pred\n",
        "            given_num += step_size\n",
        "    rewards = rescale(rewards, delta) / rollout_num\n",
        "    if use_cuda:\n",
        "        rewards = rewards.cuda(non_blocking=True)\n",
        "    discriminator = discriminator.train()\n",
        "    return rewards\n",
        "\n",
        "def rescale(rewards, delta=16.0):\n",
        "    \"\"\"\n",
        "    Why Rescaled activation: during adversarial training of SeqGAN severe gradient vanishing occurs when D is much stronger than G, i.e. the reward is too small value to update the parameters\n",
        "    and thus need to be rescaled before being fed into G.\n",
        "        parameters for rewards:\n",
        "            type: list\n",
        "            length: seq_len / c, where c is c recent goals(steps into future)\n",
        "            elements: np.array(size=batch_size)\n",
        "            R(reward matrix) = expit(delta * (0.5 - rank(i)/B)), where expit, is an activation function that re-projects the equidifferent scoring based on ranking to a more effective distribution.\n",
        "            In this model authors of the paper decided expit to be sigmoid function: expit = 1/(1+exp(-x))\n",
        "    \"\"\"\n",
        "    r = np.array(rewards)\n",
        "    _, batch_size = r.shape\n",
        "    order = np.argsort(r)\n",
        "    rank = np.argsort(order)\n",
        "    rank = batch_size - rank\n",
        "    rescaled_rewards = expit(delta*(0.5 - rank/batch_size))\n",
        "    rescaled_rewards = np.transpose(rescaled_rewards)\n",
        "    return Variable(torch.from_numpy(rescaled_rewards)).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xMCHE4zGlyEr"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFxV58yCgy_J",
        "outputId": "39fa757c-b68f-46bd-9eea-f75010e2674b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################################################################\n",
            "Start Adversarial Training...\n",
            "Epoch: 0/20  Adv\n",
            "Adv-Manager loss: -0.12296 Adv-Worker loss: 4.21129\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00092\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00073\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00072\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00048\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00041\n",
            "elapsed: 1080.9s\n",
            "Epoch: 1/20  Adv\n",
            "Adv-Manager loss: -0.12297 Adv-Worker loss: 4.21359\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00066\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00026\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00022\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00022\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00017\n",
            "elapsed: 1054.5s\n",
            "Epoch: 2/20  Adv\n",
            "Adv-Manager loss: -0.12294 Adv-Worker loss: 4.21366\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00013\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00012\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00009\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00009\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00015\n",
            "elapsed: 1051.3s\n",
            "Epoch: 3/20  Adv\n",
            "Adv-Manager loss: -0.12307 Adv-Worker loss: 4.21205\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00006\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00006\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00005\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00005\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00004\n",
            "elapsed: 1093.2s\n",
            "Epoch: 4/20  Adv\n",
            "Adv-Manager loss: -0.12320 Adv-Worker loss: 4.21221\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00004\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00003\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00004\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00003\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00003\n",
            "elapsed: 1108.7s\n",
            "Epoch: 5/20  Adv\n",
            "Adv-Manager loss: -0.12301 Adv-Worker loss: 4.21127\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00003\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "elapsed: 1048.8s\n",
            "Epoch: 6/20  Adv\n",
            "Adv-Manager loss: -0.12303 Adv-Worker loss: 4.21346\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 1062.2s\n",
            "Epoch: 8/20  Adv\n",
            "Adv-Manager loss: -0.12305 Adv-Worker loss: 4.21275\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00002\n",
            "elapsed: 1010.0s\n",
            "Epoch: 9/20  Adv\n",
            "Adv-Manager loss: -0.12293 Adv-Worker loss: 4.21202\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 1014.2s\n",
            "Epoch: 10/20  Adv\n",
            "Adv-Manager loss: -0.12275 Adv-Worker loss: 4.21070\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 1060.8s\n",
            "Epoch: 11/20  Adv\n",
            "Adv-Manager loss: -0.12324 Adv-Worker loss: 4.21821\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 952.9s\n",
            "Epoch: 12/20  Adv\n",
            "Adv-Manager loss: -0.12321 Adv-Worker loss: 4.21326\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 781.3s\n",
            "Epoch: 13/20  Adv\n",
            "Adv-Manager loss: -0.12299 Adv-Worker loss: 4.21031\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 794.1s\n",
            "Epoch: 14/20  Adv\n",
            "Adv-Manager loss: -0.12306 Adv-Worker loss: 4.21223\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 769.2s\n",
            "Epoch: 15/20  Adv\n",
            "Adv-Manager loss: -0.12297 Adv-Worker loss: 4.21067\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 792.6s\n",
            "Epoch: 16/20  Adv\n",
            "Adv-Manager loss: -0.12307 Adv-Worker loss: 4.21272\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 808.5s\n",
            "Epoch: 17/20  Adv\n",
            "Adv-Manager loss: -0.12297 Adv-Worker loss: 4.21556\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 781.6s\n",
            "Epoch: 18/20  Adv\n",
            "Adv-Manager loss: -0.12285 Adv-Worker loss: 4.20811\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 787.1s\n",
            "Epoch: 19/20  Adv\n",
            "Adv-Manager loss: -0.12278 Adv-Worker loss: 4.21041\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00001\n",
            "elapsed: 777.0s\n"
          ]
        }
      ],
      "source": [
        "ckpt_num = 1\n",
        "#Adversarial train of D and G\n",
        "print (\"#########################################################################\")\n",
        "print (\"Start Adversarial Training...\")\n",
        "save_num = train_params[\"save_num\"] #save checkpoint after this number of repetitions\n",
        "replace_num = train_params[\"replace_num\"]\n",
        "\n",
        "train_params[\"total_epoch\"] = 20\n",
        "for epoch in range(train_params[\"total_epoch\"]):\n",
        "    t0 = time()\n",
        "    print(\"Epoch: {}/{}  Adv\".format(epoch, train_params[\"total_epoch\"]))\n",
        "    # break\n",
        "    model_dict, optimizer_dict, scheduler_dict = adversarial_train(\n",
        "        model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size, pos_file=pos_file,\n",
        "        neg_file=neg_file, batch_size=batch_size, use_cuda=use_cuda, epoch=epoch, tot_epoch=train_params[\"total_epoch\"])\n",
        "    print(f'elapsed: {time() - t0:.1f}s')\n",
        "    if (epoch + 1) % save_num == 0:\n",
        "        ckpt_num += 1\n",
        "        # if ckpt_num % replace_num == 0:\n",
        "        #     save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=True)\n",
        "        # else:\n",
        "        #     save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dLEJ5oCklyEr",
        "outputId": "fe55bcca-9acc-4f5a-b808-f7496fbffc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 8200\n",
            "-rw-r--r-- 1 root root 5111936 Oct 12 19:21 gen_corpus.npy\n",
            "-rw-r--r-- 1 root root 3276928 Oct 12 13:25 train_corpus.npy\n"
          ]
        }
      ],
      "source": [
        "!ls -l data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_FHvMbh7lyEs",
        "outputId": "f3912d6b-ce0b-4a54-cf7a-ebed8b2400f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6400, 64), (9984, 64))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Xr = np.load('data/train_corpus.npy')\n",
        "Xg = np.load('data/gen_corpus.npy')\n",
        "Xr.shape, Xg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbQpXs-WlyEs"
      },
      "outputs": [],
      "source": [
        "def to_text(t):\n",
        "    return ''.join(inv_vocab[i] for i in t.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DehNj_VclyEs",
        "outputId": "419e5860-fe2e-47a2-ab12-c93888a8ef00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[35 50 11 16 59 50 54 61 49 13  3 46  9 22 25 32 16 62 37 40  0  4 49 57\n",
            " 21 62 13 48 17 30 58 28 39 48  6 55 49 13  0 20 20 59 41 25 29 24 19 39\n",
            " 64 46 50 47  1 13 23 63  9 54 10 30  5 56 20 64]\n",
            "Ti6Arimth8,e4GJQAuVY\n",
            ".hpFu8gBOqMXg1nh8\n",
            "EErZJNIDXweif 8Hv4m5O0oEw\n",
            "[67 61 22 11 23 41 37 62 56 35 17 43 44  8 29 24 56 17 63 64 27 52 10 25\n",
            " 62 34 14 46 12 51 37 37 38 11 17 38 25 32  3 43 11 34 15 25 53 15 14 66\n",
            " 61 65 61 26 34 46 61 63 56  8 57 23 41 65  2 11]\n",
            "ztG6HZVuoTBbc3NIoBvwLk5JuS9e7jVVW6BWJQ,b6S;Jl;9ytxtKSetvo3pHZx'6\n",
            "[39 47  4 17 11 26 67  3 32 49 52 43 36 47 31 27 52 55 59 25 50 48 39 25\n",
            " 41 22  8 45 23 38 15 29 20  9 67  5 42 62  7  9  6  0 62 40  9  4  8 15\n",
            " 61  2 62 51 48 30 67 21 50 50 65  4 42 52 14 51]\n",
            "Xf.B6Kz,QhkbUfPLknrJigXJZG3dHW;NE4z0au241\n",
            "uY4.3;t'ujgOzFiix.ak9j\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(Xg[i])\n",
        "    print(to_text(Xg[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9QmbuZZYlyEt",
        "outputId": "f5187710-8f2e-4ac5-8f9b-e52cd4a81a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1 30 55  1 21 50 55 50 61 46 53 66  1 22 46 55 46 59 42 61 46 45  1 28\n",
            " 56 45 46 53 60  1 56 47  1 35 49 46 56 59 50 46 60  1 64 50 61 49  1 42\n",
            " 61  1 28 56 60 61  1 18 56 62 55 61 42 43 53 66]\n",
            " On Finitely Generated Models of Theories with at Most Countably\n",
            "[ 1 22 46 55 46 59 42 53 50 67 46 45  1 54 56 45 46 53 50 55 48  1 56 47\n",
            "  1 46 44 56 53 56 48 50 44 42 53  1 57 56 57 62 53 42 61 50 56 55  1 45\n",
            " 66 55 42 54 50 44 60  1 15  1 30 63 46 59  1 61]\n",
            " Generalized modeling of ecological population dynamics ; Over t\n",
            "[ 1 22 46 55 46 59 42 61 50 55 48  1 34 62 43 60 62 59 47 42 44 46  1 20\n",
            " 42 59 61 49  1 28 56 45 46 53 60  1 62 60 50 55 48  1 19 50 60 44 59 46\n",
            " 61 46  1 33 46 57 59 46 60 46 55 61 42 61 50 56]\n",
            " Generating Subsurface Earth Models using Discrete Representatio\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(Xr[i])\n",
        "    print(to_text(Xr[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Упражнения\n",
        "\n",
        "Да какие упражнения, до конца дочитал - уже молодец ✊"
      ],
      "metadata": {
        "id": "xKT9H5BFm5mY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjSN5MGCM602"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}