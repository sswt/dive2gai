{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention, self-attention, transformers\n",
    "\n",
    "Теория:\n",
    "* [Encoder-Decoder](#encoder-decoder)\n",
    "* [Attention](#attention)\n",
    "* [Self-Attention](#self-attention)\n",
    "* [Transformer](#transformer)\n",
    "* [LLM: BERT, GPT, ...](#llm)\n",
    "* [Размеры LLM моделей](#llm-sizes)\n",
    "* [Byte Pair Encoding](#bpe)\n",
    "* [Генерация текста](generation)\n",
    "\n",
    "Примеры кода:\n",
    "* [Модель PGT на pytorch](1_pure_torch.ipynb)\n",
    "* [GPT с помощью модулей Huggingface](2_transformers.ipynb)\n",
    "* [Предобученные модели](3_pretrained.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"encoder-decoder\"></a>\n",
    "## Encoder-Decoder\n",
    "\n",
    "* [Lena Voita, Sequence to Sequence (seq2seq) and Attention, NLP Course](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "\n",
    "<div><img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png\" width=\"80%\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"attention\"></a>\n",
    "## Attention\n",
    "\n",
    "[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)\n",
    "\n",
    "<div><img src=\"https://i.stack.imgur.com/Xtzg4.png\" width=\"60%\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"self-attention\"></a>\n",
    "## Self-Attention\n",
    "\n",
    "[Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "\n",
    "<div><img src=\"https://i.stack.imgur.com/J45g2.png\" width=\"60%\"/></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transformer\"></a>\n",
    "## Transformer\n",
    "\n",
    "[Vaswani et al. (2017) Attention Is All You Need, NIPS](https://arxiv.org/pdf/1706.03762.pdf) \n",
    "\n",
    "\n",
    "<div><img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"50%\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm\"></a>\n",
    "## LLM: BERT, GPT, ...\n",
    "\n",
    "[Jacob Devlin, et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "[BERT Google repo](https://github.com/google-research/bert)\n",
    "\n",
    "[Radford et al. (2018) Improving language understanding by generative pre-training](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)\n",
    "\n",
    "[GPT-2 OpenAI repo](https://github.com/openai/gpt-2)\n",
    "\n",
    "[PyTorch transformers](https://pytorch.org/hub/huggingface_pytorch-transformers/)\n",
    "\n",
    "<div><img src=\"https://www.researchgate.net/publication/354908597/figure/fig1/AS:1073191841722368@1632880282219/Model-structure-of-BERT-and-GPT.png\" width=\"50%\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm-sizes\"></a>\n",
    "## Размеры LLM моделей\n",
    "\n",
    "<div><img src=\"https://amatriain.net/blog/images/02-09.png\" width=\"80%\"/></div>\n",
    "\n",
    "[Amatriain et al. (2023) Transformer models: an introduction and catalog](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bpe\"></a>\n",
    "## Byte Pair Encoding\n",
    "\n",
    "Пусть, мы собираемся закодировать\n",
    "\n",
    " aaabdaaabac\n",
    "\n",
    "Пара букв «аа» встречается чаще всего, поэтому она будет заменена символом, который не используется в данных, например «Z». Теперь есть следующие данные и таблица замены:\n",
    "\n",
    " ZabdZabac\\\n",
    " Z=aa\n",
    "\n",
    "Затем процесс повторяется с парой букв «ab», заменяя ее на «Y»:\n",
    "\n",
    " ZYdZYac\\\n",
    " Y=ab\\\n",
    " Z=aa\n",
    "\n",
    "Единственная оставшаяся пара строчных букв встречается только один раз, и на этом кодирование можно остановить, или продолжить процесс с рекурсивным кодированием, заменив «ZY» на «X»:\n",
    "\n",
    " XdXac\\\n",
    " X=ZY\\\n",
    " Y=ab\\\n",
    " Z=aa\n",
    "\n",
    "Эти данные не могут быть сжаты ещё больше с помощью BPE, поскольку не существует пар, встречающихся более одного раза.\n",
    "\n",
    "Чтобы распаковать данные, просто выполните замены в обратном порядке.\n",
    "\n",
    "[Wikipedia: Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"generation\"></a>\n",
    "## Генерация текста\n",
    "\n",
    "[How to generate text](https://huggingface.co/blog/how-to-generate)\n",
    "\n",
    "### Greedy search\n",
    "<div><img src=\"https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png\" width=\"40%\"/></div>\n",
    "\n",
    "### Beam search\n",
    "<div><img src=\"https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png\" width=\"40%\"/></div>\n",
    "\n",
    "### Sampling\n",
    "<div><img src=\"https://huggingface.co/blog/assets/02_how-to-generate/sampling_search.png\" width=\"50%\"/></div>\n",
    "\n",
    "### Temperature\n",
    "\n",
    "<div><img src=\"https://huggingface.co/blog/assets/02_how-to-generate/sampling_search_with_temp.png\" width=\"50%\"/></div>\n",
    "\n",
    "### Top-K Sampling, Top-p sampling и др.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
