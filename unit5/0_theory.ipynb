{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention, self-attention, transformers\n",
    "\n",
    "Теория:\n",
    "* [Encoder-Decoder](#encoder-decoder)\n",
    "* [Attention](#attention)\n",
    "* [Self-Attention](#self-attention)\n",
    "* [Transformer](#transformer)\n",
    "* [LLM: BERT, GPT, ...](#llm)\n",
    "* [Размеры LLM моделей](#llm-sizes)\n",
    "* [Byte Pair Encoding](#bpe)\n",
    "* [Генерация текста](generation)\n",
    "\n",
    "Примеры кода:\n",
    "* [Модель PGT на pytorch](1_pure_torch.ipynb)\n",
    "* [GPT с помощью модулей Huggingface](2_transformers.ipynb)\n",
    "* [Предобученные модели](3_pretrained.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"encoder-decoder\"></a>\n",
    "## Encoder-Decoder\n",
    "\n",
    "* [Lena Voita, Sequence to Sequence (seq2seq) and Attention, NLP Course](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "\n",
    "<div><img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png\" width=\"50%\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"attention\"></a>\n",
    "## Attention\n",
    "\n",
    "[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)\n",
    "\n",
    "<div><img src=\"https://i.stack.imgur.com/Xtzg4.png\" width=\"50%\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"self-attention\"></a>\n",
    "## Self-Attention\n",
    "\n",
    "[Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "\n",
    "<div><img src=\"https://i.stack.imgur.com/J45g2.png\" width=\"50%\"/></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transformer\"></a>\n",
    "## Transformer\n",
    "\n",
    "[Vaswani et al. (2017) Attention Is All You Need, NIPS](https://arxiv.org/pdf/1706.03762.pdf) \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm\"></a>\n",
    "## LLM: BERT, GPT, ...\n",
    "\n",
    "[Jacob Devlin, et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "[BERT Google repo](https://github.com/google-research/bert)\n",
    "\n",
    "[Radford et al. (2018) Improving language understanding by generative pre-training](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)\n",
    "\n",
    "[GPT-2 OpenAI repo](https://github.com/openai/gpt-2)\n",
    "\n",
    "[PyTorch transformers](https://pytorch.org/hub/huggingface_pytorch-transformers/)\n",
    "\n",
    "![image.png](https://www.researchgate.net/publication/354908597/figure/fig1/AS:1073191841722368@1632880282219/Model-structure-of-BERT-and-GPT.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm-sizes\"></a>\n",
    "## Размеры LLM моделей\n",
    "\n",
    "<div><img src=\"https://amatriain.net/blog/images/02-09.png\" width=\"50%\"/></div>\n",
    "\n",
    "[Amatriain et al. (2023) Transformer models: an introduction and catalog](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bpe\"></a>\n",
    "## Byte Pair Encoding\n",
    "\n",
    "Suppose the data to be encoded is\n",
    "\n",
    " aaabdaaabac\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n",
    "\n",
    " ZabdZabac\\\n",
    " Z=aa\n",
    "\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n",
    "\n",
    " ZYdZYac\\\n",
    " Y=ab\\\n",
    " Z=aa\n",
    "\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with [[Recursion|recursive]] byte pair encoding, replacing \"ZY\" with \"X\":\n",
    "\n",
    " XdXac\\\n",
    " X=ZY\\\n",
    " Y=ab\\\n",
    " Z=aa\n",
    "\n",
    "This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\n",
    "\n",
    "To decompress the data, simply perform the replacements in the reverse order.\n",
    "\n",
    "[Wikipedia: Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"generation\"></a>\n",
    "## Генерация текста\n",
    "\n",
    "[How to generate text](https://huggingface.co/blog/how-to-generate)\n",
    "\n",
    "### Greedy search\n",
    "![image.png](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n",
    "\n",
    "### Beam search\n",
    "![image-2.png](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)\n",
    "\n",
    "### Sampling\n",
    "![image-2.png](https://huggingface.co/blog/assets/02_how-to-generate/sampling_search.png)\n",
    "\n",
    "### Temperature\n",
    "\n",
    "<div><img src=\"https://huggingface.co/blog/assets/02_how-to-generate/sampling_search_with_temp.png\" width=\"40%\"/></div>\n",
    "\n",
    "### Top-K Sampling, Top-p sampling и др.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
