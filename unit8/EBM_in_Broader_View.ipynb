{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Энергетические модели как концепция\n",
        "\n",
        "Цели:\n",
        "\n",
        "* Получить более широкий взгляд на концепцию energy-based моделей\n",
        "* Рассмотреть известные архитектуры моделей с этой точки зрения\n",
        "\n",
        "Содержание:\n",
        "\n",
        "* Преимущества EBM\n",
        "* Связь EBM с физикой\n",
        "* Заметки по обучению EBM\n",
        "* Архитектуры моделей\n",
        "  * Binary classifier\n",
        "  * AutoEncoder\n",
        "  * Denoising AutoEncoder\n",
        "  * Variational AutoEncoder\n",
        "  * Normalizing flows\n",
        "  * Diffusion models\n",
        "\n",
        "Ссылки:\n",
        "\n",
        "* [Energy-based models for sparse overcomplete representations. Teh, Welling, Osindero, Hinton (2003). ](https://www.jmlr.org/papers/v4/teh03a.html)\n",
        "* [A Tutorial on Energy-Based Learning. LeCun, Chopra, Hadsell, Ranzato, Huang (2006). ](https://cs.nyu.edu/~sumit/publications/assets/ebmtutorial.pdf)\n",
        "* [Energy-based generative adversarial network. Zhao, Mathieu, LeCun (2016).](https://arxiv.org/pdf/1609.03126.pdf)\n",
        "* [Слайды Alfredo Canziani с практического занятия на курсе DLSP21 NYU](https://drive.google.com/file/d/1aa1Hzq5KRekq32mlW4_pgIXMec18WgOg/edit)\n",
        "* [NYU DLSP21 страничка курса](https://atcold.github.io/NYU-DLSP21/)\n",
        "* [NYU DLFL22 страничка курса](https://atcold.github.io/NYU-DLFL22/)\n",
        "* [NYU DLSP21 YT playlist](https://www.youtube.com/watch?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n",
        "* [NYU DLFL22 YT playlist](https://www.youtube.com/watch?list=PLLHTzKZzVU9d_3TcHbyiAjl5qCbpJR-o0)\n",
        "* https://lilianweng.github.io/posts/2018-10-13-flow-models/\n",
        "* https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eofgYSRFW2N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Преимущества EBM\n",
        "\n",
        "Главная цель построения классических моделей машинного обучения - закодировать зависимости между переменными. Модель, которая выучила эти взаимосвязи, может быть использована для ответов на вопросы о значениях неизвестных переменных (Y) при данных известных (X).\n",
        "\n",
        "EBM ассоциируют скалярную величину энергии, как меру совместимости для каждого набора входных данных (X, Y). Инференс состоит в определении наблюдаемых величин и нахождении значений оставшихся величин так, чтобы энергия была минимальна.\n",
        "\n",
        "Обучение состоит в поиске энергетической функции, которая ассоциирует низкую энергию для корректных значений оставшихся переменных и высокую для некорректных.\n",
        "\n",
        "Функционал потерь, минимизируемый во время обучения, используется для определения качества доступных энергетических функций.\n",
        "\n",
        "E(Y, X) - энергетическая функция, отражающая степень совместимости между X и Y, где хорошо совместимым парам соот. низкие значения энергии.\n",
        "\n",
        "Следует провести различие между энергетической функцией, которая минимизируется в процессе вывода, и функционалом потерь (введенным в разделе 2), который минимизируется в процессе обучения.\n",
        "\n",
        "На вход подаётся X и модель производит ответ Y, наиболее совместимый с наблюдаемым X. Более точно, модель должна произвести значение $Y^∗$, выбранное из множества $\\mathcal{Y}$, для которого $E(Y, X)$ минимально:\n",
        "$$Y^* = \\arg \\min_{Y \\in \\mathcal{Y}} E(Y, X).$$\n",
        "\n",
        "Когда размер множества $\\mathcal{Y}$ невелик, мы можем просто вычислить $E(Y, X)$ для всех возможных значений $Y ∈ \\mathcal{Y}$ и выбрать наименьшее.\n",
        "Но иногда $\\mathcal{Y}$ слишком велико для полного перебора, например повышение качества изображения, перевод и пр..\n",
        "\n",
        "В таких ситуациях специальная стратегия, именуемая процедурой инференса должна быть использована, для поиска Y минимизирующего энергию. Выбор такой процедуры зависит от данных, например если Y непрерывная величина и энергия гладкая, это может быть алгоритм оптимизации на основе градиента. Или, если энергия представляется в виде графа, то инференс будет представлять собой поиск пути по взвешенному ациклическому графу, где энергия будет стоимость этого пути. Например в задаче распознавания рукописного текста.\n",
        "\n",
        "Примеры задач, решаемых в парадигме энергетических моделей:\n",
        "\n",
        "* классификация - насколько Y совместим c X\n",
        "* ранжирование - кто больше совместим Y1 или Y2\n",
        "* детекция - обнаружение лиц на фото и отбор областей по порогу\n",
        "* принятие решений (управление роботом) - хорошие направления продолжения движения имеют наименьшую энергию\n"
      ],
      "metadata": {
        "id": "bE7MVHmGRn91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Связь EBM с физикой\n",
        "\n",
        "Нормируют чаще всего с помощью распределения Больцмана (Гиббса):\n",
        "$$P(Y | X) = \\frac{e^{-\\beta E(Y, X)}}{\\int_{y \\in Y} e^{-\\beta E(y, X)} \\,dy},\n",
        "$$\n",
        "\n",
        "где $β$ - произвольная положительная константа, а знаменатель называется функцией разбиения (partition function) или интеграл состояний или каноническая статистическая сумма, по аналогии с подобными концепциями в статистической физике. Формула очень похожа на softmax.\n",
        "\n",
        "Распределение Гиббса в физике - это распределение вероятностей, которое даёт вероятность того, что система будет находиться в определённом состоянии как функция энергии этого состояния и температуры системы. Состояния с низкой энергией имеют более высокую вероятность быть занятыми.\n",
        "\n",
        "Из физики известно, что в системе с фиксированными _температурой_ и _объёмом_ положение устойчивого равновесия соответствует точке минимума свободной энергии.\n"
      ],
      "metadata": {
        "id": "4bpcNLboRqdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Заметки по обучению EBM\n",
        "\n",
        "Обучение EBM заключается в поиске энергетической функции, которая для любого значения X дает наилучшее Y. Поиск наилучшей энергетической функции выполняется в пределах семейства энергетических функций $\\mathcal{E}$, параметризуемых параметром $W$:\n",
        "$$\\mathcal{E} = \\{E(W, Y, X) : W \\in \\mathcal{W}\\}$$\n",
        "\n",
        "Задача обучения - поиск $W^∗$ минимизирующих функционал $\\mathcal{L}$ на обучающей выборке $\\mathcal{S} = \\{(X_i, Y_i) : i = 1, \\ldots, P\\}$\n",
        "$$W^∗=\\underset{W \\in \\mathcal{W}}{\\min}\\mathcal{​L}(W,\\mathcal{S})$$\n",
        "Часто функционал потерь определяется как\n",
        "$$\\mathcal{L}(E, \\mathcal{S}) = \\frac{1}{P} \\sum_{i=1}^{P} L(Y_i, E(W, \\mathcal{Y}, X_i)) + R(W)\n",
        "$$\n",
        "\n",
        "Это просто среднее значение функционалов по образцам из обучающей выборки, зависящее от желаемого ответа $Y_i$ и энергии, получаемой при фиксированном X и изменении Y. Так для каждого образца мы оцениваем срез поверхности энергии. $R(W)$ - регуляризация, которая может быть использована, чтобы встроить априорные знания о том, какие энергетические функции предпочитаемы в случае отсутствия обучающих данных.\n"
      ],
      "metadata": {
        "id": "2eMRS3gUTF-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/energy_surface.png\" width=\"600\">\n",
        "\n",
        "Энергетическая функция (F) даёт низкие значения для совместимых пар и высокие для несовместимых, что похоже на физику.\n",
        "Inference - найти такие y, что F(x, y) принимает низкие значения и таких решений может быть несколько. Всё, что от неё нужно - низкая энергия, для того, что мы хотим получать и высокая для тех, что не хотим.\n",
        "\n",
        "Например, FFNN - явная функция по вычислению y по x, EBM - неявная функция, которая выучивает зависимости между y и x.\n",
        "\n",
        "Если одному значению x соответствует множество y, то обычной сети будет плохо, а энергетической функции - нормально).\n",
        "\n",
        "Поиск y - задача оптимизации и может например выполняться градиентным спуском (не стохастическим).\n",
        "\n",
        "Пример для дискретного случая очень напоминает обучение языковых моделей.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/ebm_discrete_case_example.png\" width=\"600\">\n"
      ],
      "metadata": {
        "id": "POiJl4XvYALA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Суть обучения - **Push down the energy of data points, make sure the energy is higher elsewhere.**\n",
        "\n",
        "Как этого добиться, есть два способа:\n",
        "\n",
        "* контрастивные методы - понижаем энергию известных пар (x, y), повышаем энергию остальных пар, простые методы, но менее эффективны\n",
        "* архитектурные методы / регуляризация - строим F(x, y) так, чтобы объём областей с низкой энергией был ограничен или минимизирован с помощью регуляризации\n"
      ],
      "metadata": {
        "id": "1lQ5H4jcTLpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слайд с лекции ЛеКуна:\n",
        "\n",
        "* Contrastive: (they all are different ways to pick which points to push up)\n",
        "\t* C1: push down of the energy of data points, push up everywhere else: Max likelihood (needs tractable partition function or variational approximation)\n",
        "\t* C2: push down of the energy of data points, push up on chosen locations: max likelihood with MC/MMC/HMC, Contrastive divergence, Metric learning/Siamese nets, Ratio Matching, Noise Contrastive Estimation, Min Probability Flow, adversarial generator/GANs\n",
        "\t* C3: train a function that maps points off the data manifold to points on the data manifold: denoising auto-encoder, masked auto-encoder (e.g. BERT)\n",
        "* Regularized/Architectural: (Different ways to limit the information capacity of the latent representation)\n",
        "\t* A1: build the machine so that the volume of low energy space is bounded: PCA, K-means, Gaussian Mixture Model, Square ICA, normalizing flows...\n",
        "\t* A2: use a regularization term that measures the volume of space that has low energy: Sparse coding, sparse auto-encoder, LISTA, Variational Auto-Encoders, discretization/VQ/VQVAE.\n",
        "\t* A3: F(x,y) = C(y, G(x,y)), make G(x,y) as \"constant\" as possible with respect to y: Contracting auto-encoder, saturating auto-encoder\n",
        "\t* A4: minimize the gradient and maximize the curvature around data points: score matching"
      ],
      "metadata": {
        "id": "iAeDHSVxTPxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примеры моделей\n",
        "\n",
        "Contrastive Joint Embedding - контрастивный метод. Нужно очень много данных (hard negative mining).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/contrastive_joint_embedding.png\" width=\"300\">\n",
        "\n",
        "Barlow twins - похожий, но не контрастивный.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/barlow_twins.png\" width=\"600\">"
      ],
      "metadata": {
        "id": "hZdsM0zacp27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Архитектуры моделей"
      ],
      "metadata": {
        "id": "vRnRpWvkTO-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary classifier\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/classifier_arch.png)"
      ],
      "metadata": {
        "id": "q4J5hQ22ZXxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoEncoder\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/ae_arch.png)"
      ],
      "metadata": {
        "id": "hxsV4AzUTHC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Denoising AutoEncoder\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/dae_arch.png)"
      ],
      "metadata": {
        "id": "ErWJZ63iSQAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational AutoEncoder\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/vae_arch.png)"
      ],
      "metadata": {
        "id": "f_Qai7gDZAXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing flows\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/nf_arch.png)"
      ],
      "metadata": {
        "id": "TP9PcZO6ZDvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diffusion models\n",
        "\n",
        "![AE](https://raw.githubusercontent.com/sswt/dive2gai/main/.github/images/w8/diffusion_arch.png)"
      ],
      "metadata": {
        "id": "poz5BByLZPFX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acNbP2KhZVWF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}